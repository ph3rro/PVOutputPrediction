{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8MWBOKc_p8N"
      },
      "source": [
        "### Libraries and data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv-USH6qHowm",
        "outputId": "678daabb-cabf-494f-ca17-90c707cc9923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: CRPS in c:\\users\\aiden\\onedrive\\documents\\pvoutputprediction\\pytorch_env\\lib\\site-packages (2.0.4)\n",
            "Requirement already satisfied: numpy in c:\\users\\aiden\\onedrive\\documents\\pvoutputprediction\\pytorch_env\\lib\\site-packages (from CRPS) (2.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install CRPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "lZ8wnidH_p8O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "import h5py\n",
        "import math\n",
        "import matplotlib.dates as mdates\n",
        "import numpy.ma as ma\n",
        "import CRPS.CRPS as pscore\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.image as mpimg\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "writer = SummaryWriter()\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.1+cu128\n",
            "CUDA available: True\n",
            "CUDA version: 12.8\n",
            "GPU count: 1\n",
            "GPU name: NVIDIA GeForce RTX 4070 Ti\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "# Check if CUDA is available\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "\n",
        "# Get GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJYAy-t7_p8P",
        "outputId": "abc24607-f813-4790-9773-6dd4494b3283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data_folder: c:\\Users\\aiden\\OneDrive\\Documents\\PVOutputPrediction\\data\n",
            "data_path: c:\\Users\\aiden\\OneDrive\\Documents\\PVOutputPrediction\\data\\video_prediction_dataset.hdf5\n",
            "output_folder: c:\\Users\\aiden\\OneDrive\\Documents\\PVOutputPrediction\\model_output\\UNet_sky_image_PV_mapping\n"
          ]
        }
      ],
      "source": [
        "# define the data location and load data\n",
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "data_folder = os.path.join(cwd,\"data\")\n",
        "data_path = os.path.join(data_folder,'video_prediction_dataset.hdf5')\n",
        "\n",
        "# !change model name for different models!\n",
        "model_name = 'UNet_sky_image_PV_mapping'\n",
        "output_folder = os.path.join(cwd,\"model_output\", model_name)\n",
        "if os.path.isdir(output_folder)==False:\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "print(\"data_folder:\", data_folder)\n",
        "print(\"data_path:\", data_path)\n",
        "print(\"output_folder:\", output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJKp70Oy_p8P",
        "outputId": "815bb243-8fd5-4be2-9d8a-08acbdb9236a"
      },
      "outputs": [],
      "source": [
        "# # generate handler for the hdf5 data\n",
        "# forecast_dataset = h5py.File(data_path, 'r')\n",
        "\n",
        "# # show structure of the hdf5 data\n",
        "# def get_all(name):\n",
        "#     if name!=None:\n",
        "#         print(forecast_dataset[name])\n",
        "\n",
        "# forecast_dataset.visit(get_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# ds = load_dataset(\"skyimagenet/SKIPPD\")\n",
        "\n",
        "# images_train =[]\n",
        "# pv_train = []\n",
        "# times_train = []\n",
        "# images_test =[]\n",
        "# pv_test = []\n",
        "# times_test = []\n",
        "# for mode in [\"train\", \"test\"]:\n",
        "    # globals()[f'images_{mode}'].append(np.array(ds[mode][:]['image']))\n",
        "    # globals()[f'pv_{mode}'].append(ds[mode][:]['pv'])\n",
        "    # globals()[f'times_{mode}'] = ds[mode][:]['time']\n",
        "\n",
        "# for mode in [\"train\", \"test\"]:\n",
        "#     globals()[f'images_{mode}'] = np.array(globals()[f'images_{mode}'])\n",
        "#     globals()[f'pv_{mode}'] = np.array(globals()[f'pv_{mode}'] )\n",
        "    \n",
        "# images_train = np.squeeze(images_train)\n",
        "# pv_train = np.squeeze(pv_train)\n",
        "# times_train = np.squeeze(times_train)\n",
        "\n",
        "# images_test = np.squeeze(images_test)\n",
        "# pv_test = np.squeeze(pv_test)\n",
        "# times_test = np.squeeze(times_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.save(\"times_train.npy\", np.array(times_train))\n",
        "# np.save(\"times_test.npy\", np.array(times_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "images_trainval = np.load(\"images_train.npy\")\n",
        "pv_trainval = np.load(\"pv_train.npy\")\n",
        "times_trainval = np.load(\"times_train.npy\", allow_pickle=True)\n",
        "\n",
        "# images_test = np.load(\"images_test.npy\")\n",
        "# pv_test = np.load(\"pv_test.npy\")\n",
        "# times_test = np.load(\"times_test.npy\", allow_pickle=True)\n",
        "\n",
        "images_trainval = images_trainval.transpose(0, 3, 1, 2).astype(np.float32)\n",
        "# images_test = images_test.transpose(0, 3, 1, 2)\n",
        "images_trainval = torch.from_numpy(images_trainval)\n",
        "# images_test = torch.from_numpy(images_test).float()\n",
        "# pv_trainval = torch.from_numpy(pv_trainval).float()\n",
        "# pv_test = torch.from_numpy(pv_test).float()\n",
        "# times_trainval = np.squeeze(times_trainval)\n",
        "# times_test = np.squeeze(times_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mn1bvNMd_p8Q",
        "outputId": "2b8fd69a-60a2-4c5d-dc92-3c09394bfc72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nprint(\\'-\\'*50)\\n# get the input dimension for constructing the model\\n# the input images will be reshaped from (None, 8, 64, 64, 3) to (None, 64, 64, 24)\\n# the Stanford dataset contains 16 log terms, we only take 8 with 2-min interval\\n# to be consistent with the Cambridge dataset\\nimg_side_len = forecast_dataset[\\'trainval\\'][\\'images_log\\'].shape[2]\\nnum_color_channel = forecast_dataset[\\'trainval\\'][\\'images_log\\'].shape[4]\\nimage_input_dim = [img_side_len,img_side_len,num_color_channel]\\n\\nprint(\"image side length:\", img_side_len)\\nprint(\"number of color channels:\", num_color_channel)\\nprint(\"input image dimension:\", image_input_dim)\\n\\n# load time stamps into the memory\\ntimes_trainval = np.load(os.path.join(data_folder,\"times_curr_trainval.npy\"),allow_pickle=True)\\n# !change samplpe fraction for different models! ########\\nfraction = 1 # get only 10% sample of the whole dataset\\n#########################################################\\n\\nnum_samples = int(len(times_trainval)*fraction)\\ntimes_trainval = times_trainval[:num_samples]\\nprint(\"times_trainval.shape:\", times_trainval.shape)\\n\\n# read through the dataset once in order to cache it but not store it into the memory\\n## read the data by batch\\nnum_samples = len(times_trainval)\\nbatch_size = num_samples//5\\nprint(\"batch_size:\", batch_size)\\nindices = np.arange(num_samples)\\nprint(\\'-\\'*50)\\nprint(\\'data reading start...\\')\\nfor i in range(int(num_samples / batch_size) + 1):\\n    start_time = time.time()\\n    start_idx = (i * batch_size) % num_samples\\n    idxs = indices[start_idx:start_idx + batch_size]\\n    image = forecast_dataset[\\'trainval\\'][\\'images_log\\'][idxs]\\n    print (\"shape: \", forecast_dataset[\\'trainval\\'][\\'images_log\\'].shape)\\n    print(image.shape)\\n    plt.figure(figsize=(6, 6))\\n    plt.imshow(image[0,10])\\n    plt.title(\\'3D RGB Image\\')\\n    plt.show()\\n    pv = forecast_dataset[\\'trainval\\'][\\'pv_log\\'][idxs]\\n    print(pv.shape)\\n    #_ = forecast_dataset[\\'trainval\\'][\\'pv_pred\\'][idxs]\\n    end_time = time.time()\\n    print(\"batch {0} samples: {1} to {2}, {3:.2f}% finished, processing time {4:.2f}s\"\\n          .format(i+1, idxs[0],idxs[-1],(idxs[-1]/num_samples)*100,(end_time-start_time)))\\n\\n# temporially close the dataset, will use \"with\" statement to open it when we use it\\n#forecast_dataset.close()'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "print('-'*50)\n",
        "# get the input dimension for constructing the model\n",
        "# the input images will be reshaped from (None, 8, 64, 64, 3) to (None, 64, 64, 24)\n",
        "# the Stanford dataset contains 16 log terms, we only take 8 with 2-min interval\n",
        "# to be consistent with the Cambridge dataset\n",
        "img_side_len = forecast_dataset['trainval']['images_log'].shape[2]\n",
        "num_color_channel = forecast_dataset['trainval']['images_log'].shape[4]\n",
        "image_input_dim = [img_side_len,img_side_len,num_color_channel]\n",
        "\n",
        "print(\"image side length:\", img_side_len)\n",
        "print(\"number of color channels:\", num_color_channel)\n",
        "print(\"input image dimension:\", image_input_dim)\n",
        "\n",
        "# load time stamps into the memory\n",
        "times_trainval = np.load(os.path.join(data_folder,\"times_curr_trainval.npy\"),allow_pickle=True)\n",
        "# !change samplpe fraction for different models! ########\n",
        "fraction = 1 # get only 10% sample of the whole dataset\n",
        "#########################################################\n",
        "\n",
        "num_samples = int(len(times_trainval)*fraction)\n",
        "times_trainval = times_trainval[:num_samples]\n",
        "print(\"times_trainval.shape:\", times_trainval.shape)\n",
        "\n",
        "# read through the dataset once in order to cache it but not store it into the memory\n",
        "## read the data by batch\n",
        "num_samples = len(times_trainval)\n",
        "batch_size = num_samples//5\n",
        "print(\"batch_size:\", batch_size)\n",
        "indices = np.arange(num_samples)\n",
        "print('-'*50)\n",
        "print('data reading start...')\n",
        "for i in range(int(num_samples / batch_size) + 1):\n",
        "    start_time = time.time()\n",
        "    start_idx = (i * batch_size) % num_samples\n",
        "    idxs = indices[start_idx:start_idx + batch_size]\n",
        "    image = forecast_dataset['trainval']['images_log'][idxs]\n",
        "    print (\"shape: \", forecast_dataset['trainval']['images_log'].shape)\n",
        "    print(image.shape)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image[0,10])\n",
        "    plt.title('3D RGB Image')\n",
        "    plt.show()\n",
        "    pv = forecast_dataset['trainval']['pv_log'][idxs]\n",
        "    print(pv.shape)\n",
        "    #_ = forecast_dataset['trainval']['pv_pred'][idxs]\n",
        "    end_time = time.time()\n",
        "    print(\"batch {0} samples: {1} to {2}, {3:.2f}% finished, processing time {4:.2f}s\"\n",
        "          .format(i+1, idxs[0],idxs[-1],(idxs[-1]/num_samples)*100,(end_time-start_time)))\n",
        "\n",
        "# temporially close the dataset, will use \"with\" statement to open it when we use it\n",
        "#forecast_dataset.close()\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkxa-UKU_p8Q"
      },
      "source": [
        "### Input data pipeline helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "ODMVPPkP_p8Q"
      },
      "outputs": [],
      "source": [
        "# day block shuffling of the time stamps, and return shuffled indices\n",
        "def day_block_shuffle(times_trainval):\n",
        "\n",
        "    # Only keep the date of each time point\n",
        "    dates_trainval = np.zeros_like(times_trainval, dtype=datetime.date)\n",
        "    for i in range(len(times_trainval)):\n",
        "        dates_trainval[i] = times_trainval[i].date()\n",
        "\n",
        "    # Chop the indices into blocks, so that each block contains the indices of the same day\n",
        "    unique_dates = np.unique(dates_trainval)\n",
        "    blocks = []\n",
        "    for i in range(len(unique_dates)):\n",
        "        blocks.append(np.where(dates_trainval == unique_dates[i])[0])\n",
        "\n",
        "    # shuffle the blocks, and chain it back together\n",
        "    np.random.seed(1)\n",
        "    np.random.shuffle(blocks)\n",
        "    shuffled_indices = np.asarray(list(itertools.chain.from_iterable(blocks)))\n",
        "\n",
        "    return shuffled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "DDLHlv41_p8Q"
      },
      "outputs": [],
      "source": [
        "# a cross validation generator function for spliting the dayblock shuffled indices into training and validation\n",
        "def cv_split(split_data, fold_index, num_fold):\n",
        "    '''\n",
        "    input:\n",
        "    split_data: the dayblock shuffled indices to be splitted\n",
        "    fold_index: the ith fold chosen as the validation, used for generating the seed for random shuffling\n",
        "    num_fold: N-fold cross validation\n",
        "    output:\n",
        "    data_train: the train data indices\n",
        "    data_val: the validation data indices\n",
        "    '''\n",
        "    # randomly divides into a training set and a validation set\n",
        "    num_samples = len(split_data)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    # finding training and validation indices\n",
        "    val_mask = np.zeros(len(indices), dtype=bool)\n",
        "    val_mask[int(fold_index / num_fold * num_samples):int((fold_index + 1) / num_fold * num_samples)] = True\n",
        "    val_indices = indices[val_mask]\n",
        "    train_indices = indices[np.logical_not(val_mask)]\n",
        "\n",
        "    # shuffle indices\n",
        "    np.random.seed(fold_index)\n",
        "    np.random.shuffle(train_indices)\n",
        "    np.random.shuffle(val_indices)\n",
        "\n",
        "    data_train = split_data[train_indices]\n",
        "    data_val = split_data[val_indices]\n",
        "\n",
        "    return data_train,data_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "kehtU2z9_p8R"
      },
      "outputs": [],
      "source": [
        "def mask_background(img): # put all background pixels (the ones outside the circle region of sky images) to 0s\n",
        "    mask = np.ones((3,64,64), dtype=bool)\n",
        "    for i in range(64):\n",
        "        for j in range(64):\n",
        "            if (i-30)**2+(j-30)**2>=31**2:\n",
        "                mask[:,i,j]=0\n",
        "    mask_img = img*mask\n",
        "    return mask_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z24FGOTmB3LY"
      },
      "outputs": [],
      "source": [
        "class PVDataset(Dataset):\n",
        "\n",
        "    def __init__(self, images, pv, transform=None, target_transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images = mask_background(images)\n",
        "        self.images = self.images/255\n",
        "        self.pv = pv\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = self.images[idx]\n",
        "        pv = self.pv[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            pv = self.target_transform(pv)\n",
        "        return image, pv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "on2fDoJf_p8S"
      },
      "outputs": [],
      "source": [
        "def compute_winkler_score(prob_prediction,observation):\n",
        "    alpha = 0.1\n",
        "    lb = np.percentile(prob_prediction,5,axis=0)\n",
        "    ub = np.percentile(prob_prediction,95,axis=0)\n",
        "    delta = ub-lb\n",
        "    if observation<lb:\n",
        "        sc = delta+2*(lb-observation)/alpha\n",
        "    if observation>ub:\n",
        "        sc = delta+2*(observation-ub)/alpha\n",
        "    if (observation>=lb) and (observation<=ub):\n",
        "        sc = delta\n",
        "    return sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twl8BbH8_p8S"
      },
      "source": [
        "### Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "93-c3tU7_p8S"
      },
      "outputs": [],
      "source": [
        "# define training time parameters\n",
        "num_filters = 12\n",
        "num_epochs = 200 #(The maximum epoches set to 200 and there might be early stopping depends on validation loss)\n",
        "num_fold = 10 # 10-fold cross-validation\n",
        "batch_size = 128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "cPdcNds2_p8S",
        "outputId": "ed5cb4ad-b35e-449a-f39a-e8fe1bb50f9c"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torchsummary import summary\n",
        "\n",
        "# class Image2PV(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Image2PV, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_channels = 3, out_channels = num_filters, kernel_size = (3,3), padding = 'same')\n",
        "#         self.conv3x3 = nn.Conv2d(in_channels=3, out_channels=num_filters, kernel_size=(3,3), padding='same')\n",
        "\n",
        "#         self.bn = nn.BatchNorm2d(num_filters)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.dropout = nn.Dropout()\n",
        "#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         self.upsample = self.Up2x2Conv3x3(num_filters)\n",
        "\n",
        "#         # Define the convolutional blocks\n",
        "#         self.block1 = self.Conv3x3block(num_filters)\n",
        "#         self.block2 = self.Conv3x3block(2 * num_filters)\n",
        "#         self.block3 = self.Conv3x3block(4 * num_filters)\n",
        "\n",
        "#         # Define bottleneck blocks\n",
        "#         self.bottleneck1 = self.BottleNeckBlock(4 * num_filters)\n",
        "#         self.bottleneck2 = self.BottleNeckBlock(4 * num_filters)\n",
        "\n",
        "#         # Final layers\n",
        "#         self.final_conv = nn.Conv2d(in_channels=num_filters, out_channels=1, kernel_size=(1,1), padding='same')\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         self.fc = nn.Linear(num_filters * 64 * 64, 1)\n",
        "    \n",
        "#     def Conv3x3block(self, input_channel):\n",
        "#         def block(X):\n",
        "#             conv1 = self.conv3x3(X)\n",
        "#             bn1 = self.bn(input_channel)(conv1)\n",
        "#             relu1 = self.relu()(bn1)\n",
        "#             return relu1\n",
        "#         return block\n",
        "    \n",
        "#     def BottleNeckBlock(self, input_channel):\n",
        "#         def block(X):\n",
        "#             conv1 = self.conv3x3(X)\n",
        "#             bn1 = self.bn(input_channel)(conv1)\n",
        "#             relu1 = self.relu()(bn1)\n",
        "\n",
        "#             conv2 = self.conv3x3(relu1)\n",
        "#             bn2 = self.bn(input_channel)(conv2)\n",
        "\n",
        "#             output = X + bn2\n",
        "#             return output\n",
        "#         return block\n",
        "    \n",
        "#     def Up2x2Conv3x3(self, input_channel):\n",
        "#         '''\n",
        "#         input_channel: number of input channels, a scaler\n",
        "#         '''\n",
        "#         def block(X):\n",
        "#             upsampling = nn.Upsample(size=(2, 2))(X)\n",
        "\n",
        "#             conv1 = self.conv3x3(upsampling)\n",
        "#             return conv1\n",
        "#         return block\n",
        "    \n",
        "#     def forward(self, x):\n",
        "#         output1 = self.conv1(x)\n",
        "#         output3 = self.block1(output1)\n",
        "\n",
        "#         output4 = self.maxpool(output3)\n",
        "#         output6 = self.block2(output4)\n",
        "        \n",
        "#         output7 = self.maxpool(output6)\n",
        "#         output8 = self.block3(output7)\n",
        "\n",
        "#         output9 = self.bottleneck1(output8)\n",
        "#         output10 = self.bottleneck2(output9)\n",
        "\n",
        "#         output11 = self.upsample(output10)\n",
        "#         output12 = torch.cat((output11, output6), dim=1)\n",
        "\n",
        "#         output14 = self.block2(output12)\n",
        "#         output14 = self.dropout(0.4)(output14)\n",
        "\n",
        "#         output15 = self.upsample(output14)\n",
        "#         output16 = torch.cat((output15, output3), dim=1)\n",
        "\n",
        "#         output18 = self.block1(output16)\n",
        "#         output18 = self.dropout(0.4)(output18)\n",
        "\n",
        "#         y = self.final_conv(output18)\n",
        "#         y = self.flatten(y)\n",
        "#         y_out = self.fc(y)\n",
        "\n",
        "#         return y_out\n",
        "\n",
        "# def train(model, device, train_loader, optimizer):\n",
        "#     model.train()\n",
        "#     size = len(train_loader.dataset)\n",
        "#     for batch_idx, (image,pv) in enumerate(train_loader):\n",
        "#         image, pv = image.to(device), pv.to(device)\n",
        "        \n",
        "#         output = model(image)\n",
        "#         loss = nn.MSELoss(output, pv)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         if batch_idx % 100 == 0:\n",
        "#             loss, current = loss.item(), batch_idx * batch_size + len(image)\n",
        "#             print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "# # Initialize and print model architecture\n",
        "# model = Image2PV().to(device)\n",
        "# summary(model, input_size = ( 3,64,64))\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KIwj8ITRhkrA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNet(\n",
            "  (conv1x1_input): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (conv3x3_1): Conv3x3Block(\n",
            "    (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3x3_2): Conv3x3Block(\n",
            "    (conv): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3x3_3): Conv3x3Block(\n",
            "    (conv): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (bottleneck1): BottleNeckBlock(\n",
            "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (bottleneck2): BottleNeckBlock(\n",
            "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (up1): Up2x2Conv3x3(\n",
            "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (conv): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (conv3x3_4): Conv3x3Block(\n",
            "    (conv): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (dropout1): Dropout2d(p=0.4, inplace=False)\n",
            "  (up2): Up2x2Conv3x3(\n",
            "    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (conv): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (conv3x3_5): Conv3x3Block(\n",
            "    (conv): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (dropout2): Dropout2d(p=0.4, inplace=False)\n",
            "  (conv1x1_output): Conv2d(12, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (relu_final): ReLU(inplace=True)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (final_dense): Linear(in_features=4096, out_features=1, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([1, 1])\n",
            "Total parameters: 128,250\n",
            "Trainable parameters: 128,250\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 12, 64, 64]              48\n",
            "            Conv2d-2           [-1, 12, 64, 64]           1,308\n",
            "       BatchNorm2d-3           [-1, 12, 64, 64]              24\n",
            "              ReLU-4           [-1, 12, 64, 64]               0\n",
            "      Conv3x3Block-5           [-1, 12, 64, 64]               0\n",
            "         MaxPool2d-6           [-1, 12, 32, 32]               0\n",
            "            Conv2d-7           [-1, 24, 32, 32]           2,616\n",
            "       BatchNorm2d-8           [-1, 24, 32, 32]              48\n",
            "              ReLU-9           [-1, 24, 32, 32]               0\n",
            "     Conv3x3Block-10           [-1, 24, 32, 32]               0\n",
            "        MaxPool2d-11           [-1, 24, 16, 16]               0\n",
            "           Conv2d-12           [-1, 48, 16, 16]          10,416\n",
            "      BatchNorm2d-13           [-1, 48, 16, 16]              96\n",
            "             ReLU-14           [-1, 48, 16, 16]               0\n",
            "     Conv3x3Block-15           [-1, 48, 16, 16]               0\n",
            "           Conv2d-16           [-1, 48, 16, 16]          20,784\n",
            "      BatchNorm2d-17           [-1, 48, 16, 16]              96\n",
            "             ReLU-18           [-1, 48, 16, 16]               0\n",
            "           Conv2d-19           [-1, 48, 16, 16]          20,784\n",
            "      BatchNorm2d-20           [-1, 48, 16, 16]              96\n",
            "  BottleNeckBlock-21           [-1, 48, 16, 16]               0\n",
            "           Conv2d-22           [-1, 48, 16, 16]          20,784\n",
            "      BatchNorm2d-23           [-1, 48, 16, 16]              96\n",
            "             ReLU-24           [-1, 48, 16, 16]               0\n",
            "           Conv2d-25           [-1, 48, 16, 16]          20,784\n",
            "      BatchNorm2d-26           [-1, 48, 16, 16]              96\n",
            "  BottleNeckBlock-27           [-1, 48, 16, 16]               0\n",
            "         Upsample-28           [-1, 48, 32, 32]               0\n",
            "           Conv2d-29           [-1, 24, 32, 32]          10,392\n",
            "     Up2x2Conv3x3-30           [-1, 24, 32, 32]               0\n",
            "           Conv2d-31           [-1, 24, 32, 32]          10,392\n",
            "      BatchNorm2d-32           [-1, 24, 32, 32]              48\n",
            "             ReLU-33           [-1, 24, 32, 32]               0\n",
            "     Conv3x3Block-34           [-1, 24, 32, 32]               0\n",
            "        Dropout2d-35           [-1, 24, 32, 32]               0\n",
            "         Upsample-36           [-1, 24, 64, 64]               0\n",
            "           Conv2d-37           [-1, 12, 64, 64]           2,604\n",
            "     Up2x2Conv3x3-38           [-1, 12, 64, 64]               0\n",
            "           Conv2d-39           [-1, 12, 64, 64]           2,604\n",
            "      BatchNorm2d-40           [-1, 12, 64, 64]              24\n",
            "             ReLU-41           [-1, 12, 64, 64]               0\n",
            "     Conv3x3Block-42           [-1, 12, 64, 64]               0\n",
            "        Dropout2d-43           [-1, 12, 64, 64]               0\n",
            "           Conv2d-44            [-1, 1, 64, 64]              13\n",
            "             ReLU-45            [-1, 1, 64, 64]               0\n",
            "          Flatten-46                 [-1, 4096]               0\n",
            "           Linear-47                    [-1, 1]           4,097\n",
            "================================================================\n",
            "Total params: 128,250\n",
            "Trainable params: 128,250\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 9.42\n",
            "Params size (MB): 0.49\n",
            "Estimated Total Size (MB): 9.96\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "class Conv3x3Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic convolutional block with Conv2D -> BatchNorm -> ReLU\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(Conv3x3Block, self).__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class BottleNeckBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual bottleneck block with skip connection\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(BottleNeckBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = out + identity  # Skip connection\n",
        "        return out\n",
        "\n",
        "class Up2x2Conv3x3(nn.Module):\n",
        "    \"\"\"\n",
        "    Upsampling block with 2x2 upsampling followed by 3x3 convolution\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(Up2x2Conv3x3, self).__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net style architecture for Image-to-PV prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, image_input_dim, num_filters=12):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Assuming image_input_dim is (height, width, channels)\n",
        "        # PyTorch uses (channels, height, width)\n",
        "        if len(image_input_dim) == 3:\n",
        "            input_channels = image_input_dim[0]  # Assuming CHW format\n",
        "        else:\n",
        "            input_channels = image_input_dim[2]  # Assuming HWC format\n",
        "\n",
        "        # Initial 1x1 convolution\n",
        "        self.conv1x1_input = nn.Conv2d(input_channels, num_filters, kernel_size=1, padding=0)\n",
        "\n",
        "        # Encoder (contracting path)\n",
        "        self.conv3x3_1 = Conv3x3Block(num_filters, num_filters)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_2 = Conv3x3Block(num_filters, 2 * num_filters)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_3 = Conv3x3Block(2 * num_filters, 4 * num_filters)\n",
        "\n",
        "        # Bottleneck with residual blocks\n",
        "        self.bottleneck1 = BottleNeckBlock(4 * num_filters)\n",
        "        self.bottleneck2 = BottleNeckBlock(4 * num_filters)\n",
        "\n",
        "        # Decoder (expanding path)\n",
        "        self.up1 = Up2x2Conv3x3(4 * num_filters, 2 * num_filters)\n",
        "        self.conv3x3_4 = Conv3x3Block(4 * num_filters, 2 * num_filters)  # After concatenation\n",
        "        self.dropout1 = nn.Dropout2d(0.4)\n",
        "\n",
        "        self.up2 = Up2x2Conv3x3(2 * num_filters, num_filters)\n",
        "        self.conv3x3_5 = Conv3x3Block(2 * num_filters, num_filters)  # After concatenation\n",
        "        self.dropout2 = nn.Dropout2d(0.4)\n",
        "\n",
        "        # Final 1x1 convolution and output\n",
        "        self.conv1x1_output = nn.Conv2d(num_filters, 1, kernel_size=1, padding=0)\n",
        "        self.relu_final = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Global average pooling and final dense layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.final_dense = nn.Linear(4096, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial 1x1 convolution\n",
        "        x = self.conv1x1_input(x)\n",
        "\n",
        "        # Encoder path\n",
        "        # First level\n",
        "        skip1 = self.conv3x3_1(x)  # Save for skip connection\n",
        "        x = self.maxpool1(skip1)\n",
        "\n",
        "        # Second level\n",
        "        skip2 = self.conv3x3_2(x)  # Save for skip connection\n",
        "        x = self.maxpool2(skip2)\n",
        "\n",
        "        # Third level\n",
        "        x = self.conv3x3_3(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck1(x)\n",
        "        x = self.bottleneck2(x)\n",
        "\n",
        "        # Decoder path\n",
        "        # First upsampling\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, skip2], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_4(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Second upsampling\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, skip1], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_5(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Final output\n",
        "        x = self.conv1x1_output(x)\n",
        "        x = self.relu_final(x)\n",
        "\n",
        "        # Global pooling and final prediction\n",
        "        x = self.flatten(x)\n",
        "        x = self.final_dense(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "def create_model(image_input_dim=(3, 64, 64), num_filters=num_filters):\n",
        "    \"\"\"\n",
        "    Create the UNet model\n",
        "\n",
        "    Args:\n",
        "        image_input_dim: Input image dimensions (C, H, W) for PyTorch\n",
        "        num_filters: Number of base filters\n",
        "\n",
        "    Returns:\n",
        "        PyTorch model\n",
        "    \"\"\"\n",
        "    model = UNet(image_input_dim, num_filters)\n",
        "    return model\n",
        "\n",
        "# Example instantiation and summary\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 64x64 RGB images\n",
        "    model = create_model(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\n",
        "    \n",
        "    # Print model summary\n",
        "    print(model)\n",
        "\n",
        "    # Test with dummy input\n",
        "    dummy_input = torch.randn(1, 3, 64, 64).to(device)  # Batch size 1, 3 channels, 64x64\n",
        "    output = model(dummy_input)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    summary(model, input_size = (3,64,64))\n",
        "\n",
        "def trainval(model, device, loader, optimizer, criterion, mode=\"train\"):\n",
        "    \n",
        "    if mode == \"train\":\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "        \n",
        "    size = len(loader.dataset)\n",
        "         \n",
        "    total_loss = 0\n",
        "    for batch_idx, (image,pv) in enumerate(loader):\n",
        "        image, pv = image.to(device), pv.to(device)\n",
        "        \n",
        "        output = model(image)\n",
        "        loss = criterion(output, pv)\n",
        "        if mode == \"train\":\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    total_loss = total_loss / size\n",
        "    return total_loss\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "initial_learning_rate = 2e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr = initial_learning_rate)\n",
        "\n",
        "def checkpoint(model, filename):\n",
        "\ttorch.save(model.state_dict(), filename)\n",
        "\n",
        "def resume(model, filename):\n",
        "\tmodel.load_state_dict(torch.load(filename))\n",
        "\n",
        "scheduler = lr_scheduler.ExponentialLR(optimizer, 0.933)\n",
        "\n",
        "indices_dayblock_shuffled = day_block_shuffle(times_trainval)\n",
        "\n",
        "train_loss_hist = []\n",
        "val_loss_hist = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for i in range (num_fold):\n",
        "\tindices_train, indices_val = cv_split(indices_dayblock_shuffled,i,num_fold)\n",
        "\timages_train = images_trainval[indices_train]\n",
        "\tpv_train = pv_trainval[indices_train]\n",
        "\timages_val = images_trainval[indices_val]\n",
        "\tpv_val = pv_trainval[indices_val]\n",
        "\n",
        "\ttrain = PVDataset(images_train, pv_train)\t\n",
        "\tval = PVDataset(images_val, pv_val)\n",
        "\ttrain_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "\tval_loader = DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\ttrain_loss_current = []\n",
        "\tval_loss_current = []\n",
        "\tmin_loss = 10000\n",
        "\n",
        "\tfor j in range(num_epochs):\n",
        "\t\t\n",
        "\t\tloss = trainval(model, device, train_loader, optimizer, criterion, mode=\"train\")\n",
        "\t\ttrain_loss_current.append(loss)\t\n",
        "\n",
        "\t\tprint(f\"Epoch {j}: Training Loss = \", loss, )\n",
        "\n",
        "\t\tif (j % 5 == 0 or j == num_epochs-1):\n",
        "\t\t\tloss = trainval(model, device, val_loader, optimizer, criterion, mode=\"val\")\n",
        "\t\t\tif loss > val_loss_current[-2] and loss > val_loss_current[-3]:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tif loss < min_loss:\n",
        "\t\t\t\tmin_loss = loss\n",
        "\t\t\t\tcheckpoint(model, \"best_model_repetition_\" + str(i+1) + \".pth\")\n",
        "\t\t\tval_loss_current.append(loss)\n",
        "\t\t\t\n",
        "\n",
        "\t\t\tprint(f\"Epoch {j}: Val Loss = \", loss, )\n",
        "\t\tscheduler.step()\n",
        "\ttrain_loss_hist.append(train_loss_current)\n",
        "\tval_loss_hist.append(val_loss_current)\n",
        "\tplt.plot(train_loss_hist[i],label='train')\n",
        "\tplt.plot(val_loss_hist[i],label='validation')\n",
        "\tplt.legend()\n",
        "\tplt.show()\n",
        "\n",
        "np.save(os.path.join(output_folder,'train_loss_hist.npy'),train_loss_hist)\n",
        "np.save(os.path.join(output_folder,'val_loss_hist.npy'),val_loss_hist)\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsnlWK26_p8S"
      },
      "source": [
        "### Model training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BSHhhi8__p8S"
      },
      "outputs": [],
      "source": [
        "def plot_lr(history):\n",
        "    learning_rate = history.history['lr']\n",
        "    epochs = range(1, len(learning_rate) + 1)\n",
        "    plt.plot(epochs, learning_rate)\n",
        "    plt.title('Learning rate')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 14003, 64, 64, 3)\n"
          ]
        }
      ],
      "source": [
        "print(images_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float64\n",
            "Feature batch shape: torch.Size([64, 3, 64, 64])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Invalid shape (3, 64, 64) for image data",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m img = train_features[\u001b[32m0\u001b[39m].squeeze()\n\u001b[32m      7\u001b[39m label = train_labels[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgray\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m plt.show()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aiden\\OneDrive\\Documents\\PVOutputPrediction\\pytorch_env\\Lib\\site-packages\\matplotlib\\pyplot.py:3601\u001b[39m, in \u001b[36mimshow\u001b[39m\u001b[34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[39m\n\u001b[32m   3579\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.imshow)\n\u001b[32m   3580\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimshow\u001b[39m(\n\u001b[32m   3581\u001b[39m     X: ArrayLike | PIL.Image.Image,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3599\u001b[39m     **kwargs,\n\u001b[32m   3600\u001b[39m ) -> AxesImage:\n\u001b[32m-> \u001b[39m\u001b[32m3601\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[43m=\u001b[49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3606\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3607\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3611\u001b[39m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3613\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3617\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3618\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3619\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3620\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3621\u001b[39m     sci(__ret)\n\u001b[32m   3622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aiden\\OneDrive\\Documents\\PVOutputPrediction\\pytorch_env\\Lib\\site-packages\\matplotlib\\__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aiden\\OneDrive\\Documents\\PVOutputPrediction\\pytorch_env\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:5979\u001b[39m, in \u001b[36mAxes.imshow\u001b[39m\u001b[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[39m\n\u001b[32m   5976\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5977\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_aspect(aspect)\n\u001b[32m-> \u001b[39m\u001b[32m5979\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5980\u001b[39m im.set_alpha(alpha)\n\u001b[32m   5981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5982\u001b[39m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aiden\\OneDrive\\Documents\\PVOutputPrediction\\pytorch_env\\Lib\\site-packages\\matplotlib\\image.py:685\u001b[39m, in \u001b[36m_ImageBase.set_data\u001b[39m\u001b[34m(self, A)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL.Image.Image):\n\u001b[32m    684\u001b[39m     A = pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28mself\u001b[39m._A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28mself\u001b[39m._imcache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aiden\\OneDrive\\Documents\\PVOutputPrediction\\pytorch_env\\Lib\\site-packages\\matplotlib\\image.py:653\u001b[39m, in \u001b[36m_ImageBase._normalize_image_array\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m    651\u001b[39m     A = A.squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A.shape[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for image data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m A.ndim == \u001b[32m3\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[32m    657\u001b[39m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[32m    658\u001b[39m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[32m    659\u001b[39m     high = \u001b[32m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.issubdtype(A.dtype, np.integer) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m: Invalid shape (3, 64, 64) for image data"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_loader))\n",
        "print(train_labels.dtype)\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img.transpose(1,2,0), cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")\n",
        "\n",
        "print(img.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LwAhfKw_p8T"
      },
      "outputs": [],
      "source": [
        "# summary of training and validation results\n",
        "best_train_loss_MSE = np.zeros(num_fold)\n",
        "best_val_loss_MSE = np.zeros(num_fold)\n",
        "\n",
        "for i in range(num_fold):\n",
        "    best_val_loss_MSE[i] = np.min(val_loss_hist[i])\n",
        "    idx = np.argmin(val_loss_hist[i])\n",
        "    best_train_loss_MSE[i] = train_loss_hist[i][idx]\n",
        "    print('Model {0}  -- train loss: {1:.2f}, validation loss: {2:.2f} (RMSE)'.format(i+1, np.sqrt(best_train_loss_MSE[i]), np.sqrt(best_val_loss_MSE[i])))\n",
        "print('The mean train loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_train_loss_MSE))))\n",
        "print('The mean validation loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_val_loss_MSE))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZAaBHcf_p8T"
      },
      "source": [
        "### Model Testing with in-range Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk4EcIxw_p8T"
      },
      "source": [
        "Test set data are 10 cloudy days drawn from 2017 March to 2019 October, which is the same range as the training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OswWAxd__p8T"
      },
      "source": [
        "#### Feeding Real Future Sky Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSp4kqnq_p8T"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "with h5py.File(data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    images_log_test = f['test']['images_pred'][:,-1]\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # normalize image data to [0,1]\n",
        "    images_data_test = tf.image.convert_image_dtype(images_log_test, dtype=tf.float32).numpy()\n",
        "    images_data_test = mask_background(images_data_test)\n",
        "    pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"images_data_test.shape:\",images_data_test.shape)\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL3SWHlH_p8T",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=images_data_test, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(images_data_test, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Dje5maZ3_p8T"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_validation.npy'))\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "prediction_ensemble = np.mean(prediction,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3s4NKDb_p8T"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = [] # you can put sunny days here if you want to test sunny condition performance\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask for the sunny days\n",
        "mask = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask to the dataset\n",
        "times_test_sunny = times_test[mask]\n",
        "pv_log_test_sunny = pv_log_test[mask]\n",
        "images_log_test_sunny = images_log_test[mask]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask]\n",
        "pv_log_test_cloudy = pv_log_test[~mask]\n",
        "images_log_test_cloudy = images_log_test[~mask]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZsT6lgr_p8T"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdK5Sg62_p8U"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GLR9qiFV_p8U"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MpEgnaq_p8U",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_ensembles\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1, color=black, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1, label = 'SUNSET forecast',color=red,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.2], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(15,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','sunset_forecast_baseline_2017_2019_full_data_trained_2019_test_days.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNPhFObc_p8U"
      },
      "source": [
        "#### Feeding ConvLSTM generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-jP3CLj_p8U"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_ConvLSTM.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images = f['trainval']['images_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    predicted_images = mask_background(predicted_images)\n",
        "    predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images.shape:\",predicted_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlAETbt6_p8U",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_ConvLSTM_gen_images_as_input.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36nVbFOf_p8U"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x01nPo8_p8V"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcG5HHp9_p8V"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FfsTIkKC_p8V"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfdfRmoz_p8V"
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq_e_2FR_p8V"
      },
      "source": [
        "#### Feeding PhyDNet generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycgkr6AC_p8V"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_PhyDNet.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images = f['trainval']['images_pred'][:,-1]\n",
        "    #pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    predicted_images = mask_background(predicted_images)\n",
        "    predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "    #pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images.shape:\",predicted_images.shape)\n",
        "#print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHvi4O3H_p8V",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_PhyDNet_gen_images_as_input.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmZhxOkn_p8W"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLOjQQVX_p8W"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtv2MJzp_p8W"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pPzLgVBe_p8W"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvpdMdcH_p8X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDPatyhe_p8X"
      },
      "source": [
        "#### Feeding PhyDNet+GAN generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRLjvY8p_p8X"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_PhyDNetGAN.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images = f['trainval']['images_pred'][:,-1]\n",
        "    #pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    predicted_images = mask_background(predicted_images)\n",
        "    predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "    #pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images.shape:\",predicted_images.shape)\n",
        "#print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo2QrBoK_p8X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_SkyImageGAN_gen_images_as_input.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-CtFqgs_p8X"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAcy-OnC_p8X"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xaIL0OF_p8X"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(5times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e2DW7bsl_p8Y"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yehVXuSY_p8Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euCa3dq4_p8Y"
      },
      "source": [
        "#### Feeding VideoGPT generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz51jq6o_p8Y"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_VideoGPT.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images_all_samplings = f['trainval']['images_pred_all_samplings'][:,:,-1]\n",
        "    #pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    #predicted_images = mask_background(predicted_images)\n",
        "    #predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "    #pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images_all_samplings.shape:\",predicted_images_all_samplings.shape)\n",
        "#print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QmYOerN_p8Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "num_samplings = 10\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating prediction for sampling {0}'.format(j+1))\n",
        "        predicted_images = predicted_images_all_samplings[j]\n",
        "        predicted_images = mask_background(predicted_images)\n",
        "        predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\".format(i+1))\n",
        "        loss[i,j] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\".format(i+1))\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_VideoGPT_gen_images_as_input.npy'),prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2dFvG4oM_p8Y"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_validation_VideoGPT_gen_images_as_input.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Uv6F9JUH_p8Y"
      },
      "outputs": [],
      "source": [
        "prediction_samp = prediction.reshape(-1,prediction.shape[-1])\n",
        "prediction_samp.shape\n",
        "prediction_ensemble = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1uFkx0z_p8Y"
      },
      "outputs": [],
      "source": [
        "# evaluate deterministic performance\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print('test rmse for all 10 samplings is {0:3f}'.format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulkHszae_p8Z"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOxglfrh_p8Z"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmeZvVgj_p8Z"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "trw4x5Yn_p8Z"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction_samp,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwtUNZqy_p8Z",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w22mxO_3_p8a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMlChrkG_p8a"
      },
      "source": [
        "#### Feeding SkyGPT generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3TOUx5R_p8a"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_SkyGPT.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images_all_samplings = f['trainval']['images_pred_all_samplings'][:,:,-1]\n",
        "\n",
        "print(\"predicted_images_all_samplings.shape:\",predicted_images_all_samplings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7ULQajn_p8a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "num_samplings = 10\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating prediction for sampling {0}'.format(j+1))\n",
        "        predicted_images = predicted_images_all_samplings[j]\n",
        "        #predicted_images = mask_background(predicted_images)\n",
        "        predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\".format(i+1))\n",
        "        loss[i,j] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\".format(i+1))\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_SkyGPT_gen_images_as_input.npy'),prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lpE334pk_p8a"
      },
      "outputs": [],
      "source": [
        "prediction_samp = prediction.reshape(-1,prediction.shape[-1])\n",
        "prediction_samp.shape\n",
        "prediction_ensemble = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQJYOJi7_p8a"
      },
      "outputs": [],
      "source": [
        "# evaluate deterministic performance\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print('test rmse for all 10 samplings is {0:3f}'.format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFR3WwlW_p8b"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQYAeU7i_p8b"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfkF3m37_p8b"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "me_wW9KL_p8b"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction_samp,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLZ_hSLf_p8b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRG-wXH_p8b"
      },
      "source": [
        "### Model Testing with Out-range test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9m5Bm7j_p8b"
      },
      "source": [
        "Test set data are 5 cloudy days drawn from 2019 Nov. and Dec., which is the outside the range of the training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJWhoL-Y_p8b"
      },
      "source": [
        "#### Feeding Real Future Sky Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpFAG13X_p8b"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "data_folder = os.path.join(pardir,\"data\")\n",
        "test_data_path = os.path.join(data_folder,'test_set_2019nov_dec.hdf5')\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test_2019nov_dec.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "model_name = 'UNet_Image_PV_mapping_masked_image'\n",
        "output_folder = os.path.join(cwd,\"model_output\", model_name)\n",
        "\n",
        "with h5py.File(test_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    images_log_test = f['test']['images_pred'][:,-1]\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    images_log_test = mask_background(images_log_test)\n",
        "    images_log_test = tf.image.convert_image_dtype(images_log_test, dtype=tf.float32).numpy()\n",
        "    pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"images_log_test.shape:\",images_log_test.shape)\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)\n",
        "#print(\"pv_pred_test.shape:\",pv_pred_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nnp4gYd6_p8c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=images_log_test, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(images_log_test, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_2019nov_dec.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofk4Ao4O_p8c"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = [] # you can put sunny days here if you want to test sunny condition performance\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "prediction_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "prediction_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFwQErvz_p8c"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqmLX5WY_p8c"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YckeyUHA_p8c"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKifyK5S_p8c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(15,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL_m4edv_p8c"
      },
      "source": [
        "#### Feeding ConvLSTM Generated Images as Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z7M2bVwr_p8c"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','ConvLSTM','save','ConvLSTM_sky_image_dataset_interval_2min_all_data_v2')\n",
        "predicted_image = np.load(os.path.join(predicted_image_folder,'predicted_images_new_test_set_2019.npy'),allow_pickle=True)\n",
        "predicted_image = predicted_image[:,-1]\n",
        "predicted_image = mask_background(predicted_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwO-l2lk_p8d"
      },
      "outputs": [],
      "source": [
        "predicted_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_iL-ySA_p8d"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "data_folder = os.path.join(pardir,\"data\")\n",
        "test_data_path = os.path.join(data_folder,'test_set_2019nov_dec.hdf5')\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test_2019nov_dec.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "with h5py.File(test_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    images_log_test = f['test']['images_pred'][:,-1]\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "# process image data\n",
        "images_log_test = mask_background(images_log_test)\n",
        "images_log_test = tf.image.convert_image_dtype(images_log_test, dtype=tf.float32).numpy()\n",
        "pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"images_log_test.shape:\",images_log_test.shape)\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)\n",
        "#print(\"pv_pred_test.shape:\",pv_pred_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AGtQz1I_p8d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_image, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_image, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_ConvLSTM_generated_imgs_as_input_new.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "#print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the model\".format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzj28_aA_p8d"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "#pv_pred_test_sunny = pv_pred_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "images_log_test_sunny = images_log_test[mask3]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "images_log_test_cloudy = images_log_test[~mask3]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEfj1S43_p8d"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJRoSa9Q_p8d"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kZpcwXP4_p8d"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08nqCXSE_p8e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoWlyEi_p8e"
      },
      "source": [
        "#### Feeding PhyDNet Generated Images as Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVS6JX7w_p8e"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','PhyDNet','save','PhyDNet_sky_image_dataset_interval_2min_all_data_v2')\n",
        "predicted_image = np.load(os.path.join(predicted_image_folder,'predicted_images_new_test_set_2019.npy'),allow_pickle=True)\n",
        "predicted_image = mask_background(predicted_image)\n",
        "predicted_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az1maKm7_p8f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_PhyDNet_generated_imgs_as_input_new.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "#print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the model\".format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBq3ae4k_p8f"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "#pv_pred_test_sunny = pv_pred_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "images_log_test_sunny = images_log_test[mask3]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "images_log_test_cloudy = images_log_test[~mask3]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyyW-4hi_p8f"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATkN2joW_p8f"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w46dZibj_p8g"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPSLKBMk_p8g",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVq3w-Ju_p8g"
      },
      "source": [
        "#### Feeding PhyDNet+GAN Generated Images as Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "varbMhwQ_p8g"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','PhyDNet','save','PhyDNet_LSGAN_sky_image_dataset_gen_lr_0.001_batch_size_16_model_v2_scheduled_and_reverse_scheduled_sampling_MAE_loss_all_data_v3')\n",
        "predicted_image = np.load(os.path.join(predicted_image_folder,'predicted_images_new_test_set_2019.npy'),allow_pickle=True)\n",
        "predicted_image = mask_background(predicted_image)\n",
        "predicted_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvTJkS7h_p8g"
      },
      "outputs": [],
      "source": [
        "plt.imshow(predicted_image[0,0,:,:,::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEhT2y24_p8g",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_SkyImageGAN_generated_imgs_as_input_new.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "#print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the model\".format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VCFNicH_p8g"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "#pv_pred_test_sunny = pv_pred_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "images_log_test_sunny = images_log_test[mask3]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "images_log_test_cloudy = images_log_test[~mask3]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krH_iCwC_p8g"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR-fqpty_p8g"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B04lOJ3b_p8h"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWzpTqDv_p8h",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiS3fR8B_p8h"
      },
      "source": [
        "#### Feeding VideoGPT Generated Images as Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmBGQhME_p8h"
      },
      "source": [
        "We experiment with different number of futures generated, ranging from 1 future to 50 different futures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "t6QGhFq-_p8h"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','VideoGPT','inference','VideoGPT_2019_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L-YRRCW_p8h",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "num_samplings = 50\n",
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating predictions for sample {0} ...'.format(j+1))\n",
        "        # load predicted image samples\n",
        "        predicted_image = np.load(os.path.join(predicted_image_folder,'sample_'+str(j)+'.npy'),allow_pickle=True)\n",
        "        print(np.min(predicted_image))\n",
        "        print(np.max(predicted_image))\n",
        "        predicted_image = mask_background(predicted_image)\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\")\n",
        "        loss[i,j] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=200, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\")\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=200, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_VideoGPT_generated_imgs_as_input_new.npy'),prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4XdtHj6_p8i"
      },
      "source": [
        "##### Use 1 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwf_O5MB_p8i"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "data_folder = os.path.join(pardir,\"data\")\n",
        "test_data_path = os.path.join(data_folder,'test_set_2019nov_dec.hdf5')\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test_2019nov_dec.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "with h5py.File(test_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbigmsWF_p8i"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_VideoGPT_generated_imgs_as_input_new.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XF75qjU_p8i"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 1\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcqUIX-u_p8i"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 1 sampling\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 1 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbXULns0_p8i"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Yoy5Pn_p8i"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "91no2zm5_p8j"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nXzfRtiu_p8j"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnKL5FFZ_p8j",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-G8BjEd_p8j"
      },
      "source": [
        "##### Use 5 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ3vI-yQ_p8j"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_VideoGPT_generated_imgs_as_input_new.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr8f9qGe_p8j"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 5\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d-Lksd1_p8j"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_5samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 5 samplings for all sub-models\".format(loss_rmse_5samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZTL-JLV_p8j"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZNRQLFR_p8j"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o7_MDTye_p8j"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9qE7Nym_p8j"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q3Y0u9Ul_p8j"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrlddXqu_p8j",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rQmZO8Rq_p8j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLW3Lcj1_p8j"
      },
      "source": [
        "##### Use 10 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ogWopQM_p8j"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 10\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h62apQX__p8j"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 10 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOVew5LH_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-M_TPE1_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N4C8MJ_i_p8k"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhgUnX5F_p8k"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xCT1VXSb_p8k"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx2HwoW1_p8k",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLXrigmU_p8k"
      },
      "source": [
        "##### Use 20 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMAORW_F_p8k"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 20\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ZhH64Z_p8k"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtV9-eBV_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4vXpldz_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jNwZr80L_p8k"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wUwvPJ86_p8k"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTXLHkZW_p8l",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7uyBh5h_p8l"
      },
      "source": [
        "##### Use 30 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFZ87K_H_p8l"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 30\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUpOdy9R_p8l"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXGxdAf9_p8l"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSHqbdSr_p8l"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N8SuTeN2_p8l"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HIreTsgZ_p8l"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKxDi0S-_p8l",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPbcY--H_p8m"
      },
      "source": [
        "##### Use 40 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc9SiTNA_p8m"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 40\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0ib2iwR_p8m"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDX3_x4m_p8m"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EStvMzf7_p8m"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sa54VvIN_p8m"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X8vRQ-Kc_p8m"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjeOMhbC_p8m",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68hZ1P5q_p8m"
      },
      "source": [
        "##### Use 50 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy_P-pMp_p8m"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 50\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqeoJNE9_p8m"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BESrzj3h_p8m"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ockrRjCS_p8n"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ly0NhQmz_p8n"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KjtC-3Wt_p8n"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve1vob9T_p8n",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnxqk2Xo_p8n"
      },
      "source": [
        "#### Feeding SkyGPT Generated Images as Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2ECEckA_p8n"
      },
      "source": [
        "Similarly, we experimented with different number of generated future scenarios, ranging from 1 future to 50 futures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jVdXuW24_p8n"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','SkyGPT','inference','SkyGPT_2019_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fol6pypp_p8n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "num_samplings = 50\n",
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating predictions for sample {0} ...'.format(j+1))\n",
        "        # load predicted image samples\n",
        "        predicted_image = np.load(os.path.join(predicted_image_folder,'sample_'+str(j)+'.npy'),allow_pickle=True)\n",
        "\n",
        "        # scale back all the pixel values back to [0,1] with clipping\n",
        "        predicted_image = np.clip(predicted_image,-0.5,0.5)+0.5\n",
        "        predicted_image = mask_background(predicted_image)\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\")\n",
        "        loss[i,j] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=200, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\")\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=200, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_SkyGPT_generated_imgs_as_input_new.npy'),prediction)\n",
        "    # loss averaged over all samplings for model 1\n",
        "    #loss_rmse = np.sqrt(np.mean((np.mean(prediction[i],axis=0)-pv_log_test)**2))\n",
        "    #print(\"the test set RMSE is {0:.3f} averaged over all samplings for sub-model {1}\".format(loss_rmse,i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvB2sy83_p8n"
      },
      "source": [
        "##### Use 1 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHBJYwRy_p8n"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_SkyGPT_generated_imgs_as_input_new.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLsV9eVr_p8n"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 1\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uC4V7Ao_p8n"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_1samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 1 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGHK6VK-_p8n"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftcd0ue3_p8n"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TN6IsPsq_p8n"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WrcHKV3_p8n"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tPwCo0cs_p8n"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9hE6iuu_p8n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    #rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_samp[date_mask]))))\n",
        "    #mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_samp[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_samp[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp5jEquj_p8o"
      },
      "source": [
        "##### Use 5 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SblfEftx_p8o"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_SkyGPT_generated_imgs_as_input.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKqHmSqr_p8o"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 5\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqOQs1X7_p8o"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_5samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 5 samplings for all sub-models\".format(loss_rmse_5samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDq4LRoH_p8o"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQG2c2e5_p8o"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mPTdMKBC_p8o"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U1EQBEB_p8o"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Va9bzY5e_p8o"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB7xgu5X_p8o",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Mq1rKGCa_p8p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JGt7rs_p8p"
      },
      "source": [
        "##### Use 10 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtCC9__O_p8p"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 10\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX50RwJ__p8p"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 10 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1PLMMsb_p8p"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--n4GntH_p8p"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ke23VcUu_p8p"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA0qcqAM_p8p"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BzLCKS57_p8p"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOa5arbH_p8p",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5qdjmr3_p8q"
      },
      "source": [
        "##### Use 20 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvQd7cC5_p8q"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 20\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhuffEVx_p8q"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYz9hUS6_p8q"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAUYhDir_p8q"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ARCgvwu5_p8q"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0yHZC6wX_p8q"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6QpwUnn_p8q",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    #rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_samp[date_mask]))))\n",
        "    #mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_samp[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_samp[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhE4FDzj_p8q"
      },
      "source": [
        "##### Use 30 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3jeKgVe_p8q"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 30\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHdbuMtI_p8q"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNWRUW4G_p8q"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6CuhuJj_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hCS1lPS8_p8r"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vI1R08r7_p8r"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZXYOYjA_p8r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxbODDoH_p8r"
      },
      "source": [
        "##### Use 40 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq1tRNV5_p8r"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 40\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejycpmjd_p8r"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67vLci5I_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDogRw7z_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0MOtUcwV_p8r"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-8YwfkbX_p8r"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFkRDXVv_p8r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsIqEMww_p8r"
      },
      "source": [
        "##### Use 50 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTjUtFh__p8r"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 50\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvLZN-mG_p8r"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW3xLonf_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSKeYWe4_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1AKSqNVr_p8r"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-19KtFuX_p8r"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLGinsua_p8r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
