{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Fu3OT8EoaUOE"
   },
   "outputs": [],
   "source": [
    "# import packages \n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# folder path and name\n",
    "project_path = os.getcwd()\n",
    "data_folder = os.path.join(os.getcwd(),'data')\n",
    "pred_folder = os.path.join(data_folder,'data_forecast')\n",
    "pv_data_path = os.path.join(data_folder,'pv_data','pv_output_valid.pkl')\n",
    "\n",
    "image_name_format = '%Y%m%d%H%M%S'\n",
    "\n",
    "# Operating parameter\n",
    "stack_height = 15 # 15 minute\n",
    "forecast_horizon = 15 # 15 minutes ahead forecast\n",
    "forecast_horizons = [15, 30, 45, 60]\n",
    "sampling_interval_all = [2]\n",
    "output_img_shape = [224, 224, 3]\n",
    "\n",
    "start_date = datetime.datetime(2017,1,1) #NOTE: Inclusive of start date\n",
    "end_date = datetime.datetime(2018,1,1) #NOTE: Exclusive of end date (only end up with 2017 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up test set\n",
    "sunny_day = [(2017,9,15),(2017,10,6),(2017,10,22),(2018,2,16),(2018,6,12),(2018,6,23),(2019,1,25),(2019,6,23),(2019,7,14),(2019,10,14)]\n",
    "cloudy_day = [(2017,6,24),(2017,9,20),(2017,10,11),(2018,1,25),(2018,3,9),(2018,10,4),(2019,5,27),(2019,6,28),(2019,8,10),(2019,10,19)]\n",
    "\n",
    "sunny_datetime = [datetime.datetime(day[0],day[1],day[2]) for day in sunny_day]\n",
    "cloudy_datetime = [datetime.datetime(day[0],day[1],day[2]) for day in cloudy_day]\n",
    "test_dates = sunny_datetime + cloudy_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2017, 9, 15, 0, 0),\n",
       " datetime.datetime(2017, 10, 6, 0, 0),\n",
       " datetime.datetime(2017, 10, 22, 0, 0),\n",
       " datetime.datetime(2018, 2, 16, 0, 0),\n",
       " datetime.datetime(2018, 6, 12, 0, 0),\n",
       " datetime.datetime(2018, 6, 23, 0, 0),\n",
       " datetime.datetime(2019, 1, 25, 0, 0),\n",
       " datetime.datetime(2019, 6, 23, 0, 0),\n",
       " datetime.datetime(2019, 7, 14, 0, 0),\n",
       " datetime.datetime(2019, 10, 14, 0, 0),\n",
       " datetime.datetime(2017, 6, 24, 0, 0),\n",
       " datetime.datetime(2017, 9, 20, 0, 0),\n",
       " datetime.datetime(2017, 10, 11, 0, 0),\n",
       " datetime.datetime(2018, 1, 25, 0, 0),\n",
       " datetime.datetime(2018, 3, 9, 0, 0),\n",
       " datetime.datetime(2018, 10, 4, 0, 0),\n",
       " datetime.datetime(2019, 5, 27, 0, 0),\n",
       " datetime.datetime(2019, 6, 28, 0, 0),\n",
       " datetime.datetime(2019, 8, 10, 0, 0),\n",
       " datetime.datetime(2019, 10, 19, 0, 0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fwRci7HpaUOx"
   },
   "outputs": [],
   "source": [
    "def find_idx_with_dates(all_times,test_dates):\n",
    "    idx=[]\n",
    "    for test_day in test_dates:\n",
    "        test_day_end = test_day + datetime.timedelta(days = 1)\n",
    "        idx+=np.nonzero((all_times>test_day)*(all_times<test_day_end))[0].tolist()\n",
    "    return idx\n",
    "\n",
    "# This two function does the same thing. Just that one is for np, the other for pd.\n",
    "\n",
    "def find_time_within_nparray(time_array,time_point):\n",
    "    probable_idx = np.searchsorted(time_array,time_point)\n",
    "    \n",
    "    # If the time point is after all the time in pv_data\n",
    "    if probable_idx == len(time_array):\n",
    "        return None   \n",
    "    \n",
    "    # See if the time point is actually a match \n",
    "    if time_array[probable_idx]== time_point: \n",
    "        return probable_idx\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_time_within_pdseries(time_array,time_point):\n",
    "    probable_idx = np.searchsorted(time_array,time_point)\n",
    "    \n",
    "    # If the time point is after all the time in pv_data\n",
    "    if probable_idx == len(time_array):\n",
    "        return None   \n",
    "    \n",
    "    # See if the time point is actually a match \n",
    "    if time_array[probable_idx] == time_point: \n",
    "        return probable_idx\n",
    "        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RPtJ7Nw-aUPC"
   },
   "outputs": [],
   "source": [
    "def store_trainval_test(all_times,image_log,pv_log,pv_pred,pred_folder):\n",
    "    \n",
    "    ## Splitting into Trainval and Test set \n",
    "    idx_test = find_idx_with_dates(all_times,test_dates)\n",
    "    image_log_test = image_log[idx_test]\n",
    "    pv_log_test = pv_log[idx_test]\n",
    "    pv_pred_test = pv_pred[idx_test]\n",
    "    times_test = all_times[idx_test]\n",
    "\n",
    "    # the rest become the trainval set\n",
    "    mask_trainval = np.ones_like(pv_pred,dtype = bool)\n",
    "    mask_trainval[idx_test] = 0\n",
    "    image_log_trainval = image_log[mask_trainval]\n",
    "    pv_log_trainval = pv_log[mask_trainval]\n",
    "    pv_pred_trainval = pv_pred[mask_trainval]\n",
    "    times_trainval = all_times[mask_trainval]\n",
    "    \n",
    "    print(\"times_trainval.shape\",times_trainval.shape)\n",
    "    print(\"image_log_trainval.shape\",image_log_trainval.shape)\n",
    "    print(\"pv_log_trainval.shape\",pv_log_trainval.shape)\n",
    "    print(\"pv_pred_trainval.shape\",pv_pred_trainval.shape)\n",
    "    \n",
    "    print(\"times_test.shape\",times_test.shape)\n",
    "    print(\"image_log_test.shape\",image_log_test.shape)\n",
    "    print(\"pv_log_test.shape\",pv_log_test.shape)\n",
    "    print(\"pv_pred_test.shape\",pv_pred_test.shape)\n",
    "    \n",
    "    ## Storing information\n",
    "    # storing the training set\n",
    "    np.save(os.path.join(pred_folder,'image_log_trainval.npy'), image_log_trainval)\n",
    "    np.save(os.path.join(pred_folder,'pv_log_trainval.npy'), pv_log_trainval)\n",
    "    np.save(os.path.join(pred_folder,'pv_pred_trainval.npy'),pv_pred_trainval)\n",
    "    np.save(os.path.join(pred_folder,'times_trainval.npy'),times_trainval)\n",
    "\n",
    "    # storing the testing set\n",
    "    np.save(os.path.join(pred_folder,'image_log_test.npy'), image_log_test)\n",
    "    np.save(os.path.join(pred_folder,'pv_log_test.npy'), pv_log_test)\n",
    "    np.save(os.path.join(pred_folder,'pv_pred_test.npy'),pv_pred_test)\n",
    "    np.save(os.path.join(pred_folder,'times_test.npy'),times_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QH5ewDbBaUPT"
   },
   "outputs": [],
   "source": [
    "# Load in  high frequency data\n",
    "# the image here are ones that have corresponding PV value\n",
    "all_times = np.load(os.path.join(data_folder,'data_expanded','all_times_highfreq.npy'), allow_pickle=True)\n",
    "all_images = np.load(os.path.join(data_folder,'data_expanded','all_images_highfreq.npy'), allow_pickle=True)\n",
    "pv_data = np.load(pv_data_path, allow_pickle=True)\n",
    "#pv_data = np.load(os.path.join(data_folder,'data_expanded','pv_outputs_highfreq.npy'), allow_pickle=True)\n",
    "\n",
    "# only pick out the relevant time period\n",
    "#relevant_mask = (all_times>=start_date)&(all_times<end_date)\n",
    "#all_times = all_times[relevant_mask]\n",
    "#all_images = all_images[relevant_mask]\n",
    "#pv_data = pv_data[start_date:end_date]\n",
    "idx_test = find_idx_with_dates(all_times,test_dates)\n",
    "n_images = all_times.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-03 08:14:20    0.010697\n",
      "2017-01-03 08:14:30    0.023494\n",
      "2017-01-03 08:14:40    0.036292\n",
      "2017-01-03 08:14:50    0.049089\n",
      "2017-01-03 08:15:00    0.061887\n",
      "                         ...   \n",
      "2019-10-26 17:58:40    0.020961\n",
      "2019-10-26 17:58:50    0.015540\n",
      "2019-10-26 17:59:00    0.010119\n",
      "2019-10-26 17:59:10    0.005942\n",
      "2019-10-26 17:59:20    0.001765\n",
      "Length: 3887473, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(pv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363375\n"
     ]
    }
   ],
   "source": [
    "print(n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subarray_between_values(arr, a, b):\n",
    "    \"\"\"\n",
    "    Returns a subarray containing elements from 'arr' that are\n",
    "    greater than 'a' and less than 'b'.\n",
    "\n",
    "    Args:\n",
    "        arr (numpy.ndarray): The input NumPy array.\n",
    "        a (int or float): The lower bound (exclusive).\n",
    "        b (int or float): The upper bound (exclusive).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new NumPy array containing the filtered elements.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask where elements are greater than 'a'\n",
    "    mask_greater_than_a = arr >= a\n",
    "\n",
    "    # Create a boolean mask where elements are less than 'b'\n",
    "    mask_less_than_b = arr < b\n",
    "\n",
    "    # Combine the masks using the logical AND operator\n",
    "    # An element must satisfy both conditions to be included\n",
    "    combined_mask = mask_greater_than_a & mask_less_than_b\n",
    "\n",
    "    # Use the combined mask to select the elements from the original array\n",
    "    subarray = arr[combined_mask]\n",
    "\n",
    "    return subarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6595,
     "status": "ok",
     "timestamp": 1599952937588,
     "user": {
      "displayName": "Andea Jewel Scott",
      "photoUrl": "",
      "userId": "15144577752603562183"
     },
     "user_tz": 420
    },
    "id": "DNaDTYlsaUPk",
    "outputId": "7ca39499-ecc0-4c68-ea22-62066a383436",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new datasets\n",
      "processed 8000/363375 images\n",
      "storing data\n",
      "num_valid_trainval:  3098\n",
      "num_valid_test:  0\n",
      "stored trainval image log\n",
      "stored trainval pv log\n",
      "stored trainval pv pred\n",
      "stored test image log\n",
      "For sampling frequency:  2  minutes\n"
     ]
    }
   ],
   "source": [
    "# Create forecast training data file\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "#mmap_array = np.memmap('video_prediction_224.dat', dtype='uint8', mode='w+', shape=(n_images, 224, 224, 3))\n",
    "sampling_interval = 2\n",
    "\n",
    "n_valid = 307518 # index 307517 is the last valid index\n",
    "n_trainval = 8000\n",
    "#n_trainval = 149680\n",
    "n_test = 6145\n",
    "chunk_shape = (1, stack_height+1, *output_img_shape)\n",
    "\n",
    "batch_size = 8000\n",
    "#for sampling_interval in sampling_interval_all:\n",
    "resume_idx = 0\n",
    "with h5py.File('video_prediction_224_testing.h5', 'w') as f:\n",
    "\n",
    "\t#image_log = f.create_dataset('image_log', shape = (n_images, 224, 224, 3), dtype ='uint8', )\n",
    "\n",
    "\tif resume_idx and 'image_log' in f:\n",
    "        # Resume mode - datasets already exist\n",
    "\t\timage_log_ds = f['image_log']\n",
    "\t\tpv_log_ds = f['pv_log'] \n",
    "\t\tpv_pred_ds = f['pv_pred']\n",
    "\t\tprint(f\"Resuming from existing datasets. Current size: {image_log_ds.shape[0]}\")\n",
    "\telse:\n",
    "        # First run - create new datasets\n",
    "\n",
    "\t\ttrainval_group = f.create_group('trainval')\n",
    "\t\ttest_group = f.create_group('test')\n",
    "\t\timage_log_trainval_ds = trainval_group.create_dataset(\n",
    "\t\t\t'image_log',\n",
    "\t\t\tshape=(n_trainval, stack_height+1, *output_img_shape),\n",
    "\t\t\tchunks=chunk_shape,\n",
    "\t\t\tcompression=hdf5plugin.Blosc2(cname='zstd', clevel=1, filters=1),\n",
    "\t\t\tdtype='uint8'\n",
    "\t\t)\n",
    "\t\tpv_log_trainval_ds = trainval_group.create_dataset(\n",
    "\t\t\t'pv_log',\n",
    "\t\t\tshape=(n_trainval, stack_height+1),\n",
    "\t\t\tdtype='float64'\n",
    "\t\t)\n",
    "\t\tpv_pred_trainval_ds = trainval_group.create_dataset(\n",
    "\t\t\t'pv_pred',\n",
    "\t\t\tshape=(n_trainval,),\n",
    "\t\t\tdtype='float64'\n",
    "\t\t)\n",
    "\t\timage_log_test_ds = test_group.create_dataset(\n",
    "\t\t\t'image_log',\n",
    "\t\t\tshape=(n_test, stack_height+1, *output_img_shape),\n",
    "\t\t\tchunks=chunk_shape,\n",
    "\t\t\tcompression=hdf5plugin.Blosc2(cname='zstd', clevel=1, filters=1),\n",
    "\t\t\tdtype='uint8'\n",
    "\t\t)\n",
    "\t\tpv_log_test_ds = test_group.create_dataset(\n",
    "\t\t\t'pv_log',\n",
    "\t\t\tshape=(n_test, stack_height+1),\n",
    "\t\t\tdtype='float64'\n",
    "\t\t)\n",
    "\t\tpv_pred_test_ds = test_group.create_dataset(\n",
    "\t\t\t'pv_pred',\n",
    "\t\t\tshape=(n_test,),\n",
    "\t\t\tdtype='float64'\n",
    "\t\t)\n",
    "\t\tprint(\"Creating new datasets\")\n",
    "\n",
    "\t\n",
    "\t#image_log = np.empty([0,stack_height+1]+output_img_shape,dtype = 'uint8')\n",
    "\t#pv_log = np.empty([0, stack_height+1])\n",
    "\t#pv_pred = np.empty([0])\n",
    "\n",
    "\tlast_valid_index = 0\n",
    "\tcurr_trainval_size = 0\n",
    "\tcurr_test_size = 0\n",
    "\n",
    "\ttic = time.process_time()\n",
    "\tfor b in range(resume_idx if (resume_idx and 'image_log' in f) else 0, 8000, batch_size):\n",
    "\t\tcurrent_batch_size =  min(batch_size, n_images-b)\n",
    "\t\t# Initialize variables to save pv values\n",
    "\t\t#image_log_batch = np.zeros([n_images,stack_height+1]+output_img_shape,dtype = 'uint8')\n",
    "\t\tidx_test_batch = get_subarray_between_values(np.asarray(idx_test),b,b+current_batch_size)\n",
    "\n",
    "\t\timage_log_batch = np.zeros([current_batch_size,stack_height+1]+output_img_shape,dtype = 'uint8')\n",
    "\t\t\n",
    "\t\t#all_times_batch = all_times[b*batch_size : b*batch_size + current_batch_size]\n",
    "\t\tpv_log_batch = np.zeros((current_batch_size,stack_height+1))\n",
    "\t\tpv_pred_batch = np.zeros(current_batch_size)\n",
    "\t\tvalidity_mask = np.ones(current_batch_size,dtype = bool)\n",
    "\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tsampling_interval_td = datetime.timedelta(minutes = sampling_interval) - datetime.timedelta(seconds=1)\n",
    "\t\tfor i in range(current_batch_size):\n",
    "\t\t\tcount = b+i\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t# See if the specified sampling frequency is met \n",
    "\t\t\tif all_times[count] - all_times[last_valid_index] > sampling_interval_td:\n",
    "\n",
    "\t\t\t\t# Collecting groud truth for predicted value\n",
    "\t\t\t\tpred_time = all_times[count]+datetime.timedelta(minutes=forecast_horizon)\n",
    "\t\t\t\t\n",
    "\t\t\t\tpv_pred_idx = find_time_within_nparray(pv_data.index,pred_time)\n",
    "\t\t\t\tif pv_pred_idx is None:# if prediction ground truth not found\n",
    "\t\t\t\t\tvalidity_mask[i] = False\n",
    "\t\t\t\t\t#print(all_times[i],'has no PV pred')\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tpv_pred_batch[i] = pv_data.iloc[pv_pred_idx] \n",
    "\n",
    "\t\t\t\t# Collecting image log and PV log\n",
    "\t\t\t\tfor j in range(stack_height+1):\n",
    "\t\t\t\t\tlog_time = all_times[count] - datetime.timedelta(minutes = j)\n",
    "\t\t\t\t\t# Collecting a stack of image\n",
    "\t\t\t\t\tlog_time_idx = find_time_within_nparray(all_times,log_time)\n",
    "\t\t\t\t\tif log_time_idx is not None:\n",
    "\t\t\t\t\t\timage_log_batch[i,j] = all_images[log_time_idx]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tvalidity_mask[i] = False\n",
    "\t\t\t\t\t\t#print(all_times[count],'has no image log')\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t\t# Collecting a stack of PV value\n",
    "\t\t\t\t\tpv_log_idx = find_time_within_nparray(pv_data.index,log_time)\n",
    "\t\t\t\t\t# Check if PV value present\n",
    "\t\t\t\t\tif pv_log_idx is None:\n",
    "\t\t\t\t\t\tvalidity_mask[i] = False\n",
    "\t\t\t\t\t\t#print(all_times[count],'has no PV log')\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\telse: \n",
    "\t\t\t\t\t\tpv_log_batch[i,j] = pv_data.iloc[pv_log_idx]    \n",
    "\n",
    "\t\t\telse: # if this is in between the sampling points, discard\n",
    "\t\t\t\tvalidity_mask[i] = False\n",
    "\t\t\t\n",
    "\t\t\tif validity_mask[i]:\n",
    "\t\t\t\tlast_valid_index = count\n",
    "\t\t\t\n",
    "\t\t# Prompt progress of current work\n",
    "\n",
    "\t\tprint('processed {0}/{1} images'.format(b+current_batch_size,len(all_times)))\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\t# Only pick out the valid time points\n",
    "\t\t#all_times_batch = all_times_batch[validity_mask]\n",
    "\t\ttest_mask = np.zeros(current_batch_size, dtype=bool)\n",
    "\t\ttest_mask[idx_test_batch-b] = True\n",
    "\t\tvalidity_test_mask = validity_mask & test_mask\n",
    "\t\ttrainval_mask = np.ones(current_batch_size, dtype=bool)\n",
    "\t\ttrainval_mask[idx_test_batch-b] = False\n",
    "\t\tvalidity_trainval_mask = validity_mask & trainval_mask\n",
    "\n",
    "\t\timage_log_trainval_batch = image_log_batch[validity_trainval_mask]\n",
    "\t\tpv_log_trainval_batch = pv_log_batch[validity_trainval_mask]\n",
    "\t\tpv_pred_trainval_batch = pv_pred_batch[validity_trainval_mask]\n",
    "\n",
    "\t\timage_log_test_batch = image_log_batch[validity_test_mask]\n",
    "\t\tpv_log_test_batch = pv_log_batch[validity_test_mask]\n",
    "\t\tpv_pred_test_batch = pv_pred_batch[validity_test_mask]\n",
    "\t\t# Store information\n",
    "\t\t\n",
    "\t\tprint(\"storing data\")\n",
    "\t\t#image_log_ds[curr_size:curr_size+validity_mask.sum()] = image_log_batch\n",
    "\t\t#pv_log_ds[curr_size:curr_size+validity_mask.sum()] = pv_log_batch\n",
    "\t\t#pv_pred_ds[curr_size:curr_size+validity_mask.sum()] = pv_pred_batch\n",
    "\n",
    "\t\tnum_valid_trainval = validity_trainval_mask.sum()\n",
    "\t\tnum_valid_test = validity_test_mask.sum()\n",
    "\t\tprint(\"num_valid_trainval: \",num_valid_trainval)\n",
    "\t\tprint(\"num_valid_test: \",num_valid_test)\n",
    "\t\timage_log_trainval_ds[curr_trainval_size:curr_trainval_size+validity_trainval_mask.sum()] = image_log_trainval_batch\n",
    "\t\t#print(\"image_log_trainval_batch: \",image_log_trainval_batch)\n",
    "\t\t#print(\"image_log_trainval_ds.shape: \",image_log_trainval_ds.shape)\n",
    "\t\tprint(\"stored trainval image log\")\n",
    "\t\tpv_log_trainval_ds[curr_trainval_size:curr_trainval_size+validity_trainval_mask.sum()] = pv_log_trainval_batch\n",
    "\t\tprint(\"stored trainval pv log\")\n",
    "\t\tpv_pred_trainval_ds[curr_trainval_size:curr_trainval_size+validity_trainval_mask.sum()] = pv_pred_trainval_batch\n",
    "\t\tprint(\"stored trainval pv pred\")\n",
    "\t\timage_log_test_ds[curr_test_size:curr_test_size+validity_test_mask.sum()] = image_log_test_batch\n",
    "\t\tprint(\"stored test image log\")\n",
    "\t\tpv_log_test_ds[curr_test_size:curr_test_size+validity_test_mask.sum()] = pv_log_test_batch\n",
    "\t\tpv_pred_test_ds[curr_test_size:curr_test_size+validity_test_mask.sum()] = pv_pred_test_batch\n",
    "\n",
    "\t\tcurr_trainval_size += num_valid_trainval\n",
    "\t\t#print(\"trainval_size: \",validity_trainval_mask.sum())\n",
    "\t\tcurr_test_size += num_valid_test\n",
    "\t\t#print(\"test_size: \",validity_test_mask.sum())\n",
    "\n",
    "\t\tprint('For sampling frequency: ',sampling_interval,' minutes')\n",
    "\t\t#print('Expected finishing time:', datetime.datetime.now()+\n",
    "\t\t#\t\tdatetime.timedelta(seconds = (time.process_time() - tic)*(len(all_times)/(b+batch_size))))\n",
    "\t\tf.flush()\n",
    "\t\tdel image_log_trainval_batch\n",
    "\t\tdel pv_log_trainval_batch\n",
    "\t\tdel pv_pred_trainval_batch\n",
    "\t\tdel image_log_test_batch\n",
    "\t\tdel pv_log_test_batch\n",
    "\t\tdel pv_pred_test_batch\n",
    "\n",
    "\tpred_folder_child = os.path.join(pred_folder,'frequency_'+str(sampling_interval))\n",
    "\t#store_trainval_test(all_times,image_log,pv_log,pv_pred,pred_folder_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149680 151138\n"
     ]
    }
   ],
   "source": [
    "print(curr_trainval_size, curr_trainval_size+validity_trainval_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458, 16, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image_log_batch[validity_trainval_mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[1 3 4 6 7 8 9 0]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3,4,5,6,7,8,9,0])\n",
    "b=np.array([True,False,True,True,False,True,True,True,True,True])\n",
    "print(b.sum())\n",
    "\n",
    "print(a[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocess_forecast.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
