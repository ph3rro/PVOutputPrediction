{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fu3OT8EoaUOE"
   },
   "outputs": [],
   "source": [
    "# import packages \n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# folder path and name\n",
    "project_path = os.getcwd()\n",
    "data_folder = os.path.join(os.getcwd(),'data')\n",
    "pred_folder = os.path.join(data_folder,'data_forecast')\n",
    "pv_data_path = os.path.join(data_folder,'pv_data','pv_output_valid.pkl')\n",
    "data_path = os.path.join(data_folder,'video_prediction_224.h5')\n",
    "\n",
    "image_name_format = '%Y%m%d%H%M%S'\n",
    "\n",
    "# Operating parameter\n",
    "stack_height = 15 # 15 minute\n",
    "forecast_horizon = 15 # 15 minutes ahead forecast\n",
    "sampling_interval_all = [2]\n",
    "output_img_shape = [224, 224, 3]\n",
    "\n",
    "start_date = datetime.datetime(2017,1,1) #NOTE: Inclusive of start date\n",
    "end_date = datetime.datetime(2018,1,1) #NOTE: Exclusive of end date (only end up with 2017 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up test set\n",
    "sunny_day = [(2017,9,15),(2017,10,6),(2017,10,22),(2018,2,16),(2018,6,12),(2018,6,23),(2019,1,25),(2019,6,23),(2019,7,14),(2019,10,14)]\n",
    "cloudy_day = [(2017,6,24),(2017,9,20),(2017,10,11),(2018,1,25),(2018,3,9),(2018,10,4),(2019,5,27),(2019,6,28),(2019,8,10),(2019,10,19)]\n",
    "\n",
    "sunny_datetime = [datetime.datetime(day[0],day[1],day[2]) for day in sunny_day]\n",
    "cloudy_datetime = [datetime.datetime(day[0],day[1],day[2]) for day in cloudy_day]\n",
    "test_dates = sunny_datetime + cloudy_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2017, 9, 15, 0, 0),\n",
       " datetime.datetime(2017, 10, 6, 0, 0),\n",
       " datetime.datetime(2017, 10, 22, 0, 0),\n",
       " datetime.datetime(2018, 2, 16, 0, 0),\n",
       " datetime.datetime(2018, 6, 12, 0, 0),\n",
       " datetime.datetime(2018, 6, 23, 0, 0),\n",
       " datetime.datetime(2019, 1, 25, 0, 0),\n",
       " datetime.datetime(2019, 6, 23, 0, 0),\n",
       " datetime.datetime(2019, 7, 14, 0, 0),\n",
       " datetime.datetime(2019, 10, 14, 0, 0),\n",
       " datetime.datetime(2017, 6, 24, 0, 0),\n",
       " datetime.datetime(2017, 9, 20, 0, 0),\n",
       " datetime.datetime(2017, 10, 11, 0, 0),\n",
       " datetime.datetime(2018, 1, 25, 0, 0),\n",
       " datetime.datetime(2018, 3, 9, 0, 0),\n",
       " datetime.datetime(2018, 10, 4, 0, 0),\n",
       " datetime.datetime(2019, 5, 27, 0, 0),\n",
       " datetime.datetime(2019, 6, 28, 0, 0),\n",
       " datetime.datetime(2019, 8, 10, 0, 0),\n",
       " datetime.datetime(2019, 10, 19, 0, 0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fwRci7HpaUOx"
   },
   "outputs": [],
   "source": [
    "def find_idx_with_dates(all_times,test_dates):\n",
    "    idx=[]\n",
    "    for test_day in test_dates:\n",
    "        test_day_end = test_day + datetime.timedelta(days = 1)\n",
    "        idx+=np.nonzero((all_times>test_day)*(all_times<test_day_end))[0].tolist()\n",
    "    return idx\n",
    "\n",
    "# This two function does the same thing. Just that one is for np, the other for pd.\n",
    "\n",
    "def find_time_within_nparray(time_array,time_point):\n",
    "    probable_idx = np.searchsorted(time_array,time_point)\n",
    "    \n",
    "    # If the time point is after all the time in pv_data\n",
    "    if probable_idx == len(time_array):\n",
    "        return None   \n",
    "    \n",
    "    # See if the time point is actually a match \n",
    "    if time_array[probable_idx]== time_point: \n",
    "        return probable_idx\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_time_within_pdseries(time_array,time_point):\n",
    "    probable_idx = np.searchsorted(time_array,time_point)\n",
    "    \n",
    "    # If the time point is after all the time in pv_data\n",
    "    if probable_idx == len(time_array):\n",
    "        return None   \n",
    "    \n",
    "    # See if the time point is actually a match \n",
    "    if time_array[probable_idx] == time_point: \n",
    "        return probable_idx\n",
    "        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RPtJ7Nw-aUPC"
   },
   "outputs": [],
   "source": [
    "def store_trainval_test(all_times,image_log,pv_log,pv_pred,pred_folder):\n",
    "    \n",
    "    ## Splitting into Trainval and Test set \n",
    "    idx_test = find_idx_with_dates(all_times,test_dates)\n",
    "    image_log_test = image_log[idx_test]\n",
    "    pv_log_test = pv_log[idx_test]\n",
    "    pv_pred_test = pv_pred[idx_test]\n",
    "    times_test = all_times[idx_test]\n",
    "\n",
    "    # the rest become the trainval set\n",
    "    mask_trainval = np.ones_like(pv_pred,dtype = bool)\n",
    "    mask_trainval[idx_test] = 0\n",
    "    image_log_trainval = image_log[mask_trainval]\n",
    "    pv_log_trainval = pv_log[mask_trainval]\n",
    "    pv_pred_trainval = pv_pred[mask_trainval]\n",
    "    times_trainval = all_times[mask_trainval]\n",
    "    \n",
    "    print(\"times_trainval.shape\",times_trainval.shape)\n",
    "    print(\"image_log_trainval.shape\",image_log_trainval.shape)\n",
    "    print(\"pv_log_trainval.shape\",pv_log_trainval.shape)\n",
    "    print(\"pv_pred_trainval.shape\",pv_pred_trainval.shape)\n",
    "    \n",
    "    print(\"times_test.shape\",times_test.shape)\n",
    "    print(\"image_log_test.shape\",image_log_test.shape)\n",
    "    print(\"pv_log_test.shape\",pv_log_test.shape)\n",
    "    print(\"pv_pred_test.shape\",pv_pred_test.shape)\n",
    "    \n",
    "    ## Storing information\n",
    "    # storing the training set\n",
    "    np.save(os.path.join(pred_folder,'image_log_trainval.npy'), image_log_trainval)\n",
    "    np.save(os.path.join(pred_folder,'pv_log_trainval.npy'), pv_log_trainval)\n",
    "    np.save(os.path.join(pred_folder,'pv_pred_trainval.npy'),pv_pred_trainval)\n",
    "    np.save(os.path.join(pred_folder,'times_trainval.npy'),times_trainval)\n",
    "\n",
    "    # storing the testing set\n",
    "    np.save(os.path.join(pred_folder,'image_log_test.npy'), image_log_test)\n",
    "    np.save(os.path.join(pred_folder,'pv_log_test.npy'), pv_log_test)\n",
    "    np.save(os.path.join(pred_folder,'pv_pred_test.npy'),pv_pred_test)\n",
    "    np.save(os.path.join(pred_folder,'times_test.npy'),times_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QH5ewDbBaUPT"
   },
   "outputs": [],
   "source": [
    "# Load in  high frequency data\n",
    "# the image here are ones that have corresponding PV value\n",
    "all_times = np.load(os.path.join(data_folder,'data_expanded','all_times_highfreq.npy'), allow_pickle=True)\n",
    "all_images = np.load(os.path.join(data_folder,'data_expanded','all_images_highfreq.npy'), allow_pickle=True)\n",
    "pv_data = np.load(pv_data_path, allow_pickle=True)\n",
    "#pv_data = np.load(os.path.join(data_folder,'data_expanded','pv_outputs_highfreq.npy'), allow_pickle=True)\n",
    "\n",
    "# only pick out the relevant time period\n",
    "#relevant_mask = (all_times>=start_date)&(all_times<end_date)\n",
    "#all_times = all_times[relevant_mask]\n",
    "#all_images = all_images[relevant_mask]\n",
    "#pv_data = pv_data[start_date:end_date]\n",
    "\n",
    "n_images = all_times.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-03 08:14:20    0.010697\n",
      "2017-01-03 08:14:30    0.023494\n",
      "2017-01-03 08:14:40    0.036292\n",
      "2017-01-03 08:14:50    0.049089\n",
      "2017-01-03 08:15:00    0.061887\n",
      "                         ...   \n",
      "2019-10-26 17:58:40    0.020961\n",
      "2019-10-26 17:58:50    0.015540\n",
      "2019-10-26 17:59:00    0.010119\n",
      "2019-10-26 17:59:10    0.005942\n",
      "2019-10-26 17:59:20    0.001765\n",
      "Length: 3887473, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(pv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363375\n"
     ]
    }
   ],
   "source": [
    "print(n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2017-01-03 08:14:20', '2017-01-03 08:14:30',\n",
      "               '2017-01-03 08:14:40', '2017-01-03 08:14:50',\n",
      "               '2017-01-03 08:15:00', '2017-01-03 08:15:10',\n",
      "               '2017-01-03 08:15:20', '2017-01-03 08:15:30',\n",
      "               '2017-01-03 08:15:40', '2017-01-03 08:15:50',\n",
      "               ...\n",
      "               '2019-10-26 17:57:50', '2019-10-26 17:58:00',\n",
      "               '2019-10-26 17:58:10', '2019-10-26 17:58:20',\n",
      "               '2019-10-26 17:58:30', '2019-10-26 17:58:40',\n",
      "               '2019-10-26 17:58:50', '2019-10-26 17:59:00',\n",
      "               '2019-10-26 17:59:10', '2019-10-26 17:59:20'],\n",
      "              dtype='datetime64[ns]', length=3887473, freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(pv_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6595,
     "status": "ok",
     "timestamp": 1599952937588,
     "user": {
      "displayName": "Andea Jewel Scott",
      "photoUrl": "",
      "userId": "15144577752603562183"
     },
     "user_tz": 420
    },
    "id": "DNaDTYlsaUPk",
    "outputId": "7ca39499-ecc0-4c68-ea22-62066a383436",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new datasets\n",
      "processed 7000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 14000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 21000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 28000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 35000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 42000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 49000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 56000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 63000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 70000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 77000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 84000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 91000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 98000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 105000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 112000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 119000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 126000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 133000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 140000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 147000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 154000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 161000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 168000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 175000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 182000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 189000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 196000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 203000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 210000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 217000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 224000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 231000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 238000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 245000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 252000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 259000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 266000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 273000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 280000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 287000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 294000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 301000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 308000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 315000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 322000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 329000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 336000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 343000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 350000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 357000/363375 images\n",
      "For sampling frequency:  2  minutes\n",
      "processed 364000/363375 images\n",
      "For sampling frequency:  2  minutes\n"
     ]
    }
   ],
   "source": [
    "# Create forecast training data file\n",
    "import h5py\n",
    "#mmap_array = np.memmap('video_prediction_224.dat', dtype='uint8', mode='w+', shape=(n_images, 224, 224, 3))\n",
    "sampling_interval = 2\n",
    "\n",
    "batch_size = 7000\n",
    "#for sampling_interval in sampling_interval_all:\n",
    "resume_idx = 0\n",
    "with h5py.File('video_prediction_224.h5', 'w') as f:\n",
    "\n",
    "\t#image_log = f.create_dataset('image_log', shape = (n_images, 224, 224, 3), dtype ='uint8', )\n",
    "\n",
    "\tif resume_idx and 'image_log' in f:\n",
    "        # Resume mode - datasets already exist\n",
    "\t\timage_log_ds = f['image_log']\n",
    "\t\tpv_log_ds = f['pv_log'] \n",
    "\t\tpv_pred_ds = f['pv_pred']\n",
    "\t\tprint(f\"Resuming from existing datasets. Current size: {image_log_ds.shape[0]}\")\n",
    "\telse:\n",
    "        # First run - create new datasets\n",
    "\t\timage_log_ds = f.create_dataset(\n",
    "\t\t\t'image_log',\n",
    "\t\t\tshape=(n_images, stack_height+1, *output_img_shape),\n",
    "\t\t\tcompression = \"gzip\",\n",
    "\t\t\tdtype='uint8'\n",
    "\t\t)\n",
    "\t\tpv_log_ds = f.create_dataset(\n",
    "\t\t\t'pv_log',\n",
    "\t\t\tshape=(n_images, stack_height+1),\n",
    "\t\t\tdtype='float64'\n",
    "\t\t)\n",
    "\t\tpv_pred_ds = f.create_dataset(\n",
    "\t\t\t'pv_pred',\n",
    "\t\t\tshape=(n_images,),\n",
    "\t\t\tdtype='float64'\n",
    "\t\t)\n",
    "\t\tprint(\"Creating new datasets\")\n",
    "\n",
    "\t\n",
    "\t#image_log = np.empty([0,stack_height+1]+output_img_shape,dtype = 'uint8')\n",
    "\t#pv_log = np.empty([0, stack_height+1])\n",
    "\t#pv_pred = np.empty([0])\n",
    "\n",
    "\tlast_valid_index = 0\n",
    "\tcurr_size = 0\n",
    "\n",
    "\ttic = time.process_time()\n",
    "\tfor b in range(resume_idx if (resume_idx and 'image_log' in f) else 0, n_images, batch_size):\n",
    "\t\tcurrent_batch_size =  min(batch_size, n_images-b)\n",
    "\t\t# Initialize variables to save pv values\n",
    "\t\t#image_log_batch = np.zeros([n_images,stack_height+1]+output_img_shape,dtype = 'uint8')\n",
    "\t\timage_log_batch = np.zeros([current_batch_size,stack_height+1]+output_img_shape,dtype = 'uint8')\n",
    "\t\t#all_times_batch = all_times[b*batch_size : b*batch_size + current_batch_size]\n",
    "\t\tpv_log_batch = np.zeros((current_batch_size,stack_height+1))\n",
    "\t\tpv_pred_batch = np.zeros(current_batch_size)\n",
    "\t\tvalidity_mask = np.ones(current_batch_size,dtype = bool)\n",
    "\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tsampling_interval_td = datetime.timedelta(minutes = sampling_interval) - datetime.timedelta(seconds=1)\n",
    "\t\tfor i in range(current_batch_size):\n",
    "\t\t\tcount = b+i\n",
    "\t\t\t# See if the specified sampling frequency is met \n",
    "\t\t\tif all_times[count] - all_times[last_valid_index] > sampling_interval_td:\n",
    "\n",
    "\t\t\t\t# Collecting groud truth for predicted value\n",
    "\t\t\t\tpred_time = all_times[count]+datetime.timedelta(minutes=forecast_horizon)\n",
    "\t\t\t\t\n",
    "\t\t\t\tpv_pred_idx = find_time_within_nparray(pv_data.index,pred_time)\n",
    "\t\t\t\tif pv_pred_idx is None:# if prediction ground truth not found\n",
    "\t\t\t\t\tvalidity_mask[i] = False\n",
    "\t\t\t\t\t#print(all_times[i],'has no PV pred')\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tpv_pred_batch[i] = pv_data.iloc[pv_pred_idx] \n",
    "\n",
    "\t\t\t\t# Collecting image log and PV log\n",
    "\t\t\t\tfor j in range(stack_height+1):\n",
    "\t\t\t\t\tlog_time = all_times[count] - datetime.timedelta(minutes = j)\n",
    "\t\t\t\t\t# Collecting a stack of image\n",
    "\t\t\t\t\tlog_time_idx = find_time_within_nparray(all_times,log_time)\n",
    "\t\t\t\t\tif log_time_idx is not None:\n",
    "\t\t\t\t\t\timage_log_batch[i,j] = all_images[log_time_idx]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tvalidity_mask[i] = False\n",
    "\t\t\t\t\t\t#print(all_times[count],'has no image log')\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t\t# Collecting a stack of PV value\n",
    "\t\t\t\t\tpv_log_idx = find_time_within_nparray(pv_data.index,log_time)\n",
    "\t\t\t\t\t# Check if PV value present\n",
    "\t\t\t\t\tif pv_log_idx is None:\n",
    "\t\t\t\t\t\tvalidity_mask[i] = False\n",
    "\t\t\t\t\t\t#print(all_times[count],'has no PV log')\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\telse: \n",
    "\t\t\t\t\t\tpv_log_batch[i,j] = pv_data.iloc[pv_log_idx]    \n",
    "\n",
    "\t\t\telse: # if this is in between the sampling points, discard\n",
    "\t\t\t\tvalidity_mask[i] = False\n",
    "\t\t\t\n",
    "\t\t\tif validity_mask[i]:\n",
    "\t\t\t\tlast_valid_index = i\n",
    "\t\t\t\n",
    "\t\t# Prompt progress of current work\n",
    "\n",
    "\t\tprint('processed {0}/{1} images'.format(b+batch_size,len(all_times)))\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\t# Only pick out the valid time points\n",
    "\t\t#all_times_batch = all_times_batch[validity_mask]\n",
    "\t\timage_log_batch = image_log_batch[validity_mask]\n",
    "\t\tpv_log_batch = pv_log_batch[validity_mask]\n",
    "\t\tpv_pred_batch = pv_pred_batch[validity_mask]\n",
    "\t\t# Store information\n",
    "\t\t\n",
    "\t\timage_log_ds[curr_size:curr_size+validity_mask.sum()] = image_log_batch\n",
    "\t\tpv_log_ds[curr_size:curr_size+validity_mask.sum()] = pv_log_batch\n",
    "\t\tpv_pred_ds[curr_size:curr_size+validity_mask.sum()] = pv_pred_batch\n",
    "\t\tcurr_size += validity_mask.sum()\n",
    "\n",
    "\t\tprint('For sampling frequency: ',sampling_interval,' minutes')\n",
    "\t\t#print('Expected finishing time:', datetime.datetime.now()+\n",
    "\t\t#\t\tdatetime.timedelta(seconds = (time.process_time() - tic)*(len(all_times)/(b+batch_size))))\n",
    "\t\tf.flush()\n",
    "\n",
    "\tpred_folder_child = os.path.join(pred_folder,'frequency_'+str(sampling_interval))\n",
    "\t#store_trainval_test(all_times,image_log,pv_log,pv_pred,pred_folder_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6375\n"
     ]
    }
   ],
   "source": [
    "print(current_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocess_forecast.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
