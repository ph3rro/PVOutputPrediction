{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8MWBOKc_p8N"
      },
      "source": [
        "### Libraries and data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "lZ8wnidH_p8O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "import h5py\n",
        "import math\n",
        "import matplotlib.dates as mdates\n",
        "import numpy.ma as ma\n",
        "import CRPS.CRPS as pscore\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.image as mpimg\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "import pickle as pkl\n",
        "#import tables\n",
        "import hdf5plugin\n",
        "#writer = SummaryWriter()\n",
        "\n",
        "%matplotlib inline\n",
        "#%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.1+cu128\n",
            "CUDA available: True\n",
            "CUDA version: 12.8\n",
            "GPU count: 1\n",
            "GPU name: NVIDIA GeForce RTX 4070 Ti\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "# Check if CUDA is available\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "\n",
        "# Get GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "sunny_day = [(2017,9,15),(2017,10,6),(2017,10,22),(2018,2,16),(2018,6,12),(2018,6,23),(2019,1,25),(2019,6,23),(2019,7,14),(2019,10,14)]\n",
        "cloudy_day = [(2017,6,24),(2017,9,20),(2017,10,11),(2018,1,25),(2018,3,9),(2018,10,4),(2019,5,27),(2019,6,28),(2019,8,10),(2019,10,19)]\n",
        "\n",
        "sunny_datetime = [datetime.datetime(day[0],day[1],day[2]) for day in sunny_day]\n",
        "cloudy_datetime = [datetime.datetime(day[0],day[1],day[2]) for day in cloudy_day]\n",
        "test_dates = sunny_datetime + cloudy_datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[datetime.datetime(2017, 9, 15, 0, 0),\n",
              " datetime.datetime(2017, 10, 6, 0, 0),\n",
              " datetime.datetime(2017, 10, 22, 0, 0),\n",
              " datetime.datetime(2018, 2, 16, 0, 0),\n",
              " datetime.datetime(2018, 6, 12, 0, 0),\n",
              " datetime.datetime(2018, 6, 23, 0, 0),\n",
              " datetime.datetime(2019, 1, 25, 0, 0),\n",
              " datetime.datetime(2019, 6, 23, 0, 0),\n",
              " datetime.datetime(2019, 7, 14, 0, 0),\n",
              " datetime.datetime(2019, 10, 14, 0, 0),\n",
              " datetime.datetime(2017, 6, 24, 0, 0),\n",
              " datetime.datetime(2017, 9, 20, 0, 0),\n",
              " datetime.datetime(2017, 10, 11, 0, 0),\n",
              " datetime.datetime(2018, 1, 25, 0, 0),\n",
              " datetime.datetime(2018, 3, 9, 0, 0),\n",
              " datetime.datetime(2018, 10, 4, 0, 0),\n",
              " datetime.datetime(2019, 5, 27, 0, 0),\n",
              " datetime.datetime(2019, 6, 28, 0, 0),\n",
              " datetime.datetime(2019, 8, 10, 0, 0),\n",
              " datetime.datetime(2019, 10, 19, 0, 0)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJYAy-t7_p8P",
        "outputId": "abc24607-f813-4790-9773-6dd4494b3283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data_folder: D:/PVOutputPrediction/preprocessing/data\n",
            "output_folder: d:\\PVOutputPrediction\\models\\model_output\\UNet_sky_image_PV_mapping\n"
          ]
        }
      ],
      "source": [
        "# define the data location and load data\n",
        "import os\n",
        "preprocessing_folder = \"D:/PVOutputPrediction/preprocessing/\"\n",
        "cwd = os.getcwd()\n",
        "#pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "data_folder = os.path.join(preprocessing_folder,\"data\")\n",
        "#data_path = os.path.join(data_folder,'video_prediction_224_second.h5')\n",
        "\n",
        "# !change model name for different models!\n",
        "model_name = 'UNet_sky_image_PV_mapping'\n",
        "output_folder = os.path.join(cwd,\"model_output\", model_name)\n",
        "if os.path.isdir(output_folder)==False:\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "image_name_format = '%Y%m%d%H%M%S'\n",
        "\n",
        "# Operating parameter\n",
        "stack_height = 15 # 15 minute\n",
        "forecast_horizon = 15 # 15 minutes ahead forecast\n",
        "sampling_interval_all = [2]\n",
        "output_img_shape = [224, 224, 3]\n",
        "\n",
        "print(\"data_folder:\", data_folder)\n",
        "#print(\"data_path:\", data_path)\n",
        "print(\"output_folder:\", output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(pardir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import h5py\n",
        "# with h5py.File(data_path, 'r') as f:\n",
        "    # Access the dataset directly by its path\n",
        "    # This creates a dataset object, but does not load the data yet\n",
        "    # print(f[\"trainval\"])\n",
        "    \n",
        "    # Use slicing to load the data into a NumPy array\n",
        "    \n",
        "    \n",
        "    #print(f\"Dataset shape: {dataset.shape}\")\n",
        "    #print(f\"Data type: {dataset.dtype}\")\n",
        "    #print(f\"First 5 rows:\\n{data_array[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = \"../preprocessing/data/video_prediction_224_testing.h5\"\n",
        "f = h5py.File(data_path, 'r')\n",
        "image_log_trainval = f['trainval/image_log']\n",
        "image_log_test = f['test/image_log']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image = image_log_test[0][15]\n",
        "#image = image[90:130, :20]\n",
        "image[:,2] = [255,0,0]\n",
        "image[:,-6] = [255,0,0]\n",
        "image[0,:] = [255,0,0]\n",
        "image[-8,:] = [255,0,0]\n",
        "plt.imshow(image)\n",
        "plt.title(\"Color Image (RGB)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[51 43 54]\n"
          ]
        }
      ],
      "source": [
        "print(image[0,110])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJKp70Oy_p8P",
        "outputId": "815bb243-8fd5-4be2-9d8a-08acbdb9236a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<HDF5 group \"/test\" (3 members)>\n",
            "<HDF5 dataset \"image_log\": shape (6145, 16, 224, 224, 3), type \"|u1\">\n",
            "<HDF5 dataset \"pv_log\": shape (6145, 16), type \"<f8\">\n",
            "<HDF5 dataset \"pv_pred\": shape (6145,), type \"<f8\">\n",
            "<HDF5 group \"/trainval\" (3 members)>\n",
            "<HDF5 dataset \"image_log\": shape (149680, 16, 224, 224, 3), type \"|u1\">\n",
            "<HDF5 dataset \"pv_log\": shape (149680, 16), type \"<f8\">\n",
            "<HDF5 dataset \"pv_pred\": shape (149680,), type \"<f8\">\n"
          ]
        }
      ],
      "source": [
        "# generate handler for the hdf5 data\n",
        "forecast_dataset = h5py.File(data_path, 'r')\n",
        "\n",
        "# show structure of the hdf5 data\n",
        "def get_all(name):\n",
        "    if name!=None:\n",
        "        print(forecast_dataset[name])\n",
        "\n",
        "forecast_dataset.visit(get_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = 317968"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new datasets\n"
          ]
        }
      ],
      "source": [
        "'''import hdf5plugin\n",
        "chunk_shape = (1, stack_height+1, *output_img_shape)\n",
        "\n",
        "batch_size = 8000\n",
        "with h5py.File('dummyfile.h5', 'w') as f:\n",
        "\n",
        "\t#image_log = f.create_dataset('image_log', shape = (n_images, 224, 224, 3), dtype ='uint8', )\n",
        "\n",
        "        # First run - create new datasets\n",
        "\n",
        "\ttrainval_group = f.create_group('trainval')\n",
        "\ttest_group = f.create_group('test')\n",
        "\timage_log_trainval_ds = trainval_group.create_dataset(\n",
        "\t\t'image_log',\n",
        "\t\tshape=(5, stack_height+1, *output_img_shape),\n",
        "\t\tchunks=chunk_shape,\n",
        "\t\tcompression=hdf5plugin.Blosc2(cname='zstd', clevel=1, filters=hdf5plugin.Blosc2.SHUFFLE),\n",
        "\t\tdtype='uint8'\n",
        "\t)\n",
        "\n",
        "\tprint(\"Creating new datasets\")\n",
        "\tfor i in range(4):\n",
        "\t\timage_log_trainval_ds[i:i+1] = np.ones((1, stack_height+1, *output_img_shape), dtype='uint8')\n",
        "\t\tf.flush()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# ds = load_dataset(\"skyimagenet/SKIPPD\")\n",
        "\n",
        "# images_train =[]\n",
        "# pv_train = []\n",
        "# times_train = []\n",
        "# images_test =[]\n",
        "# pv_test = []\n",
        "# times_test = []\n",
        "# for mode in [\"train\", \"test\"]:\n",
        "    # globals()[f'images_{mode}'].append(np.array(ds[mode][:]['image']))\n",
        "    # globals()[f'pv_{mode}'].append(ds[mode][:]['pv'])\n",
        "    # globals()[f'times_{mode}'] = ds[mode][:]['time']\n",
        "\n",
        "# for mode in [\"train\", \"test\"]:\n",
        "#     globals()[f'images_{mode}'] = np.array(globals()[f'images_{mode}'])\n",
        "#     globals()[f'pv_{mode}'] = np.array(globals()[f'pv_{mode}'] )\n",
        "    \n",
        "# images_train = np.squeeze(images_train)\n",
        "# pv_train = np.squeeze(pv_train)\n",
        "# times_train = np.squeeze(times_train)\n",
        "\n",
        "# images_test = np.squeeze(images_test)\n",
        "# pv_test = np.squeeze(pv_test)\n",
        "# times_test = np.squeeze(times_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.save(\"times_train.npy\", np.array(times_train))\n",
        "# np.save(\"times_test.npy\", np.array(times_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def varpath(txt):\n",
        "    return(os.path.join(\"variables\", txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir('..')\n",
        "images_trainval = np.load(varpath(\"images_trainval.npy\"))\n",
        "pv_trainval = np.load(varpath(\"pv_trainval.npy\"))\n",
        "times_trainval = np.load(varpath(\"times_trainval.npy\"), allow_pickle=True)\n",
        "\n",
        "# images_test = np.load(\"images_test.npy\")\n",
        "# pv_test = np.load(\"pv_test.npy\")\n",
        "# times_test = np.load(\"times_test.npy\", allow_pickle=True)\n",
        "\n",
        "images_trainval = images_trainval.transpose(0, 3, 1, 2)\n",
        "# images_test = images_test.transpose(0, 3, 1, 2)\n",
        "images_trainval = torch.from_numpy(images_trainval).float()\n",
        "# images_test = torch.from_numpy(images_test).float()\n",
        "pv_trainval = torch.from_numpy(pv_trainval).float()\n",
        "# pv_test = torch.from_numpy(pv_test).float()\n",
        "times_trainval = np.squeeze(times_trainval)\n",
        "# times_test = np.squeeze(times_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(23.2777)\n"
          ]
        }
      ],
      "source": [
        "idx = 2000\n",
        "img = images_trainval[idx]\n",
        "\n",
        "# If it's a tensor, convert to numpy\n",
        "if torch.is_tensor(img):\n",
        "\timg = img.cpu().numpy()\n",
        "\n",
        "# Transpose from (C, H, W) to (H, W, C) for matplotlib\n",
        "arr = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "# Ensure values are in the correct range [0, 1] or [0, 255]\n",
        "# If your values are in [0, 255] range but stored as float, convert to uint8\n",
        "if arr.max() > 1.0:\n",
        "    arr = arr.astype(np.uint8)\n",
        "else:\n",
        "    # If normalized to [0, 1], matplotlib handles it automatically\n",
        "    pass\n",
        "plt.imshow(arr, cmap=None, interpolation=None, aspect=None)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(pv_trainval[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkxa-UKU_p8Q"
      },
      "source": [
        "### Input data pipeline helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "ODMVPPkP_p8Q"
      },
      "outputs": [],
      "source": [
        "# day block shuffling of the time stamps, and return shuffled indices\n",
        "def day_block_shuffle(times_trainval):\n",
        "\n",
        "    # Only keep the date of each time point\n",
        "    dates_trainval = np.zeros_like(times_trainval, dtype=datetime.date)\n",
        "    for i in range(len(times_trainval)):\n",
        "        dates_trainval[i] = times_trainval[i].date()\n",
        "\n",
        "    # Chop the indices into blocks, so that each block contains the indices of the same day\n",
        "    unique_dates = np.unique(dates_trainval)\n",
        "    blocks = []\n",
        "    for i in range(len(unique_dates)):\n",
        "        blocks.append(np.where(dates_trainval == unique_dates[i])[0])\n",
        "\n",
        "    # shuffle the blocks, and chain it back together\n",
        "    np.random.seed(1)\n",
        "    np.random.shuffle(blocks)\n",
        "    shuffled_indices = np.asarray(list(itertools.chain.from_iterable(blocks)))\n",
        "\n",
        "    return shuffled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "DDLHlv41_p8Q"
      },
      "outputs": [],
      "source": [
        "# a cross validation generator function for spliting the dayblock shuffled indices into training and validation\n",
        "def cv_split(split_data, fold_index, num_fold):\n",
        "    '''\n",
        "    input:\n",
        "    split_data: the dayblock shuffled indices to be splitted\n",
        "    fold_index: the ith fold chosen as the validation, used for generating the seed for random shuffling\n",
        "    num_fold: N-fold cross validation\n",
        "    output:\n",
        "    data_train: the train data indices\n",
        "    data_val: the validation data indices\n",
        "    '''\n",
        "    # randomly divides into a training set and a validation set\n",
        "    num_samples = len(split_data)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    # finding training and validation indices\n",
        "    val_mask = np.zeros(len(indices), dtype=bool)\n",
        "    val_mask[int(fold_index / num_fold * num_samples):int((fold_index + 1) / num_fold * num_samples)] = True\n",
        "    val_indices = indices[val_mask]\n",
        "    train_indices = indices[np.logical_not(val_mask)]\n",
        "\n",
        "    # shuffle indices\n",
        "    np.random.seed(fold_index)\n",
        "    np.random.shuffle(train_indices)\n",
        "    np.random.shuffle(val_indices)\n",
        "\n",
        "    data_train = split_data[train_indices]\n",
        "    data_val = split_data[val_indices]\n",
        "\n",
        "    return data_train,data_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "kehtU2z9_p8R"
      },
      "outputs": [],
      "source": [
        "def mask_background(img, img_size = 224): # put all background pixels (the ones outside the circle region of sky images) to 0s\n",
        "\tmask = torch.ones((3,img_size,img_size), dtype=bool)\n",
        "\n",
        "\tif img_size == 224:\n",
        "\t\tcenter_i = 108\n",
        "\t\tcenter_j = 110\n",
        "\t\tradius = 108\n",
        "\telif img_size == 64:\n",
        "\t\tcenter_i = 30\n",
        "\t\tcenter_j = 30\n",
        "\t\tradius = 31\n",
        "\tfor i in range(img_size):\n",
        "\t\tfor j in range(img_size):\n",
        "\t\t\tif (i-center_i)**2+(j-center_j)**2>=radius**2:\n",
        "\t\t\t\tmask[:,i,j]=0\n",
        "\tmask_img = img*mask\n",
        "\treturn mask_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "z24FGOTmB3LY"
      },
      "outputs": [],
      "source": [
        "class PVDataset(Dataset):\n",
        "\n",
        "    def __init__(self, images, pv, transform=None, target_transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images = mask_background(images)\n",
        "        self.images = self.images/255\n",
        "        self.pv = pv\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = self.images[idx]\n",
        "        pv = self.pv[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            pv = self.target_transform(pv)\n",
        "        return image, pv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "on2fDoJf_p8S"
      },
      "outputs": [],
      "source": [
        "def compute_winkler_score(prob_prediction,observation):\n",
        "    alpha = 0.1\n",
        "    lb = np.percentile(prob_prediction,5,axis=0)\n",
        "    ub = np.percentile(prob_prediction,95,axis=0)\n",
        "    delta = ub-lb\n",
        "    if observation<lb:\n",
        "        sc = delta+2*(lb-observation)/alpha\n",
        "    if observation>ub:\n",
        "        sc = delta+2*(observation-ub)/alpha\n",
        "    if (observation>=lb) and (observation<=ub):\n",
        "        sc = delta\n",
        "    return sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twl8BbH8_p8S"
      },
      "source": [
        "### Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "93-c3tU7_p8S"
      },
      "outputs": [],
      "source": [
        "# define training time parameters\n",
        "num_filters = 12\n",
        "num_epochs = 200 #(The maximum epoches set to 200 and there might be early stopping depends on validation loss)\n",
        "num_fold = 10 # 10-fold cross-validation\n",
        "batch_size = 256\n",
        "val_batch_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'class ViT(nn.Module):\\n    \"\"\"\\n    U-Net style architecture for Image-to-PV prediction\\n    \"\"\"\\n    def __init__(self, image_input_dim, num_filters=12):\\n        super(ViT, self).__init__()\\n\\n        # Assuming image_input_dim is (height, width, channels)\\n        # PyTorch uses (channels, height, width)\\n        if len(image_input_dim) == 3:\\n            input_channels = image_input_dim[0]  # Assuming CHW format\\n        else:\\n            input_channels = image_input_dim[2]  # Assuming HWC format\\n\\n        # Initial 1x1 convolution\\n        self.conv1x1_input = nn.Conv2d(input_channels, num_filters, kernel_size=1, padding=0)\\n\\n        # Encoder (contracting path)\\n        self.conv3x3_1 = Conv3x3Block(num_filters, num_filters)\\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\\n\\n        self.conv3x3_2 = Conv3x3Block(num_filters, 2 * num_filters)\\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\\n\\n        self.conv3x3_3 = Conv3x3Block(2 * num_filters, 4 * num_filters)\\n\\n        # Bottleneck with residual blocks\\n        self.bottleneck1 = BottleNeckBlock(4 * num_filters)\\n        self.bottleneck2 = BottleNeckBlock(4 * num_filters)\\n\\n        # Decoder (expanding path)\\n        self.up1 = Up2x2Conv3x3(4 * num_filters, 2 * num_filters)\\n        self.conv3x3_4 = Conv3x3Block(4 * num_filters, 2 * num_filters)  # After concatenation\\n        self.dropout1 = nn.Dropout2d(0.4)\\n\\n        self.up2 = Up2x2Conv3x3(2 * num_filters, num_filters)\\n        self.conv3x3_5 = Conv3x3Block(2 * num_filters, num_filters)  # After concatenation\\n        self.dropout2 = nn.Dropout2d(0.4)\\n\\n        # Final 1x1 convolution and output\\n        self.conv1x1_output = nn.Conv2d(num_filters, 1, kernel_size=1, padding=0)\\n        self.relu_final = nn.ReLU(inplace=True)\\n\\n        # Global average pooling and final dense layer\\n        self.flatten = nn.Flatten()\\n        self.final_dense = nn.Linear(4096, 1)\\n\\n    def forward(self, x):\\n        # Initial 1x1 convolution\\n        x = self.conv1x1_input(x)\\n\\n        # Encoder path\\n        # First level\\n        skip1 = self.conv3x3_1(x)  # Save for skip connection\\n        x = self.maxpool1(skip1)\\n\\n        # Second level\\n        skip2 = self.conv3x3_2(x)  # Save for skip connection\\n        x = self.maxpool2(skip2)\\n\\n        # Third level\\n        x = self.conv3x3_3(x)\\n\\n        # Bottleneck\\n        x = self.bottleneck1(x)\\n        x = self.bottleneck2(x)\\n\\n        # Decoder path\\n        # First upsampling\\n        x = self.up1(x)\\n        x = torch.cat([x, skip2], dim=1)  # Concatenate along channel dimension\\n        x = self.conv3x3_4(x)\\n        x = self.dropout1(x)\\n\\n        # Second upsampling\\n        x = self.up2(x)\\n        x = torch.cat([x, skip1], dim=1)  # Concatenate along channel dimension\\n        x = self.conv3x3_5(x)\\n        x = self.dropout2(x)\\n\\n        # Final output\\n        x = self.conv1x1_output(x)\\n        x = self.relu_final(x)\\n\\n        # Global pooling and final prediction\\n        x = self.flatten(x)\\n        x = self.final_dense(x)\\n\\n        return x\\n\\n# Example instantiation and summary\\nif __name__ == \"__main__\":\\n    # Assuming 64x64 RGB images\\n    model = ViT(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\\n\\n    # Print model summary\\n    print(model)\\n\\n    # Test with dummy input\\n    dummy_input = torch.randn(1, 3, 64, 64).to(device)  # Batch size 1, 3 channels, 64x64\\n    output = model(dummy_input)\\n    print(f\"Output shape: {output.shape}\")\\n\\n    # Count parameters\\n    total_params = sum(p.numel() for p in model.parameters())\\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\n    print(f\"Total parameters: {total_params:,}\")\\n    print(f\"Trainable parameters: {trainable_params:,}\")\\n\\n    summary(model, input_size = (3,64,64))'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import timm\n",
        "\n",
        "#model = timm.create_model(\"hf_hub:timm/convnext_tiny.in12k\", pretrained=True)\n",
        "\n",
        "'''class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net style architecture for Image-to-PV prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, image_input_dim, num_filters=12):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        # Assuming image_input_dim is (height, width, channels)\n",
        "        # PyTorch uses (channels, height, width)\n",
        "        if len(image_input_dim) == 3:\n",
        "            input_channels = image_input_dim[0]  # Assuming CHW format\n",
        "        else:\n",
        "            input_channels = image_input_dim[2]  # Assuming HWC format\n",
        "\n",
        "        # Initial 1x1 convolution\n",
        "        self.conv1x1_input = nn.Conv2d(input_channels, num_filters, kernel_size=1, padding=0)\n",
        "\n",
        "        # Encoder (contracting path)\n",
        "        self.conv3x3_1 = Conv3x3Block(num_filters, num_filters)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_2 = Conv3x3Block(num_filters, 2 * num_filters)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_3 = Conv3x3Block(2 * num_filters, 4 * num_filters)\n",
        "\n",
        "        # Bottleneck with residual blocks\n",
        "        self.bottleneck1 = BottleNeckBlock(4 * num_filters)\n",
        "        self.bottleneck2 = BottleNeckBlock(4 * num_filters)\n",
        "\n",
        "        # Decoder (expanding path)\n",
        "        self.up1 = Up2x2Conv3x3(4 * num_filters, 2 * num_filters)\n",
        "        self.conv3x3_4 = Conv3x3Block(4 * num_filters, 2 * num_filters)  # After concatenation\n",
        "        self.dropout1 = nn.Dropout2d(0.4)\n",
        "\n",
        "        self.up2 = Up2x2Conv3x3(2 * num_filters, num_filters)\n",
        "        self.conv3x3_5 = Conv3x3Block(2 * num_filters, num_filters)  # After concatenation\n",
        "        self.dropout2 = nn.Dropout2d(0.4)\n",
        "\n",
        "        # Final 1x1 convolution and output\n",
        "        self.conv1x1_output = nn.Conv2d(num_filters, 1, kernel_size=1, padding=0)\n",
        "        self.relu_final = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Global average pooling and final dense layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.final_dense = nn.Linear(4096, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial 1x1 convolution\n",
        "        x = self.conv1x1_input(x)\n",
        "\n",
        "        # Encoder path\n",
        "        # First level\n",
        "        skip1 = self.conv3x3_1(x)  # Save for skip connection\n",
        "        x = self.maxpool1(skip1)\n",
        "\n",
        "        # Second level\n",
        "        skip2 = self.conv3x3_2(x)  # Save for skip connection\n",
        "        x = self.maxpool2(skip2)\n",
        "\n",
        "        # Third level\n",
        "        x = self.conv3x3_3(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck1(x)\n",
        "        x = self.bottleneck2(x)\n",
        "\n",
        "        # Decoder path\n",
        "        # First upsampling\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, skip2], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_4(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Second upsampling\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, skip1], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_5(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Final output\n",
        "        x = self.conv1x1_output(x)\n",
        "        x = self.relu_final(x)\n",
        "\n",
        "        # Global pooling and final prediction\n",
        "        x = self.flatten(x)\n",
        "        x = self.final_dense(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example instantiation and summary\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 64x64 RGB images\n",
        "    model = ViT(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\n",
        "    \n",
        "    # Print model summary\n",
        "    print(model)\n",
        "\n",
        "    # Test with dummy input\n",
        "    dummy_input = torch.randn(1, 3, 64, 64).to(device)  # Batch size 1, 3 channels, 64x64\n",
        "    output = model(dummy_input)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    summary(model, input_size = (3,64,64))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainval(model, device, loader, optimizer, criterion, mode=\"train\"):\n",
        "    \n",
        "    if mode == \"train\":\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "        \n",
        "    size = len(loader)\n",
        "         \n",
        "    total_loss = 0\n",
        "    for batch_idx, (image,pv) in enumerate(loader):\n",
        "        image, pv = image.to(device), pv.to(device)\n",
        "        \n",
        "        output = model(image).squeeze()\n",
        "        loss = criterion(output, pv)\n",
        "        if mode == \"train\":\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "    total_loss = total_loss / size\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import timm\n",
        "\n",
        "# model = timm.create_model(\"hf_hub:timm/convnext_tiny.in12k\", pretrained=True)\n",
        "# model.head.fc = nn.Linear(768, 1)\n",
        "# model.to(device)\n",
        "# summary(model, input_size = (3, 64, 64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KIwj8ITRhkrA"
      },
      "outputs": [],
      "source": [
        "class Conv3x3Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic convolutional block with Conv2D -> BatchNorm -> ReLU\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(Conv3x3Block, self).__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class BottleNeckBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual bottleneck block with skip connection\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(BottleNeckBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = out + identity  # Skip connection\n",
        "        return out\n",
        "\n",
        "class Up2x2Conv3x3(nn.Module):\n",
        "    \"\"\"\n",
        "    Upsampling block with 2x2 upsampling followed by 3x3 convolution\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(Up2x2Conv3x3, self).__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net style architecture for Image-to-PV prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, image_input_dim, num_filters=12):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Assuming image_input_dim is (height, width, channels)\n",
        "        # PyTorch uses (channels, height, width)\n",
        "        if len(image_input_dim) == 3:\n",
        "            input_channels = image_input_dim[0]  # Assuming CHW format\n",
        "        else:\n",
        "            input_channels = image_input_dim[2]  # Assuming HWC format\n",
        "\n",
        "        # Initial 1x1 convolution\n",
        "        self.conv1x1_input = nn.Conv2d(input_channels, num_filters, kernel_size=1, padding=0)\n",
        "\n",
        "        # Encoder (contracting path)\n",
        "        self.conv3x3_1 = Conv3x3Block(num_filters, num_filters)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_2 = Conv3x3Block(num_filters, 2 * num_filters)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_3 = Conv3x3Block(2 * num_filters, 4 * num_filters)\n",
        "\n",
        "        # Bottleneck with residual blocks\n",
        "        self.bottleneck1 = BottleNeckBlock(4 * num_filters)\n",
        "        self.bottleneck2 = BottleNeckBlock(4 * num_filters)\n",
        "\n",
        "        # Decoder (expanding path)\n",
        "        self.up1 = Up2x2Conv3x3(4 * num_filters, 2 * num_filters)\n",
        "        self.conv3x3_4 = Conv3x3Block(4 * num_filters, 2 * num_filters)  # After concatenation\n",
        "        self.dropout1 = nn.Dropout2d(0.4)\n",
        "\n",
        "        self.up2 = Up2x2Conv3x3(2 * num_filters, num_filters)\n",
        "        self.conv3x3_5 = Conv3x3Block(2 * num_filters, num_filters)  # After concatenation\n",
        "        self.dropout2 = nn.Dropout2d(0.4)\n",
        "\n",
        "        # Final 1x1 convolution and output\n",
        "        self.conv1x1_output = nn.Conv2d(num_filters, 1, kernel_size=1, padding=0)\n",
        "        self.relu_final = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Global average pooling and final dense layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.final_dense = nn.Linear(4096, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial 1x1 convolution\n",
        "        x = self.conv1x1_input(x)\n",
        "\n",
        "        # Encoder path\n",
        "        # First level\n",
        "        skip1 = self.conv3x3_1(x)  # Save for skip connection\n",
        "        x = self.maxpool1(skip1)\n",
        "\n",
        "        # Second level\n",
        "        skip2 = self.conv3x3_2(x)  # Save for skip connection\n",
        "        x = self.maxpool2(skip2)\n",
        "\n",
        "        # Third level\n",
        "        x = self.conv3x3_3(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck1(x)\n",
        "        x = self.bottleneck2(x)\n",
        "\n",
        "        # Decoder path\n",
        "        # First upsampling\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, skip2], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_4(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Second upsampling\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, skip1], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_5(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Final output\n",
        "        x = self.conv1x1_output(x)\n",
        "        x = self.relu_final(x)\n",
        "\n",
        "        # Global pooling and final prediction\n",
        "        x = self.flatten(x)\n",
        "        x = self.final_dense(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# # Example instantiation and summary\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Assuming 64x64 RGB images\n",
        "#     model = UNet(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\n",
        "    \n",
        "#     # Print model summary\n",
        "#     print(model)\n",
        "\n",
        "#     # Test with dummy input\n",
        "#     dummy_input = torch.randn(1, 3, 64, 64).to(device)  # Batch size 1, 3 channels, 64x64\n",
        "#     output = model(dummy_input)\n",
        "#     print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "#     # Count parameters\n",
        "#     total_params = sum(p.numel() for p in model.parameters())\n",
        "#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#     print(f\"Total parameters: {total_params:,}\")\n",
        "#     print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "#     summary(model, input_size = (3,64,64))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "initial_learning_rate = 2e-4\n",
        "#optimizer = optim.Adam(model.parameters(), lr = initial_learning_rate)\n",
        "\n",
        "def checkpoint(model, filename):\n",
        "\ttorch.save(model.state_dict(), filename)\n",
        "\n",
        "def resume(model, filename):\n",
        "\tmodel.load_state_dict(torch.load(filename))\n",
        "\n",
        "#scheduler = lr_scheduler.ExponentialLR(optimizer, 0.933)\n",
        "\n",
        "indices_dayblock_shuffled = day_block_shuffle(times_trainval)\n",
        "\n",
        "#train_loss_hist = np.load(os.path.join(output_folder,'train_loss_hist.npy'))\n",
        "#val_loss_hist = np.load(os.path.join(output_folder,'val_loss_hist.npy'))\n",
        "train_loss_hist = []\n",
        "val_loss_hist = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resume_model = False\n",
        "\n",
        "if resume_model:\n",
        "\twith open(varpath('min_loss.pkl'), 'rb') as file:\n",
        "\t\tmin_loss = pkl.load(file)\n",
        "else:\n",
        "\tmin_loss = [10000] * num_fold\n",
        "\tstart_fold = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial val loss:  203.06669771498528\n",
            "Fold 1; Epoch 0: Training Loss =  6.368630548043402\n",
            "Fold 1; Epoch 0: Val Loss =  6.252595977506775\n",
            "Fold 1; Epoch 1: Training Loss =  3.5125728511926697\n",
            "Fold 1; Epoch 1: Val Loss =  7.1060785625291905\n",
            "Fold 1; Epoch 2: Training Loss =  3.1112700670690825\n",
            "Fold 1; Epoch 2: Val Loss =  6.990498957426651\n",
            "Fold 1; Epoch 3: Training Loss =  2.900419053069162\n",
            "Fold 1; Epoch 3: Val Loss =  7.26374103711999\n",
            "Fold 1; Epoch 4: Training Loss =  2.576978204888576\n",
            "Fold 1; Epoch 4: Val Loss =  7.715039502019468\n",
            "Fold 1; Epoch 5: Training Loss =  2.206390385078744\n",
            "Fold 1; Epoch 5: Val Loss =  7.901738270469334\n",
            "Fold 1; Epoch 6: Training Loss =  1.8497705887836398\n",
            "Fold 1; Epoch 6: Val Loss =  7.878121458965799\n",
            "Fold 1; Epoch 7: Training Loss =  1.6356828667653878\n",
            "Fold 1; Epoch 7: Val Loss =  8.690351119939832\n",
            "Fold 1; Epoch 8: Training Loss =  1.5299624581953877\n",
            "Fold 1; Epoch 8: Val Loss =  8.02236717334692\n",
            "Fold 1; Epoch 9: Training Loss =  1.4416183187308789\n",
            "Fold 1; Epoch 9: Val Loss =  7.680653565171836\n",
            "Fold 1; Epoch 10: Training Loss =  1.2704402831655095\n",
            "Fold 1; Epoch 10: Val Loss =  7.398347771686057\n",
            "Fold 1; Epoch 11: Training Loss =  1.2146283972544627\n",
            "Fold 1; Epoch 11: Val Loss =  7.689385946246161\n",
            "Fold 1; Epoch 12: Training Loss =  1.157697010176079\n",
            "Fold 1; Epoch 12: Val Loss =  7.549162512240202\n",
            "Fold 1; Epoch 13: Training Loss =  1.1359943985041805\n",
            "Fold 1; Epoch 13: Val Loss =  7.5124659330948536\n",
            "Fold 1; Epoch 14: Training Loss =  1.1694969996579592\n",
            "Fold 1; Epoch 14: Val Loss =  8.046383519103562\n",
            "Fold 1; Epoch 15: Training Loss =  1.175597162182201\n",
            "Fold 1; Epoch 15: Val Loss =  8.169366643048715\n",
            "Fold 1; Epoch 16: Training Loss =  1.196759573527232\n",
            "Fold 1; Epoch 16: Val Loss =  7.966280411982882\n",
            "Fold 1; Epoch 17: Training Loss =  1.054141724013865\n",
            "Fold 1; Epoch 17: Val Loss =  7.711023662401282\n",
            "Fold 1; Epoch 18: Training Loss =  0.9752177216258109\n",
            "Fold 1; Epoch 18: Val Loss =  7.7293320946071455\n",
            "Fold 1; Epoch 19: Training Loss =  0.9524416845864742\n",
            "Fold 1; Epoch 19: Val Loss =  7.801035418026689\n",
            "Fold 1; Epoch 20: Training Loss =  0.9445102183743886\n",
            "Fold 1; Epoch 20: Val Loss =  7.7220995322517725\n",
            "Fold 1; Epoch 21: Training Loss =  0.9655981422640668\n",
            "Fold 1; Epoch 21: Val Loss =  7.823506369107012\n",
            "Fold 1; Epoch 22: Training Loss =  0.9447031524532495\n",
            "Fold 1; Epoch 22: Val Loss =  7.727268744206083\n",
            "Fold 1; Epoch 23: Training Loss =  0.925667429802086\n",
            "Fold 1; Epoch 23: Val Loss =  7.268044755078744\n",
            "Fold 1; Epoch 24: Training Loss =  0.9163709463592561\n",
            "Fold 1; Epoch 24: Val Loss =  7.571034307065218\n",
            "Fold 1; Epoch 25: Training Loss =  0.9265138776798598\n",
            "Fold 1; Epoch 25: Val Loss =  7.655930228855299\n",
            "Fold 1; Epoch 26: Training Loss =  0.9063491774685044\n",
            "Fold 1; Epoch 26: Val Loss =  7.670564651489258\n",
            "Fold 1; Epoch 27: Training Loss =  0.8924498750780466\n",
            "Fold 1; Epoch 27: Val Loss =  7.576330233311308\n",
            "Fold 1; Epoch 28: Training Loss =  0.8797217930922477\n",
            "Fold 1; Epoch 28: Val Loss =  7.478882098543471\n",
            "Fold 1; Epoch 29: Training Loss =  0.8765462244375085\n",
            "Fold 1; Epoch 29: Val Loss =  7.583084445068802\n",
            "Fold 1; Epoch 30: Training Loss =  0.8704730287448869\n",
            "Fold 1; Epoch 30: Val Loss =  7.6625711952430615\n",
            "Fold 1; Epoch 31: Training Loss =  0.870177398125078\n",
            "Fold 1; Epoch 31: Val Loss =  7.617237243099489\n",
            "Fold 1; Epoch 32: Training Loss =  0.8657754104132813\n",
            "Fold 1; Epoch 32: Val Loss =  7.558121252751005\n",
            "Fold 1; Epoch 33: Training Loss =  0.8654077973548684\n",
            "Fold 1; Epoch 33: Val Loss =  7.631397350974705\n",
            "Fold 1; Epoch 34: Training Loss =  0.8616089713195167\n",
            "Fold 1; Epoch 34: Val Loss =  7.533459946729135\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m j==\u001b[32m0\u001b[39m:\n\u001b[32m     34\u001b[39m \t\u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minitial val loss: \u001b[39m\u001b[33m\"\u001b[39m, trainval(model, device, val_loader, optimizer, criterion, mode=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m loss = \u001b[43mtrainval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m train_loss_current.append(loss)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Training Loss = \u001b[39m\u001b[33m\"\u001b[39m, loss, )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrainval\u001b[39m\u001b[34m(model, device, loader, optimizer, criterion, mode)\u001b[39m\n\u001b[32m     18\u001b[39m         optimizer.step()\n\u001b[32m     19\u001b[39m         optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m total_loss = total_loss / size\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "#load_weights = True\n",
        "\n",
        "for i in range (start_fold if 'start_fold' in globals() else 1, num_fold+1): # 1-indexing\n",
        "\n",
        "\t\n",
        "\tgc.collect()\n",
        "\ttorch.cuda.empty_cache()\n",
        "\tmodel = timm.create_model(\"hf_hub:timm/convnext_tiny.in12k\", pretrained=True)\n",
        "\tmodel.head.fc = nn.Linear(768, 1)\n",
        "\tmodel.to(device)\n",
        "\t\n",
        "\t#model = UNet(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\n",
        "\toptimizer = optim.Adam(model.parameters(), lr = initial_learning_rate)\n",
        "\tscheduler = lr_scheduler.ExponentialLR(optimizer, 0.933)\n",
        "\t\n",
        "\tindices_train, indices_val = cv_split(indices_dayblock_shuffled,i-1,num_fold)\n",
        "\timages_train = images_trainval[indices_train]\n",
        "\tpv_train = pv_trainval[indices_train]\n",
        "\timages_val = images_trainval[indices_val]\n",
        "\tpv_val = pv_trainval[indices_val]\n",
        "\n",
        "\ttrain = PVDataset(images_train, pv_train)\t\n",
        "\tval = PVDataset(images_val, pv_val)\n",
        "\ttrain_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "\tval_loader = DataLoader(val, batch_size=val_batch_size, shuffle=False)\n",
        "\n",
        "\ttrain_loss_current = []\n",
        "\tval_loss_current = []\n",
        "\tpatience = 0\n",
        "\n",
        "\tfor j in range(num_epochs):\n",
        "\t\t\n",
        "\t\tif j==0:\n",
        "\t\t\tprint(\"initial val loss: \", trainval(model, device, val_loader, optimizer, criterion, mode=\"val\"))\n",
        "\n",
        "\t\tloss = trainval(model, device, train_loader, optimizer, criterion, mode=\"train\")\n",
        "\t\ttrain_loss_current.append(loss)\n",
        "\t\t\t\n",
        "\n",
        "\t\tprint(f\"Fold {i}; Epoch {j}: Training Loss = \", loss, )\n",
        "\n",
        "\t\t#if (j % 5 == 0 or j == num_epochs-1):\n",
        "\t\tloss = trainval(model, device, val_loader, optimizer, criterion, mode=\"val\")\n",
        "\t\tif j > 10:\n",
        "\t\t\tif loss > min_loss[i-1]:\n",
        "\t\t\t\tpatience += 1\n",
        "\t\t\t\tif patience == 15:\n",
        "\t\t\t\t\tprint(f\"Fold {i}; Loss stopped decreasing, stopping training.\")\n",
        "\t\t\t\t\tbreak\n",
        "\t\tif loss < min_loss[i-1]:\n",
        "\t\t\tmin_loss[i-1] = loss\n",
        "\t\t\tpatience = 0\n",
        "\t\t\tcheckpoint(model, \"repetitions/best_\" + model.__class__.__name__ + \"model_repetition_\" + str(i) + \".pth\")\n",
        "\t\tval_loss_current.append(loss)\n",
        "\t\t\n",
        "\n",
        "\t\tprint(f\"Fold {i}; Epoch {j}: Val Loss = \", loss, )\n",
        "\t\tscheduler.step()\n",
        "\ttrain_loss_hist.append(train_loss_current)\n",
        "\tval_loss_hist.append(val_loss_current)\n",
        "\tplt.plot(train_loss_hist[i-1],label='train')\n",
        "\tplt.plot(val_loss_hist[i-1],label='validation')\n",
        "\tplt.legend()\n",
        "\tplt.show()\n",
        "\n",
        "\tdel model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "print(patience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "with open(varpath('train_loss_hist.pkl'), 'wb') as file:\n",
        "    pkl.dump(train_loss_hist, file)\n",
        "with open(varpath('val_loss_hist.pkl'), 'wb') as file:\n",
        "    pkl.dump(val_loss_hist, file)\n",
        "with open(varpath('min_loss.pkl'), 'wb') as file:\n",
        "    pkl.dump(min_loss, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del indices_train, indices_val\n",
        "del images_train\n",
        "del pv_train\n",
        "del images_val\n",
        "del pv_val\n",
        "del train\n",
        "del val\n",
        "del train_loader\n",
        "del val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "images_test = np.load(varpath(\"images_test.npy\"))\n",
        "pv_test = np.load(varpath(\"pv_test.npy\"))\n",
        "times_test = np.load(varpath(\"times_test.npy\"), allow_pickle=True)\n",
        "\n",
        "images_test = images_test.transpose(0, 3, 1, 2)\n",
        "\n",
        "images_test = torch.from_numpy(images_test).float()\n",
        "\n",
        "pv_test = torch.from_numpy(pv_test).float()\n",
        "\n",
        "times_test = np.squeeze(times_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsnlWK26_p8S"
      },
      "source": [
        "### Model training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BSHhhi8__p8S"
      },
      "outputs": [],
      "source": [
        "def plot_lr(history):\n",
        "    learning_rate = history.history['lr']\n",
        "    epochs = range(1, len(learning_rate) + 1)\n",
        "    plt.plot(epochs, learning_rate)\n",
        "    plt.title('Learning rate')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float64\n",
            "Feature batch shape: torch.Size([128, 3, 64, 64])\n",
            "Labels batch shape: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# Display image and label.\n",
        "#train_features, train_labels = next(iter(train_loader))\n",
        "#print(train_labels.dtype)\n",
        "#print(f\"Feature batch shape: {train_features.size()}\")\n",
        "#print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "#img = train_features[0].squeeze()\n",
        "#label = train_labels[0]\n",
        "#plt.imshow(img.transpose(1,2,0), cmap=\"gray\")\n",
        "#plt.show()\n",
        "#print(f\"Label: {label}\")\n",
        "\n",
        "#print(img.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8LwAhfKw_p8T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 1  -- train loss: 2.13, validation loss: 2.51 (RMSE)\n",
            "Model 2  -- train loss: 1.90, validation loss: 1.93 (RMSE)\n",
            "Model 3  -- train loss: 2.05, validation loss: 1.92 (RMSE)\n",
            "Model 4  -- train loss: 1.82, validation loss: 2.53 (RMSE)\n",
            "Model 5  -- train loss: 1.93, validation loss: 2.27 (RMSE)\n",
            "Model 6  -- train loss: 1.87, validation loss: 2.25 (RMSE)\n",
            "Model 7  -- train loss: 2.37, validation loss: 2.52 (RMSE)\n",
            "Model 8  -- train loss: 2.61, validation loss: 2.33 (RMSE)\n",
            "Model 9  -- train loss: 2.12, validation loss: 1.75 (RMSE)\n",
            "Model 10  -- train loss: 1.98, validation loss: 2.33 (RMSE)\n",
            "The mean train loss (RMSE) for all models is 2.08\n",
            "The mean validation loss (RMSE) for all models is 2.23\n"
          ]
        }
      ],
      "source": [
        "# summary of training and validation results\n",
        "best_train_loss_MSE = np.zeros(num_fold)\n",
        "best_val_loss_MSE = np.zeros(num_fold)\n",
        "\n",
        "for i in range(num_fold):\n",
        "    best_val_loss_MSE[i] = np.min(val_loss_hist[i])\n",
        "    idx = np.argmin(val_loss_hist[i])\n",
        "    best_train_loss_MSE[i] = train_loss_hist[i][idx]\n",
        "    print('Model {0}  -- train loss: {1:.2f}, validation loss: {2:.2f} (RMSE)'.format(i+1, np.sqrt(best_train_loss_MSE[i]), np.sqrt(best_val_loss_MSE[i])))\n",
        "print('The mean train loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_train_loss_MSE))))\n",
        "print('The mean validation loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_val_loss_MSE))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Test Loss =  5.8380634540861305\n",
            "Fold 2: Test Loss =  5.885873394391753\n",
            "Fold 3: Test Loss =  5.869903418760408\n",
            "Fold 4: Test Loss =  6.055969829721884\n",
            "Fold 5: Test Loss =  6.281514589827169\n",
            "Fold 6: Test Loss =  6.0961378921162\n",
            "Fold 7: Test Loss =  6.040499654886397\n",
            "Fold 8: Test Loss =  5.907315125519579\n",
            "Fold 9: Test Loss =  5.513154856318777\n",
            "Fold 10: Test Loss =  6.008880087326873\n"
          ]
        }
      ],
      "source": [
        "for i in range (1, num_fold+1): # 1-indexing\n",
        "\n",
        "\tdel model, optimizer\n",
        "\tgc.collect()\n",
        "\ttorch.cuda.empty_cache()\n",
        "\t\n",
        "\tmodel = UNet(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\n",
        "\toptimizer = optim.Adam(model.parameters(), lr = initial_learning_rate)\n",
        "\n",
        "\ttest = PVDataset(images_test, pv_test)\t\n",
        "\ttest_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\tmodel_path = 'repetitions/' + 'best_model_repetition_'+str(i)+'.pth'\n",
        "\t# load the trained model\n",
        "\tresume(model, model_path)\n",
        "\n",
        "\tloss = trainval(model, device, test_loader, optimizer, criterion, mode=\"val\")\n",
        "\n",
        "\tprint(f\"Fold {i}: Test Loss = \", loss)\n",
        "\t\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.7024]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.0048)\n",
            "tensor([[2.3459]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.3315)\n",
            "tensor([[2.5306]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.5194)\n",
            "tensor([[3.4859]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.8531)\n",
            "tensor([[3.2123]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.1316)\n",
            "tensor([[2.0521]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.3022)\n",
            "tensor([[2.9972]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.6619)\n",
            "tensor([[3.8655]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.9850)\n",
            "tensor([[3.5333]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.3075)\n",
            "tensor([[5.7348]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.6501)\n",
            "tensor([[3.9446]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.8600)\n",
            "tensor([[2.9676]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.1890)\n",
            "tensor([[5.2048]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.9897)\n",
            "tensor([[7.7223]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(5.9170)\n",
            "tensor([[4.1096]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(6.0269)\n",
            "tensor([[6.3463]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(6.9663)\n",
            "tensor([[8.3611]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.3373)\n",
            "tensor([[7.9316]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(7.9864)\n",
            "tensor([[9.5408]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(9.6600)\n",
            "tensor([[16.9994]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(15.6311)\n",
            "tensor([[15.6808]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.2195)\n",
            "tensor([[18.0373]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.1557)\n",
            "tensor([[17.9192]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.0068)\n",
            "tensor([[18.6484]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.7497)\n",
            "tensor([[19.7071]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(19.2252)\n",
            "tensor([[20.2705]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(19.9222)\n",
            "tensor([[20.7884]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(20.5028)\n",
            "tensor([[21.4766]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(21.1679)\n",
            "tensor([[22.8978]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(21.9739)\n",
            "tensor([[23.3738]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.4044)\n",
            "tensor([[22.9229]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(20.2984)\n",
            "tensor([[22.2444]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(25.3745)\n",
            "tensor([[24.8158]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(20.2316)\n",
            "tensor([[23.8179]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(21.0362)\n",
            "tensor([[23.4092]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(14.9052)\n",
            "tensor([[21.9579]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(27.5502)\n",
            "tensor([[22.9921]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(15.4521)\n",
            "tensor([[24.5549]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(23.2820)\n",
            "tensor([[25.6997]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(26.5275)\n",
            "tensor([[21.7887]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(23.4020)\n",
            "tensor([[23.3613]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.0582)\n",
            "tensor([[24.2417]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(28.6738)\n",
            "tensor([[22.8610]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(21.9514)\n",
            "tensor([[23.5789]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(23.6199)\n",
            "tensor([[24.2780]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(19.3623)\n",
            "tensor([[24.6272]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.5109)\n",
            "tensor([[22.6558]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.3710)\n",
            "tensor([[24.8593]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.1887)\n",
            "tensor([[23.6374]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.6998)\n",
            "tensor([[23.6666]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(22.8630)\n",
            "tensor([[23.4383]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.2586)\n",
            "tensor([[23.2263]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.0865)\n",
            "tensor([[16.1744]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(10.7260)\n",
            "tensor([[16.3653]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.3651)\n",
            "tensor([[13.7611]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.3112)\n",
            "tensor([[11.7747]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(9.9737)\n",
            "tensor([[12.8701]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.3333)\n",
            "tensor([[13.8102]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.9422)\n",
            "tensor([[17.1726]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(17.4458)\n",
            "tensor([[13.9169]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.4140)\n",
            "tensor([[13.4732]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.5229)\n",
            "tensor([[14.4482]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(15.1385)\n",
            "tensor([[9.4249]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(7.7075)\n",
            "tensor([[9.6362]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.2268)\n",
            "tensor([[14.4890]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.0639)\n",
            "tensor([[6.5104]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.6577)\n",
            "tensor([[9.5591]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(5.1559)\n",
            "tensor([[12.6439]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.3025)\n",
            "tensor([[9.7861]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(10.3623)\n",
            "tensor([[5.3763]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.9136)\n",
            "tensor([[8.4632]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(9.4224)\n",
            "tensor([[5.7585]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.5614)\n",
            "tensor([[5.6322]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.9575)\n",
            "tensor([[3.7680]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(5.2910)\n",
            "tensor([[3.8359]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(3.1742)\n",
            "tensor([[3.2593]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.0891)\n",
            "tensor([[2.5972]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(3.7030)\n",
            "tensor([[5.3044]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.7905)\n",
            "tensor([[3.6717]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.6810)\n",
            "tensor([[1.9777]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.7671)\n",
            "tensor([[0.8430]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.2138)\n",
            "tensor([[-0.1057]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.1649)\n",
            "tensor([[0.0090]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.4222)\n",
            "tensor([[0.8746]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.9117)\n",
            "tensor([[1.3118]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.6240)\n",
            "tensor([[2.5478]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.5782)\n",
            "tensor([[3.5434]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(3.6429)\n",
            "tensor([[4.3347]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.8449)\n",
            "tensor([[5.6057]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(6.2251)\n",
            "tensor([[6.3560]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(7.4929)\n",
            "tensor([[7.8976]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.6945)\n",
            "tensor([[8.6945]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(9.8036)\n",
            "tensor([[9.5457]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(10.8166)\n",
            "tensor([[10.6055]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.0240)\n",
            "tensor([[11.6993]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.8768)\n",
            "tensor([[12.3912]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(13.8529)\n",
            "tensor([[13.4415]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(14.8596)\n",
            "tensor([[14.3318]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(15.7607)\n",
            "tensor([[15.5689]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(16.4946)\n",
            "tensor([[16.2570]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(17.4322)\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, 1000, 10):\n",
        "    input = test[i][0].to(device).unsqueeze(0)\n",
        "    print(model(input))\n",
        "    print(test[i][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZAaBHcf_p8T"
      },
      "source": [
        "### Model Testing with in-range Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk4EcIxw_p8T"
      },
      "source": [
        "Test set data are 10 cloudy days drawn from 2017 March to 2019 October, which is the same range as the training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OswWAxd__p8T"
      },
      "source": [
        "#### Feeding Real Future Sky Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSp4kqnq_p8T"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "with h5py.File(data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    images_log_test = f['test']['images_pred'][:,-1]\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # normalize image data to [0,1]\n",
        "    images_data_test = tf.image.convert_image_dtype(images_log_test, dtype=tf.float32).numpy()\n",
        "    images_data_test = mask_background(images_data_test)\n",
        "    pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"images_data_test.shape:\",images_data_test.shape)\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL3SWHlH_p8T",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=images_data_test, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(images_data_test, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Dje5maZ3_p8T"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_validation.npy'))\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "prediction_ensemble = np.mean(prediction,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3s4NKDb_p8T"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = [] # you can put sunny days here if you want to test sunny condition performance\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask for the sunny days\n",
        "mask = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask to the dataset\n",
        "times_test_sunny = times_test[mask]\n",
        "pv_log_test_sunny = pv_log_test[mask]\n",
        "images_log_test_sunny = images_log_test[mask]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask]\n",
        "pv_log_test_cloudy = pv_log_test[~mask]\n",
        "images_log_test_cloudy = images_log_test[~mask]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZsT6lgr_p8T"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdK5Sg62_p8U"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GLR9qiFV_p8U"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MpEgnaq_p8U",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_ensembles\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1, color=black, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1, label = 'SUNSET forecast',color=red,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.2], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(15,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','sunset_forecast_baseline_2017_2019_full_data_trained_2019_test_days.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNPhFObc_p8U"
      },
      "source": [
        "#### Feeding ConvLSTM generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-jP3CLj_p8U"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_ConvLSTM.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images = f['trainval']['images_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    predicted_images = mask_background(predicted_images)\n",
        "    predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images.shape:\",predicted_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlAETbt6_p8U",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_ConvLSTM_gen_images_as_input.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36nVbFOf_p8U"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x01nPo8_p8V"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcG5HHp9_p8V"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FfsTIkKC_p8V"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfdfRmoz_p8V"
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq_e_2FR_p8V"
      },
      "source": [
        "#### Feeding PhyDNet generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycgkr6AC_p8V"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_PhyDNet.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images = f['trainval']['images_pred'][:,-1]\n",
        "    #pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    predicted_images = mask_background(predicted_images)\n",
        "    predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "    #pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images.shape:\",predicted_images.shape)\n",
        "#print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHvi4O3H_p8V",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_PhyDNet_gen_images_as_input.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmZhxOkn_p8W"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLOjQQVX_p8W"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtv2MJzp_p8W"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pPzLgVBe_p8W"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvpdMdcH_p8X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDPatyhe_p8X"
      },
      "source": [
        "#### Feeding PhyDNet+GAN generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRLjvY8p_p8X"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_PhyDNetGAN.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images = f['trainval']['images_pred'][:,-1]\n",
        "    #pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    predicted_images = mask_background(predicted_images)\n",
        "    predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "    #pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images.shape:\",predicted_images.shape)\n",
        "#print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo2QrBoK_p8X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_SkyImageGAN_gen_images_as_input.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-CtFqgs_p8X"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAcy-OnC_p8X"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xaIL0OF_p8X"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(5times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e2DW7bsl_p8Y"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yehVXuSY_p8Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euCa3dq4_p8Y"
      },
      "source": [
        "#### Feeding VideoGPT generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz51jq6o_p8Y"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_VideoGPT.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images_all_samplings = f['trainval']['images_pred_all_samplings'][:,:,-1]\n",
        "    #pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    #predicted_images = mask_background(predicted_images)\n",
        "    #predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "    #pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images_all_samplings.shape:\",predicted_images_all_samplings.shape)\n",
        "#print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QmYOerN_p8Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "num_samplings = 10\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating prediction for sampling {0}'.format(j+1))\n",
        "        predicted_images = predicted_images_all_samplings[j]\n",
        "        predicted_images = mask_background(predicted_images)\n",
        "        predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\".format(i+1))\n",
        "        loss[i,j] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\".format(i+1))\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_VideoGPT_gen_images_as_input.npy'),prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2dFvG4oM_p8Y"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_validation_VideoGPT_gen_images_as_input.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Uv6F9JUH_p8Y"
      },
      "outputs": [],
      "source": [
        "prediction_samp = prediction.reshape(-1,prediction.shape[-1])\n",
        "prediction_samp.shape\n",
        "prediction_ensemble = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1uFkx0z_p8Y"
      },
      "outputs": [],
      "source": [
        "# evaluate deterministic performance\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print('test rmse for all 10 samplings is {0:3f}'.format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulkHszae_p8Z"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOxglfrh_p8Z"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmeZvVgj_p8Z"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "trw4x5Yn_p8Z"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction_samp,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwtUNZqy_p8Z",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w22mxO_3_p8a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMlChrkG_p8a"
      },
      "source": [
        "#### Feeding SkyGPT generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3TOUx5R_p8a"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_SkyGPT.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images_all_samplings = f['trainval']['images_pred_all_samplings'][:,:,-1]\n",
        "\n",
        "print(\"predicted_images_all_samplings.shape:\",predicted_images_all_samplings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7ULQajn_p8a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "num_samplings = 10\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating prediction for sampling {0}'.format(j+1))\n",
        "        predicted_images = predicted_images_all_samplings[j]\n",
        "        #predicted_images = mask_background(predicted_images)\n",
        "        predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\".format(i+1))\n",
        "        loss[i,j] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\".format(i+1))\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_SkyGPT_gen_images_as_input.npy'),prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lpE334pk_p8a"
      },
      "outputs": [],
      "source": [
        "prediction_samp = prediction.reshape(-1,prediction.shape[-1])\n",
        "prediction_samp.shape\n",
        "prediction_ensemble = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQJYOJi7_p8a"
      },
      "outputs": [],
      "source": [
        "# evaluate deterministic performance\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print('test rmse for all 10 samplings is {0:3f}'.format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFR3WwlW_p8b"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQYAeU7i_p8b"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfkF3m37_p8b"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "me_wW9KL_p8b"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction_samp,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLZ_hSLf_p8b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRG-wXH_p8b"
      },
      "source": [
        "### Model Testing with Out-range test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9m5Bm7j_p8b"
      },
      "source": [
        "Test set data are 5 cloudy days drawn from 2019 Nov. and Dec., which is the outside the range of the training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJWhoL-Y_p8b"
      },
      "source": [
        "#### Feeding Real Future Sky Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpFAG13X_p8b"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "data_folder = os.path.join(pardir,\"data\")\n",
        "test_data_path = os.path.join(data_folder,'test_set_2019nov_dec.hdf5')\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test_2019nov_dec.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "model_name = 'UNet_Image_PV_mapping_masked_image'\n",
        "output_folder = os.path.join(cwd,\"model_output\", model_name)\n",
        "\n",
        "with h5py.File(test_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    images_log_test = f['test']['images_pred'][:,-1]\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    images_log_test = mask_background(images_log_test)\n",
        "    images_log_test = tf.image.convert_image_dtype(images_log_test, dtype=tf.float32).numpy()\n",
        "    pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"images_log_test.shape:\",images_log_test.shape)\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)\n",
        "#print(\"pv_pred_test.shape:\",pv_pred_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nnp4gYd6_p8c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=images_log_test, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(images_log_test, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_2019nov_dec.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofk4Ao4O_p8c"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = [] # you can put sunny days here if you want to test sunny condition performance\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "prediction_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "prediction_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFwQErvz_p8c"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqmLX5WY_p8c"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YckeyUHA_p8c"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKifyK5S_p8c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(15,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL_m4edv_p8c"
      },
      "source": [
        "#### Feeding ConvLSTM Generated Images as Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z7M2bVwr_p8c"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','ConvLSTM','save','ConvLSTM_sky_image_dataset_interval_2min_all_data_v2')\n",
        "predicted_image = np.load(os.path.join(predicted_image_folder,'predicted_images_new_test_set_2019.npy'),allow_pickle=True)\n",
        "predicted_image = predicted_image[:,-1]\n",
        "predicted_image = mask_background(predicted_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwO-l2lk_p8d"
      },
      "outputs": [],
      "source": [
        "predicted_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_iL-ySA_p8d"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "data_folder = os.path.join(pardir,\"data\")\n",
        "test_data_path = os.path.join(data_folder,'test_set_2019nov_dec.hdf5')\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test_2019nov_dec.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "with h5py.File(test_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    images_log_test = f['test']['images_pred'][:,-1]\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "# process image data\n",
        "images_log_test = mask_background(images_log_test)\n",
        "images_log_test = tf.image.convert_image_dtype(images_log_test, dtype=tf.float32).numpy()\n",
        "pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"images_log_test.shape:\",images_log_test.shape)\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)\n",
        "#print(\"pv_pred_test.shape:\",pv_pred_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AGtQz1I_p8d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_image, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_image, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_ConvLSTM_generated_imgs_as_input_new.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "#print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the model\".format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzj28_aA_p8d"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "#pv_pred_test_sunny = pv_pred_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "images_log_test_sunny = images_log_test[mask3]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "images_log_test_cloudy = images_log_test[~mask3]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEfj1S43_p8d"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJRoSa9Q_p8d"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kZpcwXP4_p8d"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08nqCXSE_p8e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoWlyEi_p8e"
      },
      "source": [
        "#### Feeding PhyDNet Generated Images as Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVS6JX7w_p8e"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','PhyDNet','save','PhyDNet_sky_image_dataset_interval_2min_all_data_v2')\n",
        "predicted_image = np.load(os.path.join(predicted_image_folder,'predicted_images_new_test_set_2019.npy'),allow_pickle=True)\n",
        "predicted_image = mask_background(predicted_image)\n",
        "predicted_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az1maKm7_p8f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_PhyDNet_generated_imgs_as_input_new.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "#print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the model\".format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBq3ae4k_p8f"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "#pv_pred_test_sunny = pv_pred_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "images_log_test_sunny = images_log_test[mask3]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "images_log_test_cloudy = images_log_test[~mask3]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyyW-4hi_p8f"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATkN2joW_p8f"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w46dZibj_p8g"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPSLKBMk_p8g",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVq3w-Ju_p8g"
      },
      "source": [
        "#### Feeding PhyDNet+GAN Generated Images as Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "varbMhwQ_p8g"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','PhyDNet','save','PhyDNet_LSGAN_sky_image_dataset_gen_lr_0.001_batch_size_16_model_v2_scheduled_and_reverse_scheduled_sampling_MAE_loss_all_data_v3')\n",
        "predicted_image = np.load(os.path.join(predicted_image_folder,'predicted_images_new_test_set_2019.npy'),allow_pickle=True)\n",
        "predicted_image = mask_background(predicted_image)\n",
        "predicted_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvTJkS7h_p8g"
      },
      "outputs": [],
      "source": [
        "plt.imshow(predicted_image[0,0,:,:,::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEhT2y24_p8g",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_SkyImageGAN_generated_imgs_as_input_new.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "#print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the model\".format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VCFNicH_p8g"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "#pv_pred_test_sunny = pv_pred_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "images_log_test_sunny = images_log_test[mask3]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "images_log_test_cloudy = images_log_test[~mask3]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krH_iCwC_p8g"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR-fqpty_p8g"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B04lOJ3b_p8h"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWzpTqDv_p8h",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiS3fR8B_p8h"
      },
      "source": [
        "#### Feeding VideoGPT Generated Images as Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmBGQhME_p8h"
      },
      "source": [
        "We experiment with different number of futures generated, ranging from 1 future to 50 different futures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "t6QGhFq-_p8h"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','VideoGPT','inference','VideoGPT_2019_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L-YRRCW_p8h",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "num_samplings = 50\n",
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating predictions for sample {0} ...'.format(j+1))\n",
        "        # load predicted image samples\n",
        "        predicted_image = np.load(os.path.join(predicted_image_folder,'sample_'+str(j)+'.npy'),allow_pickle=True)\n",
        "        print(np.min(predicted_image))\n",
        "        print(np.max(predicted_image))\n",
        "        predicted_image = mask_background(predicted_image)\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\")\n",
        "        loss[i,j] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=200, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\")\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=200, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_VideoGPT_generated_imgs_as_input_new.npy'),prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4XdtHj6_p8i"
      },
      "source": [
        "##### Use 1 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwf_O5MB_p8i"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "data_folder = os.path.join(pardir,\"data\")\n",
        "test_data_path = os.path.join(data_folder,'test_set_2019nov_dec.hdf5')\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test_2019nov_dec.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "with h5py.File(test_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbigmsWF_p8i"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_VideoGPT_generated_imgs_as_input_new.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XF75qjU_p8i"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 1\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcqUIX-u_p8i"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 1 sampling\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 1 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbXULns0_p8i"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Yoy5Pn_p8i"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "91no2zm5_p8j"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nXzfRtiu_p8j"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnKL5FFZ_p8j",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-G8BjEd_p8j"
      },
      "source": [
        "##### Use 5 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ3vI-yQ_p8j"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_VideoGPT_generated_imgs_as_input_new.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr8f9qGe_p8j"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 5\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d-Lksd1_p8j"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_5samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 5 samplings for all sub-models\".format(loss_rmse_5samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZTL-JLV_p8j"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZNRQLFR_p8j"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o7_MDTye_p8j"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9qE7Nym_p8j"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q3Y0u9Ul_p8j"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrlddXqu_p8j",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rQmZO8Rq_p8j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLW3Lcj1_p8j"
      },
      "source": [
        "##### Use 10 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ogWopQM_p8j"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 10\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h62apQX__p8j"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 10 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOVew5LH_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-M_TPE1_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N4C8MJ_i_p8k"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhgUnX5F_p8k"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xCT1VXSb_p8k"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx2HwoW1_p8k",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLXrigmU_p8k"
      },
      "source": [
        "##### Use 20 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMAORW_F_p8k"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 20\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ZhH64Z_p8k"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtV9-eBV_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4vXpldz_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jNwZr80L_p8k"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wUwvPJ86_p8k"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTXLHkZW_p8l",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7uyBh5h_p8l"
      },
      "source": [
        "##### Use 30 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFZ87K_H_p8l"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 30\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUpOdy9R_p8l"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXGxdAf9_p8l"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSHqbdSr_p8l"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N8SuTeN2_p8l"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HIreTsgZ_p8l"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKxDi0S-_p8l",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPbcY--H_p8m"
      },
      "source": [
        "##### Use 40 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc9SiTNA_p8m"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 40\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0ib2iwR_p8m"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDX3_x4m_p8m"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EStvMzf7_p8m"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sa54VvIN_p8m"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X8vRQ-Kc_p8m"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjeOMhbC_p8m",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68hZ1P5q_p8m"
      },
      "source": [
        "##### Use 50 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy_P-pMp_p8m"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 50\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqeoJNE9_p8m"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BESrzj3h_p8m"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ockrRjCS_p8n"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ly0NhQmz_p8n"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KjtC-3Wt_p8n"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve1vob9T_p8n",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnxqk2Xo_p8n"
      },
      "source": [
        "#### Feeding SkyGPT Generated Images as Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2ECEckA_p8n"
      },
      "source": [
        "Similarly, we experimented with different number of generated future scenarios, ranging from 1 future to 50 futures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jVdXuW24_p8n"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','SkyGPT','inference','SkyGPT_2019_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fol6pypp_p8n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "num_samplings = 50\n",
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating predictions for sample {0} ...'.format(j+1))\n",
        "        # load predicted image samples\n",
        "        predicted_image = np.load(os.path.join(predicted_image_folder,'sample_'+str(j)+'.npy'),allow_pickle=True)\n",
        "\n",
        "        # scale back all the pixel values back to [0,1] with clipping\n",
        "        predicted_image = np.clip(predicted_image,-0.5,0.5)+0.5\n",
        "        predicted_image = mask_background(predicted_image)\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\")\n",
        "        loss[i,j] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=200, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\")\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=200, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_SkyGPT_generated_imgs_as_input_new.npy'),prediction)\n",
        "    # loss averaged over all samplings for model 1\n",
        "    #loss_rmse = np.sqrt(np.mean((np.mean(prediction[i],axis=0)-pv_log_test)**2))\n",
        "    #print(\"the test set RMSE is {0:.3f} averaged over all samplings for sub-model {1}\".format(loss_rmse,i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvB2sy83_p8n"
      },
      "source": [
        "##### Use 1 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHBJYwRy_p8n"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_SkyGPT_generated_imgs_as_input_new.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLsV9eVr_p8n"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 1\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uC4V7Ao_p8n"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_1samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 1 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGHK6VK-_p8n"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftcd0ue3_p8n"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TN6IsPsq_p8n"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WrcHKV3_p8n"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tPwCo0cs_p8n"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9hE6iuu_p8n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    #rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_samp[date_mask]))))\n",
        "    #mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_samp[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_samp[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp5jEquj_p8o"
      },
      "source": [
        "##### Use 5 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SblfEftx_p8o"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_SkyGPT_generated_imgs_as_input.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKqHmSqr_p8o"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 5\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqOQs1X7_p8o"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_5samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 5 samplings for all sub-models\".format(loss_rmse_5samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDq4LRoH_p8o"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQG2c2e5_p8o"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mPTdMKBC_p8o"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U1EQBEB_p8o"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Va9bzY5e_p8o"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB7xgu5X_p8o",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Mq1rKGCa_p8p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JGt7rs_p8p"
      },
      "source": [
        "##### Use 10 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtCC9__O_p8p"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 10\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX50RwJ__p8p"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 10 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1PLMMsb_p8p"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--n4GntH_p8p"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ke23VcUu_p8p"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA0qcqAM_p8p"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BzLCKS57_p8p"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOa5arbH_p8p",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5qdjmr3_p8q"
      },
      "source": [
        "##### Use 20 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvQd7cC5_p8q"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 20\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhuffEVx_p8q"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYz9hUS6_p8q"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAUYhDir_p8q"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ARCgvwu5_p8q"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0yHZC6wX_p8q"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6QpwUnn_p8q",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    #rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_samp[date_mask]))))\n",
        "    #mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_samp[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_samp[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhE4FDzj_p8q"
      },
      "source": [
        "##### Use 30 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3jeKgVe_p8q"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 30\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHdbuMtI_p8q"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNWRUW4G_p8q"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6CuhuJj_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hCS1lPS8_p8r"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vI1R08r7_p8r"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZXYOYjA_p8r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxbODDoH_p8r"
      },
      "source": [
        "##### Use 40 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq1tRNV5_p8r"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 40\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejycpmjd_p8r"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67vLci5I_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDogRw7z_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0MOtUcwV_p8r"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-8YwfkbX_p8r"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFkRDXVv_p8r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsIqEMww_p8r"
      },
      "source": [
        "##### Use 50 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTjUtFh__p8r"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 50\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvLZN-mG_p8r"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW3xLonf_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSKeYWe4_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1AKSqNVr_p8r"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-19KtFuX_p8r"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLGinsua_p8r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
