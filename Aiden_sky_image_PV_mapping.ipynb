{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8MWBOKc_p8N"
      },
      "source": [
        "### Libraries and data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "lZ8wnidH_p8O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "import h5py\n",
        "import math\n",
        "import matplotlib.dates as mdates\n",
        "import numpy.ma as ma\n",
        "import CRPS.CRPS as pscore\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.image as mpimg\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "import pickle as pkl\n",
        "import tables\n",
        "import hdf5plugin\n",
        "#writer = SummaryWriter()\n",
        "\n",
        "%matplotlib inline\n",
        "#%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.1+cu128\n",
            "CUDA available: True\n",
            "CUDA version: 12.8\n",
            "GPU count: 1\n",
            "GPU name: NVIDIA GeForce RTX 4070 Ti\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "# Check if CUDA is available\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "\n",
        "# Get GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "sunny_day = [(2017,9,15),(2017,10,6),(2017,10,22),(2018,2,16),(2018,6,12),(2018,6,23),(2019,1,25),(2019,6,23),(2019,7,14),(2019,10,14)]\n",
        "cloudy_day = [(2017,6,24),(2017,9,20),(2017,10,11),(2018,1,25),(2018,3,9),(2018,10,4),(2019,5,27),(2019,6,28),(2019,8,10),(2019,10,19)]\n",
        "\n",
        "sunny_datetime = [datetime.datetime(day[0],day[1],day[2]) for day in sunny_day]\n",
        "cloudy_datetime = [datetime.datetime(day[0],day[1],day[2]) for day in cloudy_day]\n",
        "test_dates = sunny_datetime + cloudy_datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[datetime.datetime(2017, 9, 15, 0, 0),\n",
              " datetime.datetime(2017, 10, 6, 0, 0),\n",
              " datetime.datetime(2017, 10, 22, 0, 0),\n",
              " datetime.datetime(2018, 2, 16, 0, 0),\n",
              " datetime.datetime(2018, 6, 12, 0, 0),\n",
              " datetime.datetime(2018, 6, 23, 0, 0),\n",
              " datetime.datetime(2019, 1, 25, 0, 0),\n",
              " datetime.datetime(2019, 6, 23, 0, 0),\n",
              " datetime.datetime(2019, 7, 14, 0, 0),\n",
              " datetime.datetime(2019, 10, 14, 0, 0),\n",
              " datetime.datetime(2017, 6, 24, 0, 0),\n",
              " datetime.datetime(2017, 9, 20, 0, 0),\n",
              " datetime.datetime(2017, 10, 11, 0, 0),\n",
              " datetime.datetime(2018, 1, 25, 0, 0),\n",
              " datetime.datetime(2018, 3, 9, 0, 0),\n",
              " datetime.datetime(2018, 10, 4, 0, 0),\n",
              " datetime.datetime(2019, 5, 27, 0, 0),\n",
              " datetime.datetime(2019, 6, 28, 0, 0),\n",
              " datetime.datetime(2019, 8, 10, 0, 0),\n",
              " datetime.datetime(2019, 10, 19, 0, 0)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJYAy-t7_p8P",
        "outputId": "abc24607-f813-4790-9773-6dd4494b3283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data_folder: d:\\PVOutputPrediction\\data\n",
            "data_path: d:\\PVOutputPrediction\\video_prediction_224_second.h5\n",
            "output_folder: d:\\PVOutputPrediction\\model_output\\UNet_sky_image_PV_mapping\n"
          ]
        }
      ],
      "source": [
        "# define the data location and load data\n",
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "data_folder = os.path.join(cwd,\"data\")\n",
        "data_path = os.path.join(cwd,'video_prediction_224_second.h5')\n",
        "\n",
        "# !change model name for different models!\n",
        "model_name = 'UNet_sky_image_PV_mapping'\n",
        "output_folder = os.path.join(cwd,\"model_output\", model_name)\n",
        "if os.path.isdir(output_folder)==False:\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "image_name_format = '%Y%m%d%H%M%S'\n",
        "\n",
        "# Operating parameter\n",
        "stack_height = 15 # 15 minute\n",
        "forecast_horizon = 15 # 15 minutes ahead forecast\n",
        "sampling_interval_all = [2]\n",
        "output_img_shape = [224, 224, 3]\n",
        "\n",
        "print(\"data_folder:\", data_folder)\n",
        "print(\"data_path:\", data_path)\n",
        "print(\"output_folder:\", output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 22.4 GiB for an array with shape (10000, 16, 224, 224, 3) and data type uint8",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m dataset = f[\u001b[33m'\u001b[39m\u001b[33mtrainval/image_log\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Use slicing to load the data into a NumPy array\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m data_array = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\PVOutputPrediction\\pytorch_env\\Lib\\site-packages\\h5py\\_hl\\dataset.py:820\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, args, new_dtype)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fast_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mh5py/_selector.pyx:368\u001b[39m, in \u001b[36mh5py._selector.Reader.read\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mh5py/_selector.pyx:342\u001b[39m, in \u001b[36mh5py._selector.Reader.make_array\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mMemoryError\u001b[39m: Unable to allocate 22.4 GiB for an array with shape (10000, 16, 224, 224, 3) and data type uint8"
          ]
        }
      ],
      "source": [
        "data_path = os.path.join(cwd,'video_prediction_224_second.h5')\n",
        "with h5py.File(data_path, 'r') as f:\n",
        "    # Access the dataset directly by its path\n",
        "    # This creates a dataset object, but does not load the data yet\n",
        "    dataset = f['trainval/image_log']\n",
        "    \n",
        "    # Use slicing to load the data into a NumPy array\n",
        "    data_array = dataset[0:10000]\n",
        "    \n",
        "    print(f\"Dataset shape: {dataset.shape}\")\n",
        "    print(f\"Data type: {dataset.dtype}\")\n",
        "    print(f\"First 5 rows:\\n{data_array[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJKp70Oy_p8P",
        "outputId": "815bb243-8fd5-4be2-9d8a-08acbdb9236a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<HDF5 group \"/test\" (0 members)>\n",
            "<HDF5 group \"/trainval\" (2 members)>\n",
            "<HDF5 dataset \"image_log\": shape (8000, 16, 224, 224, 3), type \"|u1\">\n",
            "<HDF5 dataset \"pv_log\": shape (8000, 16), type \"<f8\">\n"
          ]
        }
      ],
      "source": [
        "# generate handler for the hdf5 data\n",
        "forecast_dataset = h5py.File(data_path, 'r')\n",
        "\n",
        "# show structure of the hdf5 data\n",
        "def get_all(name):\n",
        "    if name!=None:\n",
        "        print(forecast_dataset[name])\n",
        "\n",
        "forecast_dataset.visit(get_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_samples = 317968"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    forecast_dataset.close()\n",
        "except Exception as e:\n",
        "    print(f\"Couldn't close the file (which is okay): {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 6  6  6]\n",
            "   ...\n",
            "   [ 8 10 11]\n",
            "   [ 7  9 10]\n",
            "   [ 7  9 10]]\n",
            "\n",
            "  [[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 6  6  6]\n",
            "   ...\n",
            "   [11 10 12]\n",
            "   [10  9 11]\n",
            "   [10  9 11]]\n",
            "\n",
            "  [[ 5  5  5]\n",
            "   [ 5  5  5]\n",
            "   [ 6  6  6]\n",
            "   ...\n",
            "   [10  9 11]\n",
            "   [10  9 11]\n",
            "   [10  9 11]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 4  3  5]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 8  5  7]\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]]\n",
            "\n",
            "  [[ 4  3  5]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 8  5  7]\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]]\n",
            "\n",
            "  [[ 4  3  5]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 8  5  7]\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]]]\n",
            "\n",
            "\n",
            " [[[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 5  5  5]\n",
            "   ...\n",
            "   [10 10 10]\n",
            "   [ 9  9  9]\n",
            "   [ 9  9  9]]\n",
            "\n",
            "  [[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 5  5  5]\n",
            "   ...\n",
            "   [12 10 10]\n",
            "   [11  9  9]\n",
            "   [11  9  9]]\n",
            "\n",
            "  [[ 6  6  6]\n",
            "   [ 6  6  6]\n",
            "   [ 6  6  6]\n",
            "   ...\n",
            "   [12 10 10]\n",
            "   [11  9  9]\n",
            "   [11  9  9]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 4  3  5]\n",
            "   [ 3  2  6]\n",
            "   [ 3  2  4]\n",
            "   ...\n",
            "   [ 8  5  7]\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]]\n",
            "\n",
            "  [[ 2  2  2]\n",
            "   [ 3  2  4]\n",
            "   [ 4  3  5]\n",
            "   ...\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]]\n",
            "\n",
            "  [[ 2  3  1]\n",
            "   [ 3  3  3]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]]]\n",
            "\n",
            "\n",
            " [[[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 6  6  6]\n",
            "   ...\n",
            "   [ 7  9 10]\n",
            "   [ 7  9 10]\n",
            "   [ 7  9 10]]\n",
            "\n",
            "  [[ 4  4  4]\n",
            "   [ 5  5  5]\n",
            "   [ 6  6  6]\n",
            "   ...\n",
            "   [10  9 11]\n",
            "   [10  9 11]\n",
            "   [10  9 11]]\n",
            "\n",
            "  [[ 6  6  6]\n",
            "   [ 6  6  6]\n",
            "   [ 6  6  6]\n",
            "   ...\n",
            "   [10  9 11]\n",
            "   [10  9 11]\n",
            "   [10  9 11]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 6  4  4]\n",
            "   [ 6  4  4]\n",
            "   [ 6  4  4]\n",
            "   ...\n",
            "   [ 7  6  8]\n",
            "   [ 6  5  7]\n",
            "   [ 6  5  7]]\n",
            "\n",
            "  [[ 6  4  4]\n",
            "   [ 6  4  4]\n",
            "   [ 6  4  4]\n",
            "   ...\n",
            "   [ 6  5  7]\n",
            "   [ 6  5  7]\n",
            "   [ 6  5  7]]\n",
            "\n",
            "  [[ 6  4  4]\n",
            "   [ 6  4  4]\n",
            "   [ 6  4  4]\n",
            "   ...\n",
            "   [ 6  5  7]\n",
            "   [ 6  5  7]\n",
            "   [ 6  5  7]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 5  6  4]\n",
            "   [ 5  6  4]\n",
            "   [ 5  6  4]]\n",
            "\n",
            "  [[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 7  5  4]\n",
            "   [ 7  5  4]\n",
            "   [ 7  5  4]]\n",
            "\n",
            "  [[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 7  5  4]\n",
            "   [ 7  5  4]\n",
            "   [ 7  5  4]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 3  3  3]\n",
            "   [ 3  3  3]\n",
            "   [ 3  3  3]\n",
            "   ...\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]]\n",
            "\n",
            "  [[ 3  3  3]\n",
            "   [ 3  3  3]\n",
            "   [ 3  3  3]\n",
            "   ...\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]]\n",
            "\n",
            "  [[ 3  3  3]\n",
            "   [ 3  3  3]\n",
            "   [ 3  3  3]\n",
            "   ...\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]]]\n",
            "\n",
            "\n",
            " [[[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 6  6  6]\n",
            "   [ 5  5  5]\n",
            "   [ 5  5  5]]\n",
            "\n",
            "  [[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 7  4  6]\n",
            "   [ 6  3  5]\n",
            "   [ 6  3  5]]\n",
            "\n",
            "  [[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]\n",
            "   [ 7  4  6]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 5  3  3]\n",
            "   [ 5  3  3]\n",
            "   [ 5  3  3]\n",
            "   ...\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]]\n",
            "\n",
            "  [[ 5  3  3]\n",
            "   [ 5  3  3]\n",
            "   [ 5  3  3]\n",
            "   ...\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]]\n",
            "\n",
            "  [[ 5  3  3]\n",
            "   [ 5  3  3]\n",
            "   [ 5  3  3]\n",
            "   ...\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]]]\n",
            "\n",
            "\n",
            " [[[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 7  5  5]\n",
            "   [ 7  5  5]\n",
            "   [ 7  5  5]]\n",
            "\n",
            "  [[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 8  3  5]\n",
            "   [ 8  3  5]\n",
            "   [ 8  3  5]]\n",
            "\n",
            "  [[ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   [ 4  4  4]\n",
            "   ...\n",
            "   [ 9  4  6]\n",
            "   [ 6  5  7]\n",
            "   [ 6  5  7]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 5  3  3]\n",
            "   [ 3  3  3]\n",
            "   [ 3  3  3]\n",
            "   ...\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]]\n",
            "\n",
            "  [[ 5  3  3]\n",
            "   [ 3  3  3]\n",
            "   [ 3  3  3]\n",
            "   ...\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]]\n",
            "\n",
            "  [[ 5  3  3]\n",
            "   [ 3  3  3]\n",
            "   [ 3  3  3]\n",
            "   ...\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]\n",
            "   [ 5  2  4]]]]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = forecast_dataset[\"trainval\"][\"image_log\"][0]\n",
        "print(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to open 'video_prediction_224_second.h5'...\n",
            "File opened successfully.\n",
            "Listing top-level contents:\n",
            "  - Found object: test\n",
            "  - Found object: trainval\n",
            "\n",
            "SUCCESS: The file structure is readable and appears valid.\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "\n",
        "# Directly specify the file you want to check\n",
        "file_path = 'video_prediction_224_second.h5'\n",
        "\n",
        "print(f\"Attempting to open '{file_path}'...\")\n",
        "\n",
        "try:\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        print(\"File opened successfully.\")\n",
        "        print(\"Listing top-level contents:\")\n",
        "        for key in f.keys():\n",
        "            print(f\"  - Found object: {key}\")\n",
        "    print(\"\\nSUCCESS: The file structure is readable and appears valid.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: The file is likely corrupt.\")\n",
        "    print(f\"-> Details: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accessing object 'test'... OK\n",
            "Accessing object 'trainval'... OK\n",
            "\n",
            "SUCCESS: File is accessible and appears to be valid.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with forecast_dataset as f:\n",
        "        # This loop attempts to access every top-level dataset.\n",
        "        for key in f.keys():\n",
        "            print(f\"Accessing object '{key}'... OK\")\n",
        "    print(\"\\nSUCCESS: File is accessible and appears to be valid.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: File is likely corrupt. Failed with error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new datasets\n"
          ]
        }
      ],
      "source": [
        "import hdf5plugin\n",
        "chunk_shape = (1, stack_height+1, *output_img_shape)\n",
        "\n",
        "batch_size = 8000\n",
        "with h5py.File('dummyfile.h5', 'w') as f:\n",
        "\n",
        "\t#image_log = f.create_dataset('image_log', shape = (n_images, 224, 224, 3), dtype ='uint8', )\n",
        "\n",
        "        # First run - create new datasets\n",
        "\n",
        "\ttrainval_group = f.create_group('trainval')\n",
        "\ttest_group = f.create_group('test')\n",
        "\timage_log_trainval_ds = trainval_group.create_dataset(\n",
        "\t\t'image_log',\n",
        "\t\tshape=(5, stack_height+1, *output_img_shape),\n",
        "\t\tchunks=chunk_shape,\n",
        "\t\tcompression=hdf5plugin.Blosc2(cname='zstd', clevel=1, filters=hdf5plugin.Blosc2.SHUFFLE),\n",
        "\t\tdtype='uint8'\n",
        "\t)\n",
        "\n",
        "\tprint(\"Creating new datasets\")\n",
        "\tfor i in range(4):\n",
        "\t\timage_log_trainval_ds[i:i+1] = np.ones((1, stack_height+1, *output_img_shape), dtype='uint8')\n",
        "\t\tf.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "dummydataset = h5py.File(\"dummyfile.h5\", 'r')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# ds = load_dataset(\"skyimagenet/SKIPPD\")\n",
        "\n",
        "# images_train =[]\n",
        "# pv_train = []\n",
        "# times_train = []\n",
        "# images_test =[]\n",
        "# pv_test = []\n",
        "# times_test = []\n",
        "# for mode in [\"train\", \"test\"]:\n",
        "    # globals()[f'images_{mode}'].append(np.array(ds[mode][:]['image']))\n",
        "    # globals()[f'pv_{mode}'].append(ds[mode][:]['pv'])\n",
        "    # globals()[f'times_{mode}'] = ds[mode][:]['time']\n",
        "\n",
        "# for mode in [\"train\", \"test\"]:\n",
        "#     globals()[f'images_{mode}'] = np.array(globals()[f'images_{mode}'])\n",
        "#     globals()[f'pv_{mode}'] = np.array(globals()[f'pv_{mode}'] )\n",
        "    \n",
        "# images_train = np.squeeze(images_train)\n",
        "# pv_train = np.squeeze(pv_train)\n",
        "# times_train = np.squeeze(times_train)\n",
        "\n",
        "# images_test = np.squeeze(images_test)\n",
        "# pv_test = np.squeeze(pv_test)\n",
        "# times_test = np.squeeze(times_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.save(\"times_train.npy\", np.array(times_train))\n",
        "# np.save(\"times_test.npy\", np.array(times_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def varpath(txt):\n",
        "    return(os.path.join(\"variables\", txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "images_trainval = np.load(varpath(\"images_trainval.npy\"))\n",
        "pv_trainval = np.load(varpath(\"pv_trainval.npy\"))\n",
        "times_trainval = np.load(varpath(\"times_trainval.npy\"), allow_pickle=True)\n",
        "\n",
        "# images_test = np.load(\"images_test.npy\")\n",
        "# pv_test = np.load(\"pv_test.npy\")\n",
        "# times_test = np.load(\"times_test.npy\", allow_pickle=True)\n",
        "\n",
        "images_trainval = images_trainval.transpose(0, 3, 1, 2)\n",
        "# images_test = images_test.transpose(0, 3, 1, 2)\n",
        "images_trainval = torch.from_numpy(images_trainval).float()\n",
        "# images_test = torch.from_numpy(images_test).float()\n",
        "pv_trainval = torch.from_numpy(pv_trainval).float()\n",
        "# pv_test = torch.from_numpy(pv_test).float()\n",
        "times_trainval = np.squeeze(times_trainval)\n",
        "# times_test = np.squeeze(times_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkxa-UKU_p8Q"
      },
      "source": [
        "### Input data pipeline helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "ODMVPPkP_p8Q"
      },
      "outputs": [],
      "source": [
        "# day block shuffling of the time stamps, and return shuffled indices\n",
        "def day_block_shuffle(times_trainval):\n",
        "\n",
        "    # Only keep the date of each time point\n",
        "    dates_trainval = np.zeros_like(times_trainval, dtype=datetime.date)\n",
        "    for i in range(len(times_trainval)):\n",
        "        dates_trainval[i] = times_trainval[i].date()\n",
        "\n",
        "    # Chop the indices into blocks, so that each block contains the indices of the same day\n",
        "    unique_dates = np.unique(dates_trainval)\n",
        "    blocks = []\n",
        "    for i in range(len(unique_dates)):\n",
        "        blocks.append(np.where(dates_trainval == unique_dates[i])[0])\n",
        "\n",
        "    # shuffle the blocks, and chain it back together\n",
        "    np.random.seed(1)\n",
        "    np.random.shuffle(blocks)\n",
        "    shuffled_indices = np.asarray(list(itertools.chain.from_iterable(blocks)))\n",
        "\n",
        "    return shuffled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "DDLHlv41_p8Q"
      },
      "outputs": [],
      "source": [
        "# a cross validation generator function for spliting the dayblock shuffled indices into training and validation\n",
        "def cv_split(split_data, fold_index, num_fold):\n",
        "    '''\n",
        "    input:\n",
        "    split_data: the dayblock shuffled indices to be splitted\n",
        "    fold_index: the ith fold chosen as the validation, used for generating the seed for random shuffling\n",
        "    num_fold: N-fold cross validation\n",
        "    output:\n",
        "    data_train: the train data indices\n",
        "    data_val: the validation data indices\n",
        "    '''\n",
        "    # randomly divides into a training set and a validation set\n",
        "    num_samples = len(split_data)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    # finding training and validation indices\n",
        "    val_mask = np.zeros(len(indices), dtype=bool)\n",
        "    val_mask[int(fold_index / num_fold * num_samples):int((fold_index + 1) / num_fold * num_samples)] = True\n",
        "    val_indices = indices[val_mask]\n",
        "    train_indices = indices[np.logical_not(val_mask)]\n",
        "\n",
        "    # shuffle indices\n",
        "    np.random.seed(fold_index)\n",
        "    np.random.shuffle(train_indices)\n",
        "    np.random.shuffle(val_indices)\n",
        "\n",
        "    data_train = split_data[train_indices]\n",
        "    data_val = split_data[val_indices]\n",
        "\n",
        "    return data_train,data_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "kehtU2z9_p8R"
      },
      "outputs": [],
      "source": [
        "def mask_background(img): # put all background pixels (the ones outside the circle region of sky images) to 0s\n",
        "    mask = torch.ones((3,64,64), dtype=bool)\n",
        "    for i in range(64):\n",
        "        for j in range(64):\n",
        "            if (i-30)**2+(j-30)**2>=31**2:\n",
        "                mask[:,i,j]=0\n",
        "    mask_img = img*mask\n",
        "    return mask_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z24FGOTmB3LY"
      },
      "outputs": [],
      "source": [
        "class PVDataset(Dataset):\n",
        "\n",
        "    def __init__(self, images, pv, transform=None, target_transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images = mask_background(images)\n",
        "        self.images = self.images/255\n",
        "        self.pv = pv\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = self.images[idx]\n",
        "        pv = self.pv[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            pv = self.target_transform(pv)\n",
        "        return image, pv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "on2fDoJf_p8S"
      },
      "outputs": [],
      "source": [
        "def compute_winkler_score(prob_prediction,observation):\n",
        "    alpha = 0.1\n",
        "    lb = np.percentile(prob_prediction,5,axis=0)\n",
        "    ub = np.percentile(prob_prediction,95,axis=0)\n",
        "    delta = ub-lb\n",
        "    if observation<lb:\n",
        "        sc = delta+2*(lb-observation)/alpha\n",
        "    if observation>ub:\n",
        "        sc = delta+2*(observation-ub)/alpha\n",
        "    if (observation>=lb) and (observation<=ub):\n",
        "        sc = delta\n",
        "    return sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twl8BbH8_p8S"
      },
      "source": [
        "### Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "93-c3tU7_p8S"
      },
      "outputs": [],
      "source": [
        "# define training time parameters\n",
        "num_filters = 12\n",
        "num_epochs = 200 #(The maximum epoches set to 200 and there might be early stopping depends on validation loss)\n",
        "num_fold = 10 # 10-fold cross-validation\n",
        "batch_size = 256\n",
        "val_batch_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'class ViT(nn.Module):\\n    \"\"\"\\n    U-Net style architecture for Image-to-PV prediction\\n    \"\"\"\\n    def __init__(self, image_input_dim, num_filters=12):\\n        super(ViT, self).__init__()\\n\\n        # Assuming image_input_dim is (height, width, channels)\\n        # PyTorch uses (channels, height, width)\\n        if len(image_input_dim) == 3:\\n            input_channels = image_input_dim[0]  # Assuming CHW format\\n        else:\\n            input_channels = image_input_dim[2]  # Assuming HWC format\\n\\n        # Initial 1x1 convolution\\n        self.conv1x1_input = nn.Conv2d(input_channels, num_filters, kernel_size=1, padding=0)\\n\\n        # Encoder (contracting path)\\n        self.conv3x3_1 = Conv3x3Block(num_filters, num_filters)\\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\\n\\n        self.conv3x3_2 = Conv3x3Block(num_filters, 2 * num_filters)\\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\\n\\n        self.conv3x3_3 = Conv3x3Block(2 * num_filters, 4 * num_filters)\\n\\n        # Bottleneck with residual blocks\\n        self.bottleneck1 = BottleNeckBlock(4 * num_filters)\\n        self.bottleneck2 = BottleNeckBlock(4 * num_filters)\\n\\n        # Decoder (expanding path)\\n        self.up1 = Up2x2Conv3x3(4 * num_filters, 2 * num_filters)\\n        self.conv3x3_4 = Conv3x3Block(4 * num_filters, 2 * num_filters)  # After concatenation\\n        self.dropout1 = nn.Dropout2d(0.4)\\n\\n        self.up2 = Up2x2Conv3x3(2 * num_filters, num_filters)\\n        self.conv3x3_5 = Conv3x3Block(2 * num_filters, num_filters)  # After concatenation\\n        self.dropout2 = nn.Dropout2d(0.4)\\n\\n        # Final 1x1 convolution and output\\n        self.conv1x1_output = nn.Conv2d(num_filters, 1, kernel_size=1, padding=0)\\n        self.relu_final = nn.ReLU(inplace=True)\\n\\n        # Global average pooling and final dense layer\\n        self.flatten = nn.Flatten()\\n        self.final_dense = nn.Linear(4096, 1)\\n\\n    def forward(self, x):\\n        # Initial 1x1 convolution\\n        x = self.conv1x1_input(x)\\n\\n        # Encoder path\\n        # First level\\n        skip1 = self.conv3x3_1(x)  # Save for skip connection\\n        x = self.maxpool1(skip1)\\n\\n        # Second level\\n        skip2 = self.conv3x3_2(x)  # Save for skip connection\\n        x = self.maxpool2(skip2)\\n\\n        # Third level\\n        x = self.conv3x3_3(x)\\n\\n        # Bottleneck\\n        x = self.bottleneck1(x)\\n        x = self.bottleneck2(x)\\n\\n        # Decoder path\\n        # First upsampling\\n        x = self.up1(x)\\n        x = torch.cat([x, skip2], dim=1)  # Concatenate along channel dimension\\n        x = self.conv3x3_4(x)\\n        x = self.dropout1(x)\\n\\n        # Second upsampling\\n        x = self.up2(x)\\n        x = torch.cat([x, skip1], dim=1)  # Concatenate along channel dimension\\n        x = self.conv3x3_5(x)\\n        x = self.dropout2(x)\\n\\n        # Final output\\n        x = self.conv1x1_output(x)\\n        x = self.relu_final(x)\\n\\n        # Global pooling and final prediction\\n        x = self.flatten(x)\\n        x = self.final_dense(x)\\n\\n        return x\\n\\n# Example instantiation and summary\\nif __name__ == \"__main__\":\\n    # Assuming 64x64 RGB images\\n    model = ViT(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\\n\\n    # Print model summary\\n    print(model)\\n\\n    # Test with dummy input\\n    dummy_input = torch.randn(1, 3, 64, 64).to(device)  # Batch size 1, 3 channels, 64x64\\n    output = model(dummy_input)\\n    print(f\"Output shape: {output.shape}\")\\n\\n    # Count parameters\\n    total_params = sum(p.numel() for p in model.parameters())\\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\n    print(f\"Total parameters: {total_params:,}\")\\n    print(f\"Trainable parameters: {trainable_params:,}\")\\n\\n    summary(model, input_size = (3,64,64))'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import timm\n",
        "\n",
        "#model = timm.create_model(\"hf_hub:timm/convnext_tiny.in12k\", pretrained=True)\n",
        "\n",
        "'''class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net style architecture for Image-to-PV prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, image_input_dim, num_filters=12):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        # Assuming image_input_dim is (height, width, channels)\n",
        "        # PyTorch uses (channels, height, width)\n",
        "        if len(image_input_dim) == 3:\n",
        "            input_channels = image_input_dim[0]  # Assuming CHW format\n",
        "        else:\n",
        "            input_channels = image_input_dim[2]  # Assuming HWC format\n",
        "\n",
        "        # Initial 1x1 convolution\n",
        "        self.conv1x1_input = nn.Conv2d(input_channels, num_filters, kernel_size=1, padding=0)\n",
        "\n",
        "        # Encoder (contracting path)\n",
        "        self.conv3x3_1 = Conv3x3Block(num_filters, num_filters)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_2 = Conv3x3Block(num_filters, 2 * num_filters)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_3 = Conv3x3Block(2 * num_filters, 4 * num_filters)\n",
        "\n",
        "        # Bottleneck with residual blocks\n",
        "        self.bottleneck1 = BottleNeckBlock(4 * num_filters)\n",
        "        self.bottleneck2 = BottleNeckBlock(4 * num_filters)\n",
        "\n",
        "        # Decoder (expanding path)\n",
        "        self.up1 = Up2x2Conv3x3(4 * num_filters, 2 * num_filters)\n",
        "        self.conv3x3_4 = Conv3x3Block(4 * num_filters, 2 * num_filters)  # After concatenation\n",
        "        self.dropout1 = nn.Dropout2d(0.4)\n",
        "\n",
        "        self.up2 = Up2x2Conv3x3(2 * num_filters, num_filters)\n",
        "        self.conv3x3_5 = Conv3x3Block(2 * num_filters, num_filters)  # After concatenation\n",
        "        self.dropout2 = nn.Dropout2d(0.4)\n",
        "\n",
        "        # Final 1x1 convolution and output\n",
        "        self.conv1x1_output = nn.Conv2d(num_filters, 1, kernel_size=1, padding=0)\n",
        "        self.relu_final = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Global average pooling and final dense layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.final_dense = nn.Linear(4096, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial 1x1 convolution\n",
        "        x = self.conv1x1_input(x)\n",
        "\n",
        "        # Encoder path\n",
        "        # First level\n",
        "        skip1 = self.conv3x3_1(x)  # Save for skip connection\n",
        "        x = self.maxpool1(skip1)\n",
        "\n",
        "        # Second level\n",
        "        skip2 = self.conv3x3_2(x)  # Save for skip connection\n",
        "        x = self.maxpool2(skip2)\n",
        "\n",
        "        # Third level\n",
        "        x = self.conv3x3_3(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck1(x)\n",
        "        x = self.bottleneck2(x)\n",
        "\n",
        "        # Decoder path\n",
        "        # First upsampling\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, skip2], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_4(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Second upsampling\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, skip1], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_5(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Final output\n",
        "        x = self.conv1x1_output(x)\n",
        "        x = self.relu_final(x)\n",
        "\n",
        "        # Global pooling and final prediction\n",
        "        x = self.flatten(x)\n",
        "        x = self.final_dense(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example instantiation and summary\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming 64x64 RGB images\n",
        "    model = ViT(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\n",
        "    \n",
        "    # Print model summary\n",
        "    print(model)\n",
        "\n",
        "    # Test with dummy input\n",
        "    dummy_input = torch.randn(1, 3, 64, 64).to(device)  # Batch size 1, 3 channels, 64x64\n",
        "    output = model(dummy_input)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    summary(model, input_size = (3,64,64))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainval(model, device, loader, optimizer, criterion, mode=\"train\"):\n",
        "    \n",
        "    if mode == \"train\":\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "        \n",
        "    size = len(loader)\n",
        "         \n",
        "    total_loss = 0\n",
        "    for batch_idx, (image,pv) in enumerate(loader):\n",
        "        image, pv = image.to(device), pv.to(device)\n",
        "        \n",
        "        output = model(image).squeeze()\n",
        "        loss = criterion(output, pv)\n",
        "        if mode == \"train\":\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "    total_loss = total_loss / size\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import timm\n",
        "\n",
        "# model = timm.create_model(\"hf_hub:timm/convnext_tiny.in12k\", pretrained=True)\n",
        "# model.head.fc = nn.Linear(768, 1)\n",
        "# model.to(device)\n",
        "# summary(model, input_size = (3, 64, 64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KIwj8ITRhkrA"
      },
      "outputs": [],
      "source": [
        "class Conv3x3Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic convolutional block with Conv2D -> BatchNorm -> ReLU\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(Conv3x3Block, self).__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class BottleNeckBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual bottleneck block with skip connection\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(BottleNeckBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = out + identity  # Skip connection\n",
        "        return out\n",
        "\n",
        "class Up2x2Conv3x3(nn.Module):\n",
        "    \"\"\"\n",
        "    Upsampling block with 2x2 upsampling followed by 3x3 convolution\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(Up2x2Conv3x3, self).__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net style architecture for Image-to-PV prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, image_input_dim, num_filters=12):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Assuming image_input_dim is (height, width, channels)\n",
        "        # PyTorch uses (channels, height, width)\n",
        "        if len(image_input_dim) == 3:\n",
        "            input_channels = image_input_dim[0]  # Assuming CHW format\n",
        "        else:\n",
        "            input_channels = image_input_dim[2]  # Assuming HWC format\n",
        "\n",
        "        # Initial 1x1 convolution\n",
        "        self.conv1x1_input = nn.Conv2d(input_channels, num_filters, kernel_size=1, padding=0)\n",
        "\n",
        "        # Encoder (contracting path)\n",
        "        self.conv3x3_1 = Conv3x3Block(num_filters, num_filters)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_2 = Conv3x3Block(num_filters, 2 * num_filters)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3x3_3 = Conv3x3Block(2 * num_filters, 4 * num_filters)\n",
        "\n",
        "        # Bottleneck with residual blocks\n",
        "        self.bottleneck1 = BottleNeckBlock(4 * num_filters)\n",
        "        self.bottleneck2 = BottleNeckBlock(4 * num_filters)\n",
        "\n",
        "        # Decoder (expanding path)\n",
        "        self.up1 = Up2x2Conv3x3(4 * num_filters, 2 * num_filters)\n",
        "        self.conv3x3_4 = Conv3x3Block(4 * num_filters, 2 * num_filters)  # After concatenation\n",
        "        self.dropout1 = nn.Dropout2d(0.4)\n",
        "\n",
        "        self.up2 = Up2x2Conv3x3(2 * num_filters, num_filters)\n",
        "        self.conv3x3_5 = Conv3x3Block(2 * num_filters, num_filters)  # After concatenation\n",
        "        self.dropout2 = nn.Dropout2d(0.4)\n",
        "\n",
        "        # Final 1x1 convolution and output\n",
        "        self.conv1x1_output = nn.Conv2d(num_filters, 1, kernel_size=1, padding=0)\n",
        "        self.relu_final = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Global average pooling and final dense layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.final_dense = nn.Linear(4096, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial 1x1 convolution\n",
        "        x = self.conv1x1_input(x)\n",
        "\n",
        "        # Encoder path\n",
        "        # First level\n",
        "        skip1 = self.conv3x3_1(x)  # Save for skip connection\n",
        "        x = self.maxpool1(skip1)\n",
        "\n",
        "        # Second level\n",
        "        skip2 = self.conv3x3_2(x)  # Save for skip connection\n",
        "        x = self.maxpool2(skip2)\n",
        "\n",
        "        # Third level\n",
        "        x = self.conv3x3_3(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck1(x)\n",
        "        x = self.bottleneck2(x)\n",
        "\n",
        "        # Decoder path\n",
        "        # First upsampling\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, skip2], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_4(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Second upsampling\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, skip1], dim=1)  # Concatenate along channel dimension\n",
        "        x = self.conv3x3_5(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Final output\n",
        "        x = self.conv1x1_output(x)\n",
        "        x = self.relu_final(x)\n",
        "\n",
        "        # Global pooling and final prediction\n",
        "        x = self.flatten(x)\n",
        "        x = self.final_dense(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# # Example instantiation and summary\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Assuming 64x64 RGB images\n",
        "#     model = UNet(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\n",
        "    \n",
        "#     # Print model summary\n",
        "#     print(model)\n",
        "\n",
        "#     # Test with dummy input\n",
        "#     dummy_input = torch.randn(1, 3, 64, 64).to(device)  # Batch size 1, 3 channels, 64x64\n",
        "#     output = model(dummy_input)\n",
        "#     print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "#     # Count parameters\n",
        "#     total_params = sum(p.numel() for p in model.parameters())\n",
        "#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#     print(f\"Total parameters: {total_params:,}\")\n",
        "#     print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "#     summary(model, input_size = (3,64,64))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "initial_learning_rate = 2e-4\n",
        "#optimizer = optim.Adam(model.parameters(), lr = initial_learning_rate)\n",
        "\n",
        "def checkpoint(model, filename):\n",
        "\ttorch.save(model.state_dict(), filename)\n",
        "\n",
        "def resume(model, filename):\n",
        "\tmodel.load_state_dict(torch.load(filename))\n",
        "\n",
        "#scheduler = lr_scheduler.ExponentialLR(optimizer, 0.933)\n",
        "\n",
        "indices_dayblock_shuffled = day_block_shuffle(times_trainval)\n",
        "\n",
        "#train_loss_hist = np.load(os.path.join(output_folder,'train_loss_hist.npy'))\n",
        "#val_loss_hist = np.load(os.path.join(output_folder,'val_loss_hist.npy'))\n",
        "train_loss_hist = []\n",
        "val_loss_hist = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resume_model = False\n",
        "\n",
        "if resume_model:\n",
        "\twith open(varpath('min_loss.pkl'), 'rb') as file:\n",
        "\t\tmin_loss = pkl.load(file)\n",
        "else:\n",
        "\tmin_loss = [10000] * num_fold\n",
        "\tstart_fold = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial val loss:  203.06669771498528\n",
            "Fold 1; Epoch 0: Training Loss =  6.368630548043402\n",
            "Fold 1; Epoch 0: Val Loss =  6.252595977506775\n",
            "Fold 1; Epoch 1: Training Loss =  3.5125728511926697\n",
            "Fold 1; Epoch 1: Val Loss =  7.1060785625291905\n",
            "Fold 1; Epoch 2: Training Loss =  3.1112700670690825\n",
            "Fold 1; Epoch 2: Val Loss =  6.990498957426651\n",
            "Fold 1; Epoch 3: Training Loss =  2.900419053069162\n",
            "Fold 1; Epoch 3: Val Loss =  7.26374103711999\n",
            "Fold 1; Epoch 4: Training Loss =  2.576978204888576\n",
            "Fold 1; Epoch 4: Val Loss =  7.715039502019468\n",
            "Fold 1; Epoch 5: Training Loss =  2.206390385078744\n",
            "Fold 1; Epoch 5: Val Loss =  7.901738270469334\n",
            "Fold 1; Epoch 6: Training Loss =  1.8497705887836398\n",
            "Fold 1; Epoch 6: Val Loss =  7.878121458965799\n",
            "Fold 1; Epoch 7: Training Loss =  1.6356828667653878\n",
            "Fold 1; Epoch 7: Val Loss =  8.690351119939832\n",
            "Fold 1; Epoch 8: Training Loss =  1.5299624581953877\n",
            "Fold 1; Epoch 8: Val Loss =  8.02236717334692\n",
            "Fold 1; Epoch 9: Training Loss =  1.4416183187308789\n",
            "Fold 1; Epoch 9: Val Loss =  7.680653565171836\n",
            "Fold 1; Epoch 10: Training Loss =  1.2704402831655095\n",
            "Fold 1; Epoch 10: Val Loss =  7.398347771686057\n",
            "Fold 1; Epoch 11: Training Loss =  1.2146283972544627\n",
            "Fold 1; Epoch 11: Val Loss =  7.689385946246161\n",
            "Fold 1; Epoch 12: Training Loss =  1.157697010176079\n",
            "Fold 1; Epoch 12: Val Loss =  7.549162512240202\n",
            "Fold 1; Epoch 13: Training Loss =  1.1359943985041805\n",
            "Fold 1; Epoch 13: Val Loss =  7.5124659330948536\n",
            "Fold 1; Epoch 14: Training Loss =  1.1694969996579592\n",
            "Fold 1; Epoch 14: Val Loss =  8.046383519103562\n",
            "Fold 1; Epoch 15: Training Loss =  1.175597162182201\n",
            "Fold 1; Epoch 15: Val Loss =  8.169366643048715\n",
            "Fold 1; Epoch 16: Training Loss =  1.196759573527232\n",
            "Fold 1; Epoch 16: Val Loss =  7.966280411982882\n",
            "Fold 1; Epoch 17: Training Loss =  1.054141724013865\n",
            "Fold 1; Epoch 17: Val Loss =  7.711023662401282\n",
            "Fold 1; Epoch 18: Training Loss =  0.9752177216258109\n",
            "Fold 1; Epoch 18: Val Loss =  7.7293320946071455\n",
            "Fold 1; Epoch 19: Training Loss =  0.9524416845864742\n",
            "Fold 1; Epoch 19: Val Loss =  7.801035418026689\n",
            "Fold 1; Epoch 20: Training Loss =  0.9445102183743886\n",
            "Fold 1; Epoch 20: Val Loss =  7.7220995322517725\n",
            "Fold 1; Epoch 21: Training Loss =  0.9655981422640668\n",
            "Fold 1; Epoch 21: Val Loss =  7.823506369107012\n",
            "Fold 1; Epoch 22: Training Loss =  0.9447031524532495\n",
            "Fold 1; Epoch 22: Val Loss =  7.727268744206083\n",
            "Fold 1; Epoch 23: Training Loss =  0.925667429802086\n",
            "Fold 1; Epoch 23: Val Loss =  7.268044755078744\n",
            "Fold 1; Epoch 24: Training Loss =  0.9163709463592561\n",
            "Fold 1; Epoch 24: Val Loss =  7.571034307065218\n",
            "Fold 1; Epoch 25: Training Loss =  0.9265138776798598\n",
            "Fold 1; Epoch 25: Val Loss =  7.655930228855299\n",
            "Fold 1; Epoch 26: Training Loss =  0.9063491774685044\n",
            "Fold 1; Epoch 26: Val Loss =  7.670564651489258\n",
            "Fold 1; Epoch 27: Training Loss =  0.8924498750780466\n",
            "Fold 1; Epoch 27: Val Loss =  7.576330233311308\n",
            "Fold 1; Epoch 28: Training Loss =  0.8797217930922477\n",
            "Fold 1; Epoch 28: Val Loss =  7.478882098543471\n",
            "Fold 1; Epoch 29: Training Loss =  0.8765462244375085\n",
            "Fold 1; Epoch 29: Val Loss =  7.583084445068802\n",
            "Fold 1; Epoch 30: Training Loss =  0.8704730287448869\n",
            "Fold 1; Epoch 30: Val Loss =  7.6625711952430615\n",
            "Fold 1; Epoch 31: Training Loss =  0.870177398125078\n",
            "Fold 1; Epoch 31: Val Loss =  7.617237243099489\n",
            "Fold 1; Epoch 32: Training Loss =  0.8657754104132813\n",
            "Fold 1; Epoch 32: Val Loss =  7.558121252751005\n",
            "Fold 1; Epoch 33: Training Loss =  0.8654077973548684\n",
            "Fold 1; Epoch 33: Val Loss =  7.631397350974705\n",
            "Fold 1; Epoch 34: Training Loss =  0.8616089713195167\n",
            "Fold 1; Epoch 34: Val Loss =  7.533459946729135\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m j==\u001b[32m0\u001b[39m:\n\u001b[32m     34\u001b[39m \t\u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minitial val loss: \u001b[39m\u001b[33m\"\u001b[39m, trainval(model, device, val_loader, optimizer, criterion, mode=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m loss = \u001b[43mtrainval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m train_loss_current.append(loss)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Training Loss = \u001b[39m\u001b[33m\"\u001b[39m, loss, )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrainval\u001b[39m\u001b[34m(model, device, loader, optimizer, criterion, mode)\u001b[39m\n\u001b[32m     18\u001b[39m         optimizer.step()\n\u001b[32m     19\u001b[39m         optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m total_loss = total_loss / size\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "#load_weights = True\n",
        "\n",
        "for i in range (start_fold if 'start_fold' in globals() else 1, num_fold+1): # 1-indexing\n",
        "\n",
        "\t\n",
        "\tgc.collect()\n",
        "\ttorch.cuda.empty_cache()\n",
        "\tmodel = timm.create_model(\"hf_hub:timm/convnext_tiny.in12k\", pretrained=True)\n",
        "\tmodel.head.fc = nn.Linear(768, 1)\n",
        "\tmodel.to(device)\n",
        "\t\n",
        "\t#model = UNet(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\n",
        "\toptimizer = optim.Adam(model.parameters(), lr = initial_learning_rate)\n",
        "\tscheduler = lr_scheduler.ExponentialLR(optimizer, 0.933)\n",
        "\t\n",
        "\tindices_train, indices_val = cv_split(indices_dayblock_shuffled,i-1,num_fold)\n",
        "\timages_train = images_trainval[indices_train]\n",
        "\tpv_train = pv_trainval[indices_train]\n",
        "\timages_val = images_trainval[indices_val]\n",
        "\tpv_val = pv_trainval[indices_val]\n",
        "\n",
        "\ttrain = PVDataset(images_train, pv_train)\t\n",
        "\tval = PVDataset(images_val, pv_val)\n",
        "\ttrain_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "\tval_loader = DataLoader(val, batch_size=val_batch_size, shuffle=False)\n",
        "\n",
        "\ttrain_loss_current = []\n",
        "\tval_loss_current = []\n",
        "\tpatience = 0\n",
        "\n",
        "\tfor j in range(num_epochs):\n",
        "\t\t\n",
        "\t\tif j==0:\n",
        "\t\t\tprint(\"initial val loss: \", trainval(model, device, val_loader, optimizer, criterion, mode=\"val\"))\n",
        "\n",
        "\t\tloss = trainval(model, device, train_loader, optimizer, criterion, mode=\"train\")\n",
        "\t\ttrain_loss_current.append(loss)\n",
        "\t\t\t\n",
        "\n",
        "\t\tprint(f\"Fold {i}; Epoch {j}: Training Loss = \", loss, )\n",
        "\n",
        "\t\t#if (j % 5 == 0 or j == num_epochs-1):\n",
        "\t\tloss = trainval(model, device, val_loader, optimizer, criterion, mode=\"val\")\n",
        "\t\tif j > 10:\n",
        "\t\t\tif loss > min_loss[i-1]:\n",
        "\t\t\t\tpatience += 1\n",
        "\t\t\t\tif patience == 15:\n",
        "\t\t\t\t\tprint(f\"Fold {i}; Loss stopped decreasing, stopping training.\")\n",
        "\t\t\t\t\tbreak\n",
        "\t\tif loss < min_loss[i-1]:\n",
        "\t\t\tmin_loss[i-1] = loss\n",
        "\t\t\tpatience = 0\n",
        "\t\t\tcheckpoint(model, \"repetitions/best_\" + model.__class__.__name__ + \"model_repetition_\" + str(i) + \".pth\")\n",
        "\t\tval_loss_current.append(loss)\n",
        "\t\t\n",
        "\n",
        "\t\tprint(f\"Fold {i}; Epoch {j}: Val Loss = \", loss, )\n",
        "\t\tscheduler.step()\n",
        "\ttrain_loss_hist.append(train_loss_current)\n",
        "\tval_loss_hist.append(val_loss_current)\n",
        "\tplt.plot(train_loss_hist[i-1],label='train')\n",
        "\tplt.plot(val_loss_hist[i-1],label='validation')\n",
        "\tplt.legend()\n",
        "\tplt.show()\n",
        "\n",
        "\tdel model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "print(patience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "with open(varpath('train_loss_hist.pkl'), 'wb') as file:\n",
        "    pkl.dump(train_loss_hist, file)\n",
        "with open(varpath('val_loss_hist.pkl'), 'wb') as file:\n",
        "    pkl.dump(val_loss_hist, file)\n",
        "with open(varpath('min_loss.pkl'), 'wb') as file:\n",
        "    pkl.dump(min_loss, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del indices_train, indices_val\n",
        "del images_train\n",
        "del pv_train\n",
        "del images_val\n",
        "del pv_val\n",
        "del train\n",
        "del val\n",
        "del train_loader\n",
        "del val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "images_test = np.load(varpath(\"images_test.npy\"))\n",
        "pv_test = np.load(varpath(\"pv_test.npy\"))\n",
        "times_test = np.load(varpath(\"times_test.npy\"), allow_pickle=True)\n",
        "\n",
        "images_test = images_test.transpose(0, 3, 1, 2)\n",
        "\n",
        "images_test = torch.from_numpy(images_test).float()\n",
        "\n",
        "pv_test = torch.from_numpy(pv_test).float()\n",
        "\n",
        "times_test = np.squeeze(times_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsnlWK26_p8S"
      },
      "source": [
        "### Model training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BSHhhi8__p8S"
      },
      "outputs": [],
      "source": [
        "def plot_lr(history):\n",
        "    learning_rate = history.history['lr']\n",
        "    epochs = range(1, len(learning_rate) + 1)\n",
        "    plt.plot(epochs, learning_rate)\n",
        "    plt.title('Learning rate')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float64\n",
            "Feature batch shape: torch.Size([128, 3, 64, 64])\n",
            "Labels batch shape: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# Display image and label.\n",
        "#train_features, train_labels = next(iter(train_loader))\n",
        "#print(train_labels.dtype)\n",
        "#print(f\"Feature batch shape: {train_features.size()}\")\n",
        "#print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "#img = train_features[0].squeeze()\n",
        "#label = train_labels[0]\n",
        "#plt.imshow(img.transpose(1,2,0), cmap=\"gray\")\n",
        "#plt.show()\n",
        "#print(f\"Label: {label}\")\n",
        "\n",
        "#print(img.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8LwAhfKw_p8T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 1  -- train loss: 2.13, validation loss: 2.51 (RMSE)\n",
            "Model 2  -- train loss: 1.90, validation loss: 1.93 (RMSE)\n",
            "Model 3  -- train loss: 2.05, validation loss: 1.92 (RMSE)\n",
            "Model 4  -- train loss: 1.82, validation loss: 2.53 (RMSE)\n",
            "Model 5  -- train loss: 1.93, validation loss: 2.27 (RMSE)\n",
            "Model 6  -- train loss: 1.87, validation loss: 2.25 (RMSE)\n",
            "Model 7  -- train loss: 2.37, validation loss: 2.52 (RMSE)\n",
            "Model 8  -- train loss: 2.61, validation loss: 2.33 (RMSE)\n",
            "Model 9  -- train loss: 2.12, validation loss: 1.75 (RMSE)\n",
            "Model 10  -- train loss: 1.98, validation loss: 2.33 (RMSE)\n",
            "The mean train loss (RMSE) for all models is 2.08\n",
            "The mean validation loss (RMSE) for all models is 2.23\n"
          ]
        }
      ],
      "source": [
        "# summary of training and validation results\n",
        "best_train_loss_MSE = np.zeros(num_fold)\n",
        "best_val_loss_MSE = np.zeros(num_fold)\n",
        "\n",
        "for i in range(num_fold):\n",
        "    best_val_loss_MSE[i] = np.min(val_loss_hist[i])\n",
        "    idx = np.argmin(val_loss_hist[i])\n",
        "    best_train_loss_MSE[i] = train_loss_hist[i][idx]\n",
        "    print('Model {0}  -- train loss: {1:.2f}, validation loss: {2:.2f} (RMSE)'.format(i+1, np.sqrt(best_train_loss_MSE[i]), np.sqrt(best_val_loss_MSE[i])))\n",
        "print('The mean train loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_train_loss_MSE))))\n",
        "print('The mean validation loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_val_loss_MSE))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Test Loss =  5.8380634540861305\n",
            "Fold 2: Test Loss =  5.885873394391753\n",
            "Fold 3: Test Loss =  5.869903418760408\n",
            "Fold 4: Test Loss =  6.055969829721884\n",
            "Fold 5: Test Loss =  6.281514589827169\n",
            "Fold 6: Test Loss =  6.0961378921162\n",
            "Fold 7: Test Loss =  6.040499654886397\n",
            "Fold 8: Test Loss =  5.907315125519579\n",
            "Fold 9: Test Loss =  5.513154856318777\n",
            "Fold 10: Test Loss =  6.008880087326873\n"
          ]
        }
      ],
      "source": [
        "for i in range (1, num_fold+1): # 1-indexing\n",
        "\n",
        "\tdel model, optimizer\n",
        "\tgc.collect()\n",
        "\ttorch.cuda.empty_cache()\n",
        "\t\n",
        "\tmodel = UNet(image_input_dim=(3, 64, 64), num_filters=num_filters).to(device)\n",
        "\toptimizer = optim.Adam(model.parameters(), lr = initial_learning_rate)\n",
        "\n",
        "\ttest = PVDataset(images_test, pv_test)\t\n",
        "\ttest_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\tmodel_path = 'repetitions/' + 'best_model_repetition_'+str(i)+'.pth'\n",
        "\t# load the trained model\n",
        "\tresume(model, model_path)\n",
        "\n",
        "\tloss = trainval(model, device, test_loader, optimizer, criterion, mode=\"val\")\n",
        "\n",
        "\tprint(f\"Fold {i}: Test Loss = \", loss)\n",
        "\t\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.7024]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.0048)\n",
            "tensor([[2.3459]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.3315)\n",
            "tensor([[2.5306]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.5194)\n",
            "tensor([[3.4859]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.8531)\n",
            "tensor([[3.2123]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.1316)\n",
            "tensor([[2.0521]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.3022)\n",
            "tensor([[2.9972]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.6619)\n",
            "tensor([[3.8655]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.9850)\n",
            "tensor([[3.5333]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.3075)\n",
            "tensor([[5.7348]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.6501)\n",
            "tensor([[3.9446]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.8600)\n",
            "tensor([[2.9676]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.1890)\n",
            "tensor([[5.2048]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.9897)\n",
            "tensor([[7.7223]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(5.9170)\n",
            "tensor([[4.1096]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(6.0269)\n",
            "tensor([[6.3463]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(6.9663)\n",
            "tensor([[8.3611]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.3373)\n",
            "tensor([[7.9316]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(7.9864)\n",
            "tensor([[9.5408]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(9.6600)\n",
            "tensor([[16.9994]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(15.6311)\n",
            "tensor([[15.6808]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.2195)\n",
            "tensor([[18.0373]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.1557)\n",
            "tensor([[17.9192]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.0068)\n",
            "tensor([[18.6484]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.7497)\n",
            "tensor([[19.7071]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(19.2252)\n",
            "tensor([[20.2705]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(19.9222)\n",
            "tensor([[20.7884]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(20.5028)\n",
            "tensor([[21.4766]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(21.1679)\n",
            "tensor([[22.8978]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(21.9739)\n",
            "tensor([[23.3738]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.4044)\n",
            "tensor([[22.9229]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(20.2984)\n",
            "tensor([[22.2444]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(25.3745)\n",
            "tensor([[24.8158]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(20.2316)\n",
            "tensor([[23.8179]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(21.0362)\n",
            "tensor([[23.4092]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(14.9052)\n",
            "tensor([[21.9579]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(27.5502)\n",
            "tensor([[22.9921]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(15.4521)\n",
            "tensor([[24.5549]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(23.2820)\n",
            "tensor([[25.6997]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(26.5275)\n",
            "tensor([[21.7887]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(23.4020)\n",
            "tensor([[23.3613]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.0582)\n",
            "tensor([[24.2417]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(28.6738)\n",
            "tensor([[22.8610]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(21.9514)\n",
            "tensor([[23.5789]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(23.6199)\n",
            "tensor([[24.2780]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(19.3623)\n",
            "tensor([[24.6272]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.5109)\n",
            "tensor([[22.6558]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.3710)\n",
            "tensor([[24.8593]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.1887)\n",
            "tensor([[23.6374]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(18.6998)\n",
            "tensor([[23.6666]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(22.8630)\n",
            "tensor([[23.4383]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.2586)\n",
            "tensor([[23.2263]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(24.0865)\n",
            "tensor([[16.1744]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(10.7260)\n",
            "tensor([[16.3653]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.3651)\n",
            "tensor([[13.7611]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.3112)\n",
            "tensor([[11.7747]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(9.9737)\n",
            "tensor([[12.8701]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.3333)\n",
            "tensor([[13.8102]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.9422)\n",
            "tensor([[17.1726]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(17.4458)\n",
            "tensor([[13.9169]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.4140)\n",
            "tensor([[13.4732]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.5229)\n",
            "tensor([[14.4482]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(15.1385)\n",
            "tensor([[9.4249]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(7.7075)\n",
            "tensor([[9.6362]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.2268)\n",
            "tensor([[14.4890]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(11.0639)\n",
            "tensor([[6.5104]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.6577)\n",
            "tensor([[9.5591]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(5.1559)\n",
            "tensor([[12.6439]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.3025)\n",
            "tensor([[9.7861]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(10.3623)\n",
            "tensor([[5.3763]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.9136)\n",
            "tensor([[8.4632]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(9.4224)\n",
            "tensor([[5.7585]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.5614)\n",
            "tensor([[5.6322]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.9575)\n",
            "tensor([[3.7680]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(5.2910)\n",
            "tensor([[3.8359]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(3.1742)\n",
            "tensor([[3.2593]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.0891)\n",
            "tensor([[2.5972]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(3.7030)\n",
            "tensor([[5.3044]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.7905)\n",
            "tensor([[3.6717]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.6810)\n",
            "tensor([[1.9777]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.7671)\n",
            "tensor([[0.8430]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.2138)\n",
            "tensor([[-0.1057]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.1649)\n",
            "tensor([[0.0090]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.4222)\n",
            "tensor([[0.8746]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(0.9117)\n",
            "tensor([[1.3118]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(1.6240)\n",
            "tensor([[2.5478]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(2.5782)\n",
            "tensor([[3.5434]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(3.6429)\n",
            "tensor([[4.3347]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(4.8449)\n",
            "tensor([[5.6057]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(6.2251)\n",
            "tensor([[6.3560]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(7.4929)\n",
            "tensor([[7.8976]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(8.6945)\n",
            "tensor([[8.6945]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(9.8036)\n",
            "tensor([[9.5457]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(10.8166)\n",
            "tensor([[10.6055]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.0240)\n",
            "tensor([[11.6993]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(12.8768)\n",
            "tensor([[12.3912]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(13.8529)\n",
            "tensor([[13.4415]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(14.8596)\n",
            "tensor([[14.3318]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(15.7607)\n",
            "tensor([[15.5689]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(16.4946)\n",
            "tensor([[16.2570]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor(17.4322)\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, 1000, 10):\n",
        "    input = test[i][0].to(device).unsqueeze(0)\n",
        "    print(model(input))\n",
        "    print(test[i][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZAaBHcf_p8T"
      },
      "source": [
        "### Model Testing with in-range Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk4EcIxw_p8T"
      },
      "source": [
        "Test set data are 10 cloudy days drawn from 2017 March to 2019 October, which is the same range as the training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OswWAxd__p8T"
      },
      "source": [
        "#### Feeding Real Future Sky Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSp4kqnq_p8T"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "with h5py.File(data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    images_log_test = f['test']['images_pred'][:,-1]\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # normalize image data to [0,1]\n",
        "    images_data_test = tf.image.convert_image_dtype(images_log_test, dtype=tf.float32).numpy()\n",
        "    images_data_test = mask_background(images_data_test)\n",
        "    pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"images_data_test.shape:\",images_data_test.shape)\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL3SWHlH_p8T",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=images_data_test, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(images_data_test, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Dje5maZ3_p8T"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_validation.npy'))\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "prediction_ensemble = np.mean(prediction,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3s4NKDb_p8T"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = [] # you can put sunny days here if you want to test sunny condition performance\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask for the sunny days\n",
        "mask = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask to the dataset\n",
        "times_test_sunny = times_test[mask]\n",
        "pv_log_test_sunny = pv_log_test[mask]\n",
        "images_log_test_sunny = images_log_test[mask]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask]\n",
        "pv_log_test_cloudy = pv_log_test[~mask]\n",
        "images_log_test_cloudy = images_log_test[~mask]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZsT6lgr_p8T"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdK5Sg62_p8U"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GLR9qiFV_p8U"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MpEgnaq_p8U",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_ensembles\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1, color=black, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1, label = 'SUNSET forecast',color=red,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.2], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(15,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','sunset_forecast_baseline_2017_2019_full_data_trained_2019_test_days.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNPhFObc_p8U"
      },
      "source": [
        "#### Feeding ConvLSTM generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-jP3CLj_p8U"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_ConvLSTM.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images = f['trainval']['images_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    predicted_images = mask_background(predicted_images)\n",
        "    predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images.shape:\",predicted_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlAETbt6_p8U",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_ConvLSTM_gen_images_as_input.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36nVbFOf_p8U"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x01nPo8_p8V"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcG5HHp9_p8V"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FfsTIkKC_p8V"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfdfRmoz_p8V"
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq_e_2FR_p8V"
      },
      "source": [
        "#### Feeding PhyDNet generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycgkr6AC_p8V"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_PhyDNet.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images = f['trainval']['images_pred'][:,-1]\n",
        "    #pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    predicted_images = mask_background(predicted_images)\n",
        "    predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "    #pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images.shape:\",predicted_images.shape)\n",
        "#print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHvi4O3H_p8V",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_PhyDNet_gen_images_as_input.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmZhxOkn_p8W"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLOjQQVX_p8W"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtv2MJzp_p8W"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pPzLgVBe_p8W"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvpdMdcH_p8X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDPatyhe_p8X"
      },
      "source": [
        "#### Feeding PhyDNet+GAN generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRLjvY8p_p8X"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_PhyDNetGAN.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images = f['trainval']['images_pred'][:,-1]\n",
        "    #pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    predicted_images = mask_background(predicted_images)\n",
        "    predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "    #pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images.shape:\",predicted_images.shape)\n",
        "#print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo2QrBoK_p8X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_SkyImageGAN_gen_images_as_input.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-CtFqgs_p8X"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAcy-OnC_p8X"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xaIL0OF_p8X"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(5times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e2DW7bsl_p8Y"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yehVXuSY_p8Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euCa3dq4_p8Y"
      },
      "source": [
        "#### Feeding VideoGPT generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz51jq6o_p8Y"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_VideoGPT.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images_all_samplings = f['trainval']['images_pred_all_samplings'][:,:,-1]\n",
        "    #pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    #predicted_images = mask_background(predicted_images)\n",
        "    #predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "    #pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"predicted_images_all_samplings.shape:\",predicted_images_all_samplings.shape)\n",
        "#print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QmYOerN_p8Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "num_samplings = 10\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating prediction for sampling {0}'.format(j+1))\n",
        "        predicted_images = predicted_images_all_samplings[j]\n",
        "        predicted_images = mask_background(predicted_images)\n",
        "        predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\".format(i+1))\n",
        "        loss[i,j] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\".format(i+1))\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_VideoGPT_gen_images_as_input.npy'),prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2dFvG4oM_p8Y"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_validation_VideoGPT_gen_images_as_input.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Uv6F9JUH_p8Y"
      },
      "outputs": [],
      "source": [
        "prediction_samp = prediction.reshape(-1,prediction.shape[-1])\n",
        "prediction_samp.shape\n",
        "prediction_ensemble = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1uFkx0z_p8Y"
      },
      "outputs": [],
      "source": [
        "# evaluate deterministic performance\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print('test rmse for all 10 samplings is {0:3f}'.format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulkHszae_p8Z"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOxglfrh_p8Z"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmeZvVgj_p8Z"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "trw4x5Yn_p8Z"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction_samp,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwtUNZqy_p8Z",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w22mxO_3_p8a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMlChrkG_p8a"
      },
      "source": [
        "#### Feeding SkyGPT generated images as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3TOUx5R_p8a"
      },
      "outputs": [],
      "source": [
        "gen_data_folder = os.path.join(pardir,\"data\")\n",
        "# You should modify the path to where you save your generated images\n",
        "gen_data_path = os.path.join(gen_data_folder, \"Predicted_images_validation_set\",'predicted_images_validation_set_SkyGPT.hdf5')\n",
        "\n",
        "with h5py.File(gen_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    predicted_images_all_samplings = f['trainval']['images_pred_all_samplings'][:,:,-1]\n",
        "\n",
        "print(\"predicted_images_all_samplings.shape:\",predicted_images_all_samplings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7ULQajn_p8a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "num_samplings = 10\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating prediction for sampling {0}'.format(j+1))\n",
        "        predicted_images = predicted_images_all_samplings[j]\n",
        "        #predicted_images = mask_background(predicted_images)\n",
        "        predicted_images = tf.image.convert_image_dtype(predicted_images, dtype=tf.float32).numpy()\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\".format(i+1))\n",
        "        loss[i,j] = model.evaluate(x=predicted_images, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\".format(i+1))\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_images, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_validation_SkyGPT_gen_images_as_input.npy'),prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lpE334pk_p8a"
      },
      "outputs": [],
      "source": [
        "prediction_samp = prediction.reshape(-1,prediction.shape[-1])\n",
        "prediction_samp.shape\n",
        "prediction_ensemble = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQJYOJi7_p8a"
      },
      "outputs": [],
      "source": [
        "# evaluate deterministic performance\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print('test rmse for all 10 samplings is {0:3f}'.format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFR3WwlW_p8b"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "cloudy_dates = [(2017,9,6),(2017,11,4),(2017,12,29),\n",
        "        (2018,5,31),(2018,7,12),(2018,10,3),\n",
        "        (2019,1,26),(2019,5,27),(2019,9,7),(2019,10,19)]\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "## generate mask2 for the sunny days\n",
        "times_test_cloudy = times_test\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask2]\n",
        "pv_log_test_cloudy = pv_log_test\n",
        "images_log_test_cloudy = images_log_test\n",
        "prediction_cloudy = prediction_ensemble\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQYAeU7i_p8b"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfkF3m37_p8b"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "me_wW9KL_p8b"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction_samp,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLZ_hSLf_p8b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(10,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRG-wXH_p8b"
      },
      "source": [
        "### Model Testing with Out-range test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9m5Bm7j_p8b"
      },
      "source": [
        "Test set data are 5 cloudy days drawn from 2019 Nov. and Dec., which is the outside the range of the training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJWhoL-Y_p8b"
      },
      "source": [
        "#### Feeding Real Future Sky Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpFAG13X_p8b"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "data_folder = os.path.join(pardir,\"data\")\n",
        "test_data_path = os.path.join(data_folder,'test_set_2019nov_dec.hdf5')\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test_2019nov_dec.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "model_name = 'UNet_Image_PV_mapping_masked_image'\n",
        "output_folder = os.path.join(cwd,\"model_output\", model_name)\n",
        "\n",
        "with h5py.File(test_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    images_log_test = f['test']['images_pred'][:,-1]\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "    # process image data\n",
        "    images_log_test = mask_background(images_log_test)\n",
        "    images_log_test = tf.image.convert_image_dtype(images_log_test, dtype=tf.float32).numpy()\n",
        "    pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"images_log_test.shape:\",images_log_test.shape)\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)\n",
        "#print(\"pv_pred_test.shape:\",pv_pred_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nnp4gYd6_p8c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=images_log_test, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(images_log_test, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_2019nov_dec.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofk4Ao4O_p8c"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = [] # you can put sunny days here if you want to test sunny condition performance\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "prediction_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "prediction_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFwQErvz_p8c"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqmLX5WY_p8c"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YckeyUHA_p8c"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKifyK5S_p8c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(15,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL_m4edv_p8c"
      },
      "source": [
        "#### Feeding ConvLSTM Generated Images as Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z7M2bVwr_p8c"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','ConvLSTM','save','ConvLSTM_sky_image_dataset_interval_2min_all_data_v2')\n",
        "predicted_image = np.load(os.path.join(predicted_image_folder,'predicted_images_new_test_set_2019.npy'),allow_pickle=True)\n",
        "predicted_image = predicted_image[:,-1]\n",
        "predicted_image = mask_background(predicted_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwO-l2lk_p8d"
      },
      "outputs": [],
      "source": [
        "predicted_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_iL-ySA_p8d"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "data_folder = os.path.join(pardir,\"data\")\n",
        "test_data_path = os.path.join(data_folder,'test_set_2019nov_dec.hdf5')\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test_2019nov_dec.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "with h5py.File(test_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    images_log_test = f['test']['images_pred'][:,-1]\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "# process image data\n",
        "images_log_test = mask_background(images_log_test)\n",
        "images_log_test = tf.image.convert_image_dtype(images_log_test, dtype=tf.float32).numpy()\n",
        "pv_log_test = tf.convert_to_tensor(pv_log_test, dtype=tf.float32).numpy()\n",
        "\n",
        "print(\"images_log_test.shape:\",images_log_test.shape)\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)\n",
        "#print(\"pv_pred_test.shape:\",pv_pred_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AGtQz1I_p8d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_image, y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_image, batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_ConvLSTM_generated_imgs_as_input_new.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "#print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the model\".format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzj28_aA_p8d"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "#pv_pred_test_sunny = pv_pred_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "images_log_test_sunny = images_log_test[mask3]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "images_log_test_cloudy = images_log_test[~mask3]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEfj1S43_p8d"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJRoSa9Q_p8d"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kZpcwXP4_p8d"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08nqCXSE_p8e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoWlyEi_p8e"
      },
      "source": [
        "#### Feeding PhyDNet Generated Images as Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVS6JX7w_p8e"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','PhyDNet','save','PhyDNet_sky_image_dataset_interval_2min_all_data_v2')\n",
        "predicted_image = np.load(os.path.join(predicted_image_folder,'predicted_images_new_test_set_2019.npy'),allow_pickle=True)\n",
        "predicted_image = mask_background(predicted_image)\n",
        "predicted_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az1maKm7_p8f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_PhyDNet_generated_imgs_as_input_new.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "#print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the model\".format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBq3ae4k_p8f"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "#pv_pred_test_sunny = pv_pred_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "images_log_test_sunny = images_log_test[mask3]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "images_log_test_cloudy = images_log_test[~mask3]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyyW-4hi_p8f"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATkN2joW_p8f"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w46dZibj_p8g"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPSLKBMk_p8g",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVq3w-Ju_p8g"
      },
      "source": [
        "#### Feeding PhyDNet+GAN Generated Images as Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "varbMhwQ_p8g"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','PhyDNet','save','PhyDNet_LSGAN_sky_image_dataset_gen_lr_0.001_batch_size_16_model_v2_scheduled_and_reverse_scheduled_sampling_MAE_loss_all_data_v3')\n",
        "predicted_image = np.load(os.path.join(predicted_image_folder,'predicted_images_new_test_set_2019.npy'),allow_pickle=True)\n",
        "predicted_image = mask_background(predicted_image)\n",
        "predicted_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvTJkS7h_p8g"
      },
      "outputs": [],
      "source": [
        "plt.imshow(predicted_image[0,0,:,:,::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEhT2y24_p8g",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,len(times_test)))\n",
        "prediction = np.zeros((num_fold,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    # model evaluation\n",
        "    print(\"evaluating performance for the model\".format(i+1))\n",
        "    loss[i] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=256, verbose=1)\n",
        "\n",
        "    # generate prediction\n",
        "    print(\"generating predictions for the model\".format(i+1))\n",
        "    prediction[i] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=256, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_SkyImageGAN_generated_imgs_as_input_new.npy'),prediction)\n",
        "\n",
        "# using the ensemble mean of the 10 models as the final prediction\n",
        "print('-'*50)\n",
        "#print(\"model ensembling ...\")\n",
        "prediction_ensemble = np.mean(prediction,axis=0)\n",
        "loss_rmse = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} for the model\".format(loss_rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VCFNicH_p8g"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])\n",
        "\n",
        "## generate mask3 for the sunny days\n",
        "mask3 = np.zeros(len(pv_log_test),dtype=bool)\n",
        "for i in sunny_dates_test:\n",
        "    mask3[np.where(dates_test==i)[0]]=1\n",
        "\n",
        "## apply the mask3 to the dataset\n",
        "times_test_sunny = times_test[mask3]\n",
        "#pv_pred_test_sunny = pv_pred_test[mask3]\n",
        "pv_log_test_sunny = pv_log_test[mask3]\n",
        "images_log_test_sunny = images_log_test[mask3]\n",
        "prediction_ensemble_sunny = prediction_ensemble[mask3]\n",
        "print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n",
        "\n",
        "times_test_cloudy = times_test[~mask3]\n",
        "#pv_pred_test_cloudy = pv_pred_test[~mask3]\n",
        "pv_log_test_cloudy = pv_log_test[~mask3]\n",
        "images_log_test_cloudy = images_log_test[~mask3]\n",
        "prediction_ensemble_cloudy = prediction_ensemble[~mask3]\n",
        "print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krH_iCwC_p8g"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR-fqpty_p8g"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B04lOJ3b_p8h"
      },
      "outputs": [],
      "source": [
        "percent25_prediction = np.percentile(prediction,25,axis=0)\n",
        "percent75_prediction = np.percentile(prediction,75,axis=0)\n",
        "percent5_prediction = np.percentile(prediction,5,axis=0)\n",
        "percent95_prediction = np.percentile(prediction,95,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWzpTqDv_p8h",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast predictions\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#8C1515'\n",
        "blue = '#67AFD2'\n",
        "grey =  '#B6B1A9'\n",
        "black = '#2E2D29'\n",
        "red = '#8C1515'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "black = '#2E2D29'\n",
        "dark_red = '#820000'\n",
        "light_red = '#B83A4B'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n",
        "    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_ensemble[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=black, label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1.5,label = 'UNet nowcast mean',color=red,markerfacecolor=\"None\")\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction[date_mask], percent95_prediction[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction[date_mask], percent75_prediction[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    #for j in range(prediction.shape[0]):\n",
        "    #    if j==0:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,label = 'UNet nowcast',color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #    else:\n",
        "    #        ax.plot(hours_xaxis, prediction[j][date_mask],linewidth = 1,color=red,alpha=0.5,markerfacecolor=\"None\")\n",
        "    #ax.plot(hours_xaxis, per_prediction_ensemble[date_mask],linewidth = 1,label = 'Persistence forecast',color=blue,markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "    #ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n",
        "    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 4)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "#plt.savefig(os.path.join(pardir,'results','unet_nowcast_full_data_trained_2019_test_forecast_with_real_future_image_2.pdf'), bbox_inches='tight',dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiS3fR8B_p8h"
      },
      "source": [
        "#### Feeding VideoGPT Generated Images as Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmBGQhME_p8h"
      },
      "source": [
        "We experiment with different number of futures generated, ranging from 1 future to 50 different futures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "t6QGhFq-_p8h"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','VideoGPT','inference','VideoGPT_2019_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L-YRRCW_p8h",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "num_samplings = 50\n",
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating predictions for sample {0} ...'.format(j+1))\n",
        "        # load predicted image samples\n",
        "        predicted_image = np.load(os.path.join(predicted_image_folder,'sample_'+str(j)+'.npy'),allow_pickle=True)\n",
        "        print(np.min(predicted_image))\n",
        "        print(np.max(predicted_image))\n",
        "        predicted_image = mask_background(predicted_image)\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\")\n",
        "        loss[i,j] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=200, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\")\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=200, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_VideoGPT_generated_imgs_as_input_new.npy'),prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4XdtHj6_p8i"
      },
      "source": [
        "##### Use 1 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwf_O5MB_p8i"
      },
      "outputs": [],
      "source": [
        "# load testing data\n",
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "data_folder = os.path.join(pardir,\"data\")\n",
        "test_data_path = os.path.join(data_folder,'test_set_2019nov_dec.hdf5')\n",
        "times_test = np.load(os.path.join(data_folder,\"times_curr_test_2019nov_dec.npy\"),allow_pickle=True)\n",
        "print(\"times_test.shape:\", times_test.shape)\n",
        "\n",
        "with h5py.File(test_data_path,'r') as f:\n",
        "\n",
        "    # read in the data\n",
        "    pv_log_test = f['test']['pv_pred'][:,-1]\n",
        "\n",
        "print(\"pv_log_test.shape:\",pv_log_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbigmsWF_p8i"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_VideoGPT_generated_imgs_as_input_new.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XF75qjU_p8i"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 1\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcqUIX-u_p8i"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 1 sampling\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 1 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbXULns0_p8i"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Yoy5Pn_p8i"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "91no2zm5_p8j"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nXzfRtiu_p8j"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnKL5FFZ_p8j",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-G8BjEd_p8j"
      },
      "source": [
        "##### Use 5 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ3vI-yQ_p8j"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_VideoGPT_generated_imgs_as_input_new.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr8f9qGe_p8j"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 5\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d-Lksd1_p8j"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_5samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 5 samplings for all sub-models\".format(loss_rmse_5samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZTL-JLV_p8j"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZNRQLFR_p8j"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o7_MDTye_p8j"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9qE7Nym_p8j"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q3Y0u9Ul_p8j"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrlddXqu_p8j",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rQmZO8Rq_p8j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLW3Lcj1_p8j"
      },
      "source": [
        "##### Use 10 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ogWopQM_p8j"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 10\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h62apQX__p8j"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 10 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOVew5LH_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-M_TPE1_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N4C8MJ_i_p8k"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhgUnX5F_p8k"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xCT1VXSb_p8k"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx2HwoW1_p8k",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLXrigmU_p8k"
      },
      "source": [
        "##### Use 20 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMAORW_F_p8k"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 20\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ZhH64Z_p8k"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtV9-eBV_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4vXpldz_p8k"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jNwZr80L_p8k"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wUwvPJ86_p8k"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTXLHkZW_p8l",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7uyBh5h_p8l"
      },
      "source": [
        "##### Use 30 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFZ87K_H_p8l"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 30\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUpOdy9R_p8l"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXGxdAf9_p8l"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSHqbdSr_p8l"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N8SuTeN2_p8l"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HIreTsgZ_p8l"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKxDi0S-_p8l",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPbcY--H_p8m"
      },
      "source": [
        "##### Use 40 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc9SiTNA_p8m"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 40\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0ib2iwR_p8m"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDX3_x4m_p8m"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EStvMzf7_p8m"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sa54VvIN_p8m"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X8vRQ-Kc_p8m"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjeOMhbC_p8m",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68hZ1P5q_p8m"
      },
      "source": [
        "##### Use 50 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy_P-pMp_p8m"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 50\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqeoJNE9_p8m"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BESrzj3h_p8m"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ockrRjCS_p8n"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ly0NhQmz_p8n"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KjtC-3Wt_p8n"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve1vob9T_p8n",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnxqk2Xo_p8n"
      },
      "source": [
        "#### Feeding SkyGPT Generated Images as Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2ECEckA_p8n"
      },
      "source": [
        "Similarly, we experimented with different number of generated future scenarios, ranging from 1 future to 50 futures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jVdXuW24_p8n"
      },
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "pardir = os.path.dirname(os.path.dirname(cwd))\n",
        "# You should modify the path to where you save your generated images\n",
        "predicted_image_folder = os.path.join(pardir,'models','SkyGPT','inference','SkyGPT_2019_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fol6pypp_p8n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "num_samplings = 50\n",
        "# evaluate model on the test set and generate predictions\n",
        "loss = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "prediction = np.zeros((num_fold,num_samplings,len(times_test)))\n",
        "\n",
        "for i in range(num_fold):\n",
        "    # define model path\n",
        "    print(\"loading repetition {0} model ...\".format(i+1))\n",
        "    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n",
        "    # load the trained model\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    for j in range(num_samplings):\n",
        "        print('generating predictions for sample {0} ...'.format(j+1))\n",
        "        # load predicted image samples\n",
        "        predicted_image = np.load(os.path.join(predicted_image_folder,'sample_'+str(j)+'.npy'),allow_pickle=True)\n",
        "\n",
        "        # scale back all the pixel values back to [0,1] with clipping\n",
        "        predicted_image = np.clip(predicted_image,-0.5,0.5)+0.5\n",
        "        predicted_image = mask_background(predicted_image)\n",
        "\n",
        "        # model evaluation\n",
        "        #print(\"evaluating performance for the model\")\n",
        "        loss[i,j] = model.evaluate(x=predicted_image[:,-1], y=pv_log_test, batch_size=200, verbose=1)\n",
        "\n",
        "        # generate prediction\n",
        "        #print(\"generating predictions for the model\")\n",
        "        prediction[i,j] = np.squeeze(model.predict(predicted_image[:,-1], batch_size=200, verbose=1))\n",
        "\n",
        "# saving predictions from each model\n",
        "np.save(os.path.join(output_folder,'test_predictions_SkyGPT_generated_imgs_as_input_new.npy'),prediction)\n",
        "    # loss averaged over all samplings for model 1\n",
        "    #loss_rmse = np.sqrt(np.mean((np.mean(prediction[i],axis=0)-pv_log_test)**2))\n",
        "    #print(\"the test set RMSE is {0:.3f} averaged over all samplings for sub-model {1}\".format(loss_rmse,i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvB2sy83_p8n"
      },
      "source": [
        "##### Use 1 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHBJYwRy_p8n"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_SkyGPT_generated_imgs_as_input_new.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLsV9eVr_p8n"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 1\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uC4V7Ao_p8n"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_1samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 1 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGHK6VK-_p8n"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftcd0ue3_p8n"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TN6IsPsq_p8n"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WrcHKV3_p8n"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tPwCo0cs_p8n"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9hE6iuu_p8n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    #rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_samp[date_mask]))))\n",
        "    #mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_samp[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_samp[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp5jEquj_p8o"
      },
      "source": [
        "##### Use 5 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SblfEftx_p8o"
      },
      "outputs": [],
      "source": [
        "prediction = np.load(os.path.join(output_folder,'test_predictions_SkyGPT_generated_imgs_as_input.npy'))\n",
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKqHmSqr_p8o"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 5\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqOQs1X7_p8o"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_5samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 5 samplings for all sub-models\".format(loss_rmse_5samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDq4LRoH_p8o"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQG2c2e5_p8o"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mPTdMKBC_p8o"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U1EQBEB_p8o"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Va9bzY5e_p8o"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB7xgu5X_p8o",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Mq1rKGCa_p8p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JGt7rs_p8p"
      },
      "source": [
        "##### Use 10 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtCC9__O_p8p"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 10\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX50RwJ__p8p"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over 10 samplings for all sub-models\".format(loss_rmse_10samp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1PLMMsb_p8p"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--n4GntH_p8p"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ke23VcUu_p8p"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA0qcqAM_p8p"
      },
      "outputs": [],
      "source": [
        "percent5_prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BzLCKS57_p8p"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOa5arbH_p8p",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5qdjmr3_p8q"
      },
      "source": [
        "##### Use 20 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvQd7cC5_p8q"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 20\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhuffEVx_p8q"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYz9hUS6_p8q"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAUYhDir_p8q"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ARCgvwu5_p8q"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0yHZC6wX_p8q"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6QpwUnn_p8q",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    #rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_samp[date_mask]))))\n",
        "    #mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_samp[date_mask])))\n",
        "    #per_rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-per_prediction_samp[date_mask]))))\n",
        "    #fs = (1 - rmse/per_rmse)*100\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    #ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhE4FDzj_p8q"
      },
      "source": [
        "##### Use 30 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3jeKgVe_p8q"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 30\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHdbuMtI_p8q"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNWRUW4G_p8q"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6CuhuJj_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hCS1lPS8_p8r"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vI1R08r7_p8r"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZXYOYjA_p8r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxbODDoH_p8r"
      },
      "source": [
        "##### Use 40 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq1tRNV5_p8r"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 40\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejycpmjd_p8r"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67vLci5I_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDogRw7z_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0MOtUcwV_p8r"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-8YwfkbX_p8r"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFkRDXVv_p8r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsIqEMww_p8r"
      },
      "source": [
        "##### Use 50 samplings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTjUtFh__p8r"
      },
      "outputs": [],
      "source": [
        "use_samp_no = 50\n",
        "prediction_samp = prediction[:,:use_samp_no,:]\n",
        "prediction_samp = prediction_samp.reshape(-1,prediction_samp.shape[-1])\n",
        "prediction_samp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvLZN-mG_p8r"
      },
      "outputs": [],
      "source": [
        "# Deterministic performance\n",
        "# calculate RMSE based on ensemble mean of 10 submodels and 10 samplings\n",
        "prediction_samp_ensemble = np.mean(prediction_samp,axis=0)\n",
        "loss_rmse_10samp = np.sqrt(np.mean((prediction_samp_ensemble-pv_log_test)**2))\n",
        "print(\"the test set RMSE is {0:.3f} averaged over {1} samplings for all sub-models\".format(loss_rmse_10samp,use_samp_no))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW3xLonf_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# CRPS evaluation\n",
        "crps = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    crps[i],_,_ = pscore(prediction_samp[:,i],pv_log_test[i]).compute()\n",
        "crps_mean = np.mean(crps)\n",
        "print('The mean crps: {0:.3f}'.format(crps_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSKeYWe4_p8r"
      },
      "outputs": [],
      "source": [
        "## probabilistic eval metrics\n",
        "# Winkler score evaluation\n",
        "wscore = np.zeros(len(times_test))\n",
        "for i in range(len(times_test)):\n",
        "    wscore[i] = compute_winkler_score(prediction_samp[:,i],pv_log_test[i])\n",
        "wscore_mean = np.mean(wscore)\n",
        "print('The mean Winkler score: {0:.3f}'.format(wscore_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1AKSqNVr_p8r"
      },
      "outputs": [],
      "source": [
        "percent25_prediction_samp = np.percentile(prediction_samp,25,axis=0)\n",
        "percent75_prediction_samp = np.percentile(prediction_samp,75,axis=0)\n",
        "percent5_prediction_samp = np.percentile(prediction_samp,5,axis=0)\n",
        "percent95_prediction_samp = np.percentile(prediction_samp,95,axis=0)\n",
        "percent50_prediction_samp = np.percentile(prediction_samp,50,axis=0)\n",
        "mean_prediction_samp = np.mean(prediction_samp,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-19KtFuX_p8r"
      },
      "outputs": [],
      "source": [
        "# formulate sunny and cloudy test days\n",
        "sunny_dates = []\n",
        "cloudy_dates = [(2019, 11, 12),(2019, 11, 13),(2019, 11, 29),(2019, 12, 8),(2019, 12, 23)]\n",
        "sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n",
        "cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n",
        "\n",
        "dates_test = np.asarray([times.date() for times in times_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLGinsua_p8r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualization of forecast prediction_samps\n",
        "dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n",
        "hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n",
        "\n",
        "f,axarr = plt.subplots(5,1,sharex=False, sharey = True)\n",
        "xfmt = mdates.DateFormatter('%H')\n",
        "fmt_date = datetime.date(2000,1,1)\n",
        "\n",
        "green = '#8AB8A7'\n",
        "red = '#B83A4B'\n",
        "light_blue = '#67AFD2'\n",
        "dark_blue = '#016895'\n",
        "blue = '#4298B5'\n",
        "grey =  '#B6B1A9'\n",
        "\n",
        "for i,date in enumerate(cloudy_dates_test):\n",
        "    ax = axarr[i]\n",
        "    date_mask = (dates_test == date)\n",
        "    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]]\n",
        "\n",
        "    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color='k', label = 'Ground truth')\n",
        "    ax.fill_between(hours_xaxis, percent5_prediction_samp[date_mask], percent95_prediction_samp[date_mask], color=light_blue, alpha=0.5, label = '5~95%tile pred.')\n",
        "    ax.fill_between(hours_xaxis, percent25_prediction_samp[date_mask], percent75_prediction_samp[date_mask], color=blue, alpha=0.75, label = '25~75%tile pred.')\n",
        "    ax.plot(hours_xaxis, percent50_prediction_samp[date_mask],linewidth = 1,label = 'Median pred.',color=dark_blue,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.plot(hours_xaxis, mean_prediction_samp[date_mask],'--',linewidth = 1, label = 'Mean pred.',color=red,alpha=1, markerfacecolor=\"None\")\n",
        "    ax.set_ylabel('PV output (kW)')\n",
        "    ax.xaxis.set_major_formatter(xfmt)\n",
        "    ax.text(0.85,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n",
        "\n",
        "axarr[0].set_ylim(0, 30)\n",
        "axarr[0].legend(bbox_to_anchor= [.5,1.3], loc = 'upper center', ncol = 3)\n",
        "axarr[-1].set_xlabel('Hour of day')\n",
        "\n",
        "f.set_size_inches(10,15)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
